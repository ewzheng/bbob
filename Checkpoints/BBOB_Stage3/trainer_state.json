{
  "best_global_step": 5547,
  "best_metric": 0.015967114021037333,
  "best_model_checkpoint": "Output/detection_2025-07-24_17-23-31/checkpoint-5547",
  "epoch": 5.0,
  "eval_steps": 5547,
  "global_step": 27735,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.005768884081485488,
      "grad_norm": 3.7717647552490234,
      "learning_rate": 1.3970256872465075e-07,
      "loss": 4.8879,
      "num_input_tokens_seen": 1065152,
      "step": 32,
      "train_runtime": 66.738,
      "train_tokens_per_second": 15960.212
    },
    {
      "epoch": 0.011537768162970976,
      "grad_norm": 3.5387215614318848,
      "learning_rate": 2.8391167192429027e-07,
      "loss": 4.8127,
      "num_input_tokens_seen": 2165088,
      "step": 64,
      "train_runtime": 130.8085,
      "train_tokens_per_second": 16551.583
    },
    {
      "epoch": 0.017306652244456464,
      "grad_norm": 4.279013633728027,
      "learning_rate": 4.2812077512392976e-07,
      "loss": 4.8663,
      "num_input_tokens_seen": 3287680,
      "step": 96,
      "train_runtime": 195.8691,
      "train_tokens_per_second": 16785.09
    },
    {
      "epoch": 0.02307553632594195,
      "grad_norm": 3.900113105773926,
      "learning_rate": 5.723298783235692e-07,
      "loss": 4.8317,
      "num_input_tokens_seen": 4404256,
      "step": 128,
      "train_runtime": 260.7216,
      "train_tokens_per_second": 16892.563
    },
    {
      "epoch": 0.028844420407427438,
      "grad_norm": 4.010476112365723,
      "learning_rate": 7.165389815232087e-07,
      "loss": 4.8518,
      "num_input_tokens_seen": 5493088,
      "step": 160,
      "train_runtime": 324.3191,
      "train_tokens_per_second": 16937.296
    },
    {
      "epoch": 0.03461330448891293,
      "grad_norm": 3.9732444286346436,
      "learning_rate": 8.607480847228482e-07,
      "loss": 4.8578,
      "num_input_tokens_seen": 6626496,
      "step": 192,
      "train_runtime": 390.1513,
      "train_tokens_per_second": 16984.424
    },
    {
      "epoch": 0.040382188570398415,
      "grad_norm": 3.6878864765167236,
      "learning_rate": 1.0049571879224876e-06,
      "loss": 4.7295,
      "num_input_tokens_seen": 7692032,
      "step": 224,
      "train_runtime": 452.8548,
      "train_tokens_per_second": 16985.647
    },
    {
      "epoch": 0.0461510726518839,
      "grad_norm": 4.1480536460876465,
      "learning_rate": 1.149166291122127e-06,
      "loss": 4.8712,
      "num_input_tokens_seen": 8765536,
      "step": 256,
      "train_runtime": 516.0482,
      "train_tokens_per_second": 16985.886
    },
    {
      "epoch": 0.05191995673336939,
      "grad_norm": 3.6920270919799805,
      "learning_rate": 1.2933753943217667e-06,
      "loss": 4.8286,
      "num_input_tokens_seen": 9868096,
      "step": 288,
      "train_runtime": 580.7609,
      "train_tokens_per_second": 16991.668
    },
    {
      "epoch": 0.057688840814854876,
      "grad_norm": 3.5884461402893066,
      "learning_rate": 1.4375844975214062e-06,
      "loss": 4.8514,
      "num_input_tokens_seen": 10939520,
      "step": 320,
      "train_runtime": 644.1004,
      "train_tokens_per_second": 16984.183
    },
    {
      "epoch": 0.06345772489634037,
      "grad_norm": 4.5571746826171875,
      "learning_rate": 1.5817936007210458e-06,
      "loss": 4.8067,
      "num_input_tokens_seen": 12029120,
      "step": 352,
      "train_runtime": 708.2491,
      "train_tokens_per_second": 16984.308
    },
    {
      "epoch": 0.06922660897782586,
      "grad_norm": 3.988229990005493,
      "learning_rate": 1.726002703920685e-06,
      "loss": 4.8065,
      "num_input_tokens_seen": 13132320,
      "step": 384,
      "train_runtime": 773.063,
      "train_tokens_per_second": 16987.386
    },
    {
      "epoch": 0.07499549305931134,
      "grad_norm": 3.5282366275787354,
      "learning_rate": 1.8702118071203245e-06,
      "loss": 4.839,
      "num_input_tokens_seen": 14221472,
      "step": 416,
      "train_runtime": 837.3136,
      "train_tokens_per_second": 16984.642
    },
    {
      "epoch": 0.08076437714079683,
      "grad_norm": 3.9365994930267334,
      "learning_rate": 2.014420910319964e-06,
      "loss": 4.876,
      "num_input_tokens_seen": 15339104,
      "step": 448,
      "train_runtime": 902.8508,
      "train_tokens_per_second": 16989.634
    },
    {
      "epoch": 0.08653326122228232,
      "grad_norm": 3.5752758979797363,
      "learning_rate": 2.1586300135196038e-06,
      "loss": 4.7947,
      "num_input_tokens_seen": 16463136,
      "step": 480,
      "train_runtime": 968.7112,
      "train_tokens_per_second": 16994.886
    },
    {
      "epoch": 0.0923021453037678,
      "grad_norm": 3.892069101333618,
      "learning_rate": 2.302839116719243e-06,
      "loss": 4.7979,
      "num_input_tokens_seen": 17535872,
      "step": 512,
      "train_runtime": 1032.0943,
      "train_tokens_per_second": 16990.571
    },
    {
      "epoch": 0.09807102938525329,
      "grad_norm": 3.5640830993652344,
      "learning_rate": 2.4470482199188827e-06,
      "loss": 4.8543,
      "num_input_tokens_seen": 18646400,
      "step": 544,
      "train_runtime": 1097.3605,
      "train_tokens_per_second": 16992.046
    },
    {
      "epoch": 0.10383991346673878,
      "grad_norm": 4.015979766845703,
      "learning_rate": 2.5912573231185223e-06,
      "loss": 4.8053,
      "num_input_tokens_seen": 19732672,
      "step": 576,
      "train_runtime": 1161.539,
      "train_tokens_per_second": 16988.386
    },
    {
      "epoch": 0.10960879754822427,
      "grad_norm": 4.110789775848389,
      "learning_rate": 2.735466426318161e-06,
      "loss": 4.8351,
      "num_input_tokens_seen": 20856864,
      "step": 608,
      "train_runtime": 1227.4172,
      "train_tokens_per_second": 16992.482
    },
    {
      "epoch": 0.11537768162970975,
      "grad_norm": 4.03134298324585,
      "learning_rate": 2.8796755295178008e-06,
      "loss": 4.7897,
      "num_input_tokens_seen": 21933728,
      "step": 640,
      "train_runtime": 1290.7879,
      "train_tokens_per_second": 16992.511
    },
    {
      "epoch": 0.12114656571119524,
      "grad_norm": 3.7076642513275146,
      "learning_rate": 3.0238846327174404e-06,
      "loss": 4.7915,
      "num_input_tokens_seen": 23018336,
      "step": 672,
      "train_runtime": 1354.5675,
      "train_tokens_per_second": 16993.126
    },
    {
      "epoch": 0.12691544979268074,
      "grad_norm": 3.8626351356506348,
      "learning_rate": 3.16809373591708e-06,
      "loss": 4.8315,
      "num_input_tokens_seen": 24107744,
      "step": 704,
      "train_runtime": 1418.8055,
      "train_tokens_per_second": 16991.578
    },
    {
      "epoch": 0.13268433387416623,
      "grad_norm": 3.4273409843444824,
      "learning_rate": 3.3123028391167193e-06,
      "loss": 4.7813,
      "num_input_tokens_seen": 25185760,
      "step": 736,
      "train_runtime": 1482.5707,
      "train_tokens_per_second": 16987.898
    },
    {
      "epoch": 0.13845321795565171,
      "grad_norm": 3.4295804500579834,
      "learning_rate": 3.456511942316359e-06,
      "loss": 4.8555,
      "num_input_tokens_seen": 26337856,
      "step": 768,
      "train_runtime": 1549.8633,
      "train_tokens_per_second": 16993.664
    },
    {
      "epoch": 0.1442221020371372,
      "grad_norm": 4.141047477722168,
      "learning_rate": 3.6007210455159986e-06,
      "loss": 4.8239,
      "num_input_tokens_seen": 27472256,
      "step": 800,
      "train_runtime": 1615.9022,
      "train_tokens_per_second": 17001.187
    },
    {
      "epoch": 0.1499909861186227,
      "grad_norm": 3.7499160766601562,
      "learning_rate": 3.7449301487156382e-06,
      "loss": 4.7552,
      "num_input_tokens_seen": 28547072,
      "step": 832,
      "train_runtime": 1679.3537,
      "train_tokens_per_second": 16998.844
    },
    {
      "epoch": 0.15575987020010817,
      "grad_norm": 3.7344698905944824,
      "learning_rate": 3.8891392519152775e-06,
      "loss": 4.8674,
      "num_input_tokens_seen": 29646880,
      "step": 864,
      "train_runtime": 1744.065,
      "train_tokens_per_second": 16998.725
    },
    {
      "epoch": 0.16152875428159366,
      "grad_norm": 4.560794353485107,
      "learning_rate": 4.033348355114917e-06,
      "loss": 4.8153,
      "num_input_tokens_seen": 30743648,
      "step": 896,
      "train_runtime": 1808.7644,
      "train_tokens_per_second": 16997.044
    },
    {
      "epoch": 0.16729763836307915,
      "grad_norm": 4.194084644317627,
      "learning_rate": 4.177557458314557e-06,
      "loss": 4.8068,
      "num_input_tokens_seen": 31872160,
      "step": 928,
      "train_runtime": 1875.1087,
      "train_tokens_per_second": 16997.5
    },
    {
      "epoch": 0.17306652244456464,
      "grad_norm": 4.296507835388184,
      "learning_rate": 4.321766561514196e-06,
      "loss": 4.8454,
      "num_input_tokens_seen": 32980000,
      "step": 960,
      "train_runtime": 1940.4075,
      "train_tokens_per_second": 16996.43
    },
    {
      "epoch": 0.17883540652605012,
      "grad_norm": 3.998467206954956,
      "learning_rate": 4.465975664713835e-06,
      "loss": 4.852,
      "num_input_tokens_seen": 34066336,
      "step": 992,
      "train_runtime": 2004.7378,
      "train_tokens_per_second": 16992.914
    },
    {
      "epoch": 0.1846042906075356,
      "grad_norm": 4.273850440979004,
      "learning_rate": 4.610184767913475e-06,
      "loss": 4.7888,
      "num_input_tokens_seen": 35167808,
      "step": 1024,
      "train_runtime": 2069.8559,
      "train_tokens_per_second": 16990.462
    },
    {
      "epoch": 0.1903731746890211,
      "grad_norm": 3.7928550243377686,
      "learning_rate": 4.7543938711131145e-06,
      "loss": 4.8127,
      "num_input_tokens_seen": 36238048,
      "step": 1056,
      "train_runtime": 2133.2352,
      "train_tokens_per_second": 16987.366
    },
    {
      "epoch": 0.19614205877050658,
      "grad_norm": 3.4497323036193848,
      "learning_rate": 4.898602974312754e-06,
      "loss": 4.7842,
      "num_input_tokens_seen": 37304992,
      "step": 1088,
      "train_runtime": 2196.6298,
      "train_tokens_per_second": 16982.831
    },
    {
      "epoch": 0.20191094285199207,
      "grad_norm": 4.375091552734375,
      "learning_rate": 5.042812077512394e-06,
      "loss": 4.7948,
      "num_input_tokens_seen": 38338240,
      "step": 1120,
      "train_runtime": 2258.5602,
      "train_tokens_per_second": 16974.637
    },
    {
      "epoch": 0.20767982693347756,
      "grad_norm": 3.937845468521118,
      "learning_rate": 5.187021180712033e-06,
      "loss": 4.7786,
      "num_input_tokens_seen": 39450304,
      "step": 1152,
      "train_runtime": 2324.219,
      "train_tokens_per_second": 16973.574
    },
    {
      "epoch": 0.21344871101496304,
      "grad_norm": 4.132176399230957,
      "learning_rate": 5.331230283911672e-06,
      "loss": 4.8321,
      "num_input_tokens_seen": 40492288,
      "step": 1184,
      "train_runtime": 2386.5128,
      "train_tokens_per_second": 16967.136
    },
    {
      "epoch": 0.21921759509644853,
      "grad_norm": 3.943791627883911,
      "learning_rate": 5.475439387111312e-06,
      "loss": 4.8296,
      "num_input_tokens_seen": 41588000,
      "step": 1216,
      "train_runtime": 2451.4894,
      "train_tokens_per_second": 16964.381
    },
    {
      "epoch": 0.22498647917793402,
      "grad_norm": 3.7507877349853516,
      "learning_rate": 5.619648490310952e-06,
      "loss": 4.8155,
      "num_input_tokens_seen": 42687936,
      "step": 1248,
      "train_runtime": 2516.4212,
      "train_tokens_per_second": 16963.748
    },
    {
      "epoch": 0.2307553632594195,
      "grad_norm": 4.0337233543396,
      "learning_rate": 5.763857593510591e-06,
      "loss": 4.8053,
      "num_input_tokens_seen": 43750432,
      "step": 1280,
      "train_runtime": 2579.6882,
      "train_tokens_per_second": 16959.582
    },
    {
      "epoch": 0.236524247340905,
      "grad_norm": 4.162980079650879,
      "learning_rate": 5.908066696710231e-06,
      "loss": 4.792,
      "num_input_tokens_seen": 44819648,
      "step": 1312,
      "train_runtime": 2643.1507,
      "train_tokens_per_second": 16956.902
    },
    {
      "epoch": 0.24229313142239048,
      "grad_norm": 3.636794090270996,
      "learning_rate": 6.0522757999098706e-06,
      "loss": 4.8388,
      "num_input_tokens_seen": 45882080,
      "step": 1344,
      "train_runtime": 2706.4061,
      "train_tokens_per_second": 16953.139
    },
    {
      "epoch": 0.24806201550387597,
      "grad_norm": 4.48288631439209,
      "learning_rate": 6.19648490310951e-06,
      "loss": 4.8452,
      "num_input_tokens_seen": 46961664,
      "step": 1376,
      "train_runtime": 2770.5284,
      "train_tokens_per_second": 16950.436
    },
    {
      "epoch": 0.2538308995853615,
      "grad_norm": 4.313292503356934,
      "learning_rate": 6.340694006309149e-06,
      "loss": 4.8764,
      "num_input_tokens_seen": 48032544,
      "step": 1408,
      "train_runtime": 2834.2698,
      "train_tokens_per_second": 16947.061
    },
    {
      "epoch": 0.25959978366684694,
      "grad_norm": 3.721163034439087,
      "learning_rate": 6.484903109508788e-06,
      "loss": 4.8751,
      "num_input_tokens_seen": 49084544,
      "step": 1440,
      "train_runtime": 2897.0781,
      "train_tokens_per_second": 16942.776
    },
    {
      "epoch": 0.26536866774833245,
      "grad_norm": 3.9273955821990967,
      "learning_rate": 6.6291122127084275e-06,
      "loss": 4.8224,
      "num_input_tokens_seen": 50138272,
      "step": 1472,
      "train_runtime": 2959.9793,
      "train_tokens_per_second": 16938.724
    },
    {
      "epoch": 0.2711375518298179,
      "grad_norm": 3.915921688079834,
      "learning_rate": 6.773321315908067e-06,
      "loss": 4.8102,
      "num_input_tokens_seen": 51188736,
      "step": 1504,
      "train_runtime": 3022.7651,
      "train_tokens_per_second": 16934.408
    },
    {
      "epoch": 0.27690643591130343,
      "grad_norm": 3.7535958290100098,
      "learning_rate": 6.917530419107707e-06,
      "loss": 4.803,
      "num_input_tokens_seen": 52300000,
      "step": 1536,
      "train_runtime": 3088.4971,
      "train_tokens_per_second": 16933.803
    },
    {
      "epoch": 0.2826753199927889,
      "grad_norm": 3.999202013015747,
      "learning_rate": 7.0617395223073456e-06,
      "loss": 4.8485,
      "num_input_tokens_seen": 53428512,
      "step": 1568,
      "train_runtime": 3154.9219,
      "train_tokens_per_second": 16934.971
    },
    {
      "epoch": 0.2884442040742744,
      "grad_norm": 3.8114895820617676,
      "learning_rate": 7.205948625506985e-06,
      "loss": 4.811,
      "num_input_tokens_seen": 54499136,
      "step": 1600,
      "train_runtime": 3218.185,
      "train_tokens_per_second": 16934.743
    },
    {
      "epoch": 0.29421308815575986,
      "grad_norm": 3.736361265182495,
      "learning_rate": 7.350157728706625e-06,
      "loss": 4.8051,
      "num_input_tokens_seen": 55587232,
      "step": 1632,
      "train_runtime": 3282.6875,
      "train_tokens_per_second": 16933.452
    },
    {
      "epoch": 0.2999819722372454,
      "grad_norm": 3.8385252952575684,
      "learning_rate": 7.4943668319062645e-06,
      "loss": 4.8022,
      "num_input_tokens_seen": 56660672,
      "step": 1664,
      "train_runtime": 3346.216,
      "train_tokens_per_second": 16932.76
    },
    {
      "epoch": 0.30575085631873083,
      "grad_norm": 3.8646862506866455,
      "learning_rate": 7.638575935105903e-06,
      "loss": 4.7968,
      "num_input_tokens_seen": 57746432,
      "step": 1696,
      "train_runtime": 3410.5612,
      "train_tokens_per_second": 16931.651
    },
    {
      "epoch": 0.31151974040021635,
      "grad_norm": 4.041764736175537,
      "learning_rate": 7.782785038305544e-06,
      "loss": 4.7993,
      "num_input_tokens_seen": 58846688,
      "step": 1728,
      "train_runtime": 3475.6977,
      "train_tokens_per_second": 16930.899
    },
    {
      "epoch": 0.3172886244817018,
      "grad_norm": 3.632789373397827,
      "learning_rate": 7.926994141505183e-06,
      "loss": 4.8302,
      "num_input_tokens_seen": 59934016,
      "step": 1760,
      "train_runtime": 3540.1883,
      "train_tokens_per_second": 16929.613
    },
    {
      "epoch": 0.3230575085631873,
      "grad_norm": 3.657254695892334,
      "learning_rate": 8.071203244704823e-06,
      "loss": 4.8768,
      "num_input_tokens_seen": 61042144,
      "step": 1792,
      "train_runtime": 3605.7036,
      "train_tokens_per_second": 16929.329
    },
    {
      "epoch": 0.3288263926446728,
      "grad_norm": 3.7339327335357666,
      "learning_rate": 8.215412347904462e-06,
      "loss": 4.8044,
      "num_input_tokens_seen": 62136224,
      "step": 1824,
      "train_runtime": 3670.649,
      "train_tokens_per_second": 16927.858
    },
    {
      "epoch": 0.3345952767261583,
      "grad_norm": 3.960662603378296,
      "learning_rate": 8.359621451104102e-06,
      "loss": 4.8134,
      "num_input_tokens_seen": 63228416,
      "step": 1856,
      "train_runtime": 3735.0534,
      "train_tokens_per_second": 16928.383
    },
    {
      "epoch": 0.34036416080764376,
      "grad_norm": 3.713329792022705,
      "learning_rate": 8.503830554303741e-06,
      "loss": 4.8485,
      "num_input_tokens_seen": 64306560,
      "step": 1888,
      "train_runtime": 3798.6697,
      "train_tokens_per_second": 16928.705
    },
    {
      "epoch": 0.34613304488912927,
      "grad_norm": 4.295469284057617,
      "learning_rate": 8.648039657503382e-06,
      "loss": 4.9178,
      "num_input_tokens_seen": 65443488,
      "step": 1920,
      "train_runtime": 3865.2027,
      "train_tokens_per_second": 16931.451
    },
    {
      "epoch": 0.35190192897061473,
      "grad_norm": 3.6211910247802734,
      "learning_rate": 8.79224876070302e-06,
      "loss": 4.8405,
      "num_input_tokens_seen": 66553440,
      "step": 1952,
      "train_runtime": 3930.3176,
      "train_tokens_per_second": 16933.349
    },
    {
      "epoch": 0.35767081305210024,
      "grad_norm": 3.715689182281494,
      "learning_rate": 8.93645786390266e-06,
      "loss": 4.8613,
      "num_input_tokens_seen": 67668800,
      "step": 1984,
      "train_runtime": 3995.5078,
      "train_tokens_per_second": 16936.22
    },
    {
      "epoch": 0.3634396971335857,
      "grad_norm": 4.023760795593262,
      "learning_rate": 9.0806669671023e-06,
      "loss": 4.8746,
      "num_input_tokens_seen": 68795968,
      "step": 2016,
      "train_runtime": 4061.5144,
      "train_tokens_per_second": 16938.502
    },
    {
      "epoch": 0.3692085812150712,
      "grad_norm": 3.920278787612915,
      "learning_rate": 9.224876070301939e-06,
      "loss": 4.8811,
      "num_input_tokens_seen": 69899712,
      "step": 2048,
      "train_runtime": 4126.3412,
      "train_tokens_per_second": 16939.877
    },
    {
      "epoch": 0.3749774652965567,
      "grad_norm": 3.7498061656951904,
      "learning_rate": 9.369085173501577e-06,
      "loss": 4.8027,
      "num_input_tokens_seen": 70996128,
      "step": 2080,
      "train_runtime": 4190.8402,
      "train_tokens_per_second": 16940.786
    },
    {
      "epoch": 0.3807463493780422,
      "grad_norm": 3.332735300064087,
      "learning_rate": 9.513294276701216e-06,
      "loss": 4.8308,
      "num_input_tokens_seen": 72108992,
      "step": 2112,
      "train_runtime": 4256.1985,
      "train_tokens_per_second": 16942.112
    },
    {
      "epoch": 0.38651523345952765,
      "grad_norm": 4.047556400299072,
      "learning_rate": 9.657503379900857e-06,
      "loss": 4.8727,
      "num_input_tokens_seen": 73180992,
      "step": 2144,
      "train_runtime": 4319.5757,
      "train_tokens_per_second": 16941.708
    },
    {
      "epoch": 0.39228411754101317,
      "grad_norm": 4.16868257522583,
      "learning_rate": 9.801712483100496e-06,
      "loss": 4.8091,
      "num_input_tokens_seen": 74258048,
      "step": 2176,
      "train_runtime": 4383.0951,
      "train_tokens_per_second": 16941.92
    },
    {
      "epoch": 0.3980530016224986,
      "grad_norm": 3.7123258113861084,
      "learning_rate": 9.945921586300136e-06,
      "loss": 4.9036,
      "num_input_tokens_seen": 75366272,
      "step": 2208,
      "train_runtime": 4448.1895,
      "train_tokens_per_second": 16943.134
    },
    {
      "epoch": 0.40382188570398414,
      "grad_norm": 4.073655605316162,
      "learning_rate": 1.0090130689499775e-05,
      "loss": 4.8157,
      "num_input_tokens_seen": 76469056,
      "step": 2240,
      "train_runtime": 4512.9861,
      "train_tokens_per_second": 16944.226
    },
    {
      "epoch": 0.4095907697854696,
      "grad_norm": 3.9840800762176514,
      "learning_rate": 1.0234339792699415e-05,
      "loss": 4.7941,
      "num_input_tokens_seen": 77511296,
      "step": 2272,
      "train_runtime": 4574.8299,
      "train_tokens_per_second": 16942.99
    },
    {
      "epoch": 0.4153596538669551,
      "grad_norm": 3.3156211376190186,
      "learning_rate": 1.0378548895899054e-05,
      "loss": 4.848,
      "num_input_tokens_seen": 78618880,
      "step": 2304,
      "train_runtime": 4639.7463,
      "train_tokens_per_second": 16944.651
    },
    {
      "epoch": 0.4211285379484406,
      "grad_norm": 3.843020439147949,
      "learning_rate": 1.0522757999098695e-05,
      "loss": 4.7969,
      "num_input_tokens_seen": 79669536,
      "step": 2336,
      "train_runtime": 4702.1354,
      "train_tokens_per_second": 16943.267
    },
    {
      "epoch": 0.4268974220299261,
      "grad_norm": 3.7805213928222656,
      "learning_rate": 1.0666967102298333e-05,
      "loss": 4.8718,
      "num_input_tokens_seen": 80717024,
      "step": 2368,
      "train_runtime": 4764.2437,
      "train_tokens_per_second": 16942.254
    },
    {
      "epoch": 0.43266630611141155,
      "grad_norm": 3.808891534805298,
      "learning_rate": 1.0811176205497972e-05,
      "loss": 4.861,
      "num_input_tokens_seen": 81795072,
      "step": 2400,
      "train_runtime": 4827.9035,
      "train_tokens_per_second": 16942.151
    },
    {
      "epoch": 0.43843519019289706,
      "grad_norm": 3.514460802078247,
      "learning_rate": 1.0955385308697613e-05,
      "loss": 4.7598,
      "num_input_tokens_seen": 82917408,
      "step": 2432,
      "train_runtime": 4893.6909,
      "train_tokens_per_second": 16943.736
    },
    {
      "epoch": 0.4442040742743826,
      "grad_norm": 3.9621224403381348,
      "learning_rate": 1.1099594411897252e-05,
      "loss": 4.8392,
      "num_input_tokens_seen": 84009152,
      "step": 2464,
      "train_runtime": 4957.9119,
      "train_tokens_per_second": 16944.463
    },
    {
      "epoch": 0.44997295835586804,
      "grad_norm": 3.864633560180664,
      "learning_rate": 1.1243803515096892e-05,
      "loss": 4.8729,
      "num_input_tokens_seen": 85103296,
      "step": 2496,
      "train_runtime": 5022.1778,
      "train_tokens_per_second": 16945.496
    },
    {
      "epoch": 0.45574184243735355,
      "grad_norm": 3.6659369468688965,
      "learning_rate": 1.1388012618296531e-05,
      "loss": 4.8257,
      "num_input_tokens_seen": 86188864,
      "step": 2528,
      "train_runtime": 5086.1691,
      "train_tokens_per_second": 16945.733
    },
    {
      "epoch": 0.461510726518839,
      "grad_norm": 3.6097567081451416,
      "learning_rate": 1.1532221721496171e-05,
      "loss": 4.9087,
      "num_input_tokens_seen": 87308096,
      "step": 2560,
      "train_runtime": 5151.6348,
      "train_tokens_per_second": 16947.649
    },
    {
      "epoch": 0.4672796106003245,
      "grad_norm": 3.6743857860565186,
      "learning_rate": 1.167643082469581e-05,
      "loss": 4.8216,
      "num_input_tokens_seen": 88402208,
      "step": 2592,
      "train_runtime": 5215.9193,
      "train_tokens_per_second": 16948.538
    },
    {
      "epoch": 0.47304849468181,
      "grad_norm": 3.224478006362915,
      "learning_rate": 1.1820639927895449e-05,
      "loss": 4.8156,
      "num_input_tokens_seen": 89467520,
      "step": 2624,
      "train_runtime": 5279.003,
      "train_tokens_per_second": 16947.806
    },
    {
      "epoch": 0.4788173787632955,
      "grad_norm": 3.8495960235595703,
      "learning_rate": 1.196484903109509e-05,
      "loss": 4.8522,
      "num_input_tokens_seen": 90579232,
      "step": 2656,
      "train_runtime": 5344.2381,
      "train_tokens_per_second": 16948.951
    },
    {
      "epoch": 0.48458626284478096,
      "grad_norm": 3.9781084060668945,
      "learning_rate": 1.2109058134294728e-05,
      "loss": 4.8394,
      "num_input_tokens_seen": 91664000,
      "step": 2688,
      "train_runtime": 5408.0985,
      "train_tokens_per_second": 16949.396
    },
    {
      "epoch": 0.49035514692626647,
      "grad_norm": 3.8197264671325684,
      "learning_rate": 1.2253267237494369e-05,
      "loss": 4.8487,
      "num_input_tokens_seen": 92773632,
      "step": 2720,
      "train_runtime": 5473.2921,
      "train_tokens_per_second": 16950.243
    },
    {
      "epoch": 0.49612403100775193,
      "grad_norm": 4.258028030395508,
      "learning_rate": 1.2397476340694008e-05,
      "loss": 4.8495,
      "num_input_tokens_seen": 93897664,
      "step": 2752,
      "train_runtime": 5539.1687,
      "train_tokens_per_second": 16951.581
    },
    {
      "epoch": 0.5018929150892374,
      "grad_norm": 3.8259599208831787,
      "learning_rate": 1.2541685443893648e-05,
      "loss": 4.8423,
      "num_input_tokens_seen": 94965984,
      "step": 2784,
      "train_runtime": 5602.5532,
      "train_tokens_per_second": 16950.483
    },
    {
      "epoch": 0.507661799170723,
      "grad_norm": 4.393985748291016,
      "learning_rate": 1.2685894547093287e-05,
      "loss": 4.8442,
      "num_input_tokens_seen": 96067840,
      "step": 2816,
      "train_runtime": 5667.2092,
      "train_tokens_per_second": 16951.525
    },
    {
      "epoch": 0.5134306832522084,
      "grad_norm": 4.463126182556152,
      "learning_rate": 1.2830103650292927e-05,
      "loss": 4.7761,
      "num_input_tokens_seen": 97152992,
      "step": 2848,
      "train_runtime": 5730.9443,
      "train_tokens_per_second": 16952.353
    },
    {
      "epoch": 0.5191995673336939,
      "grad_norm": 3.613311767578125,
      "learning_rate": 1.2974312753492566e-05,
      "loss": 4.9039,
      "num_input_tokens_seen": 98222656,
      "step": 2880,
      "train_runtime": 5793.8397,
      "train_tokens_per_second": 16952.947
    },
    {
      "epoch": 0.5249684514151793,
      "grad_norm": 3.62833833694458,
      "learning_rate": 1.3118521856692205e-05,
      "loss": 4.8649,
      "num_input_tokens_seen": 99343776,
      "step": 2912,
      "train_runtime": 5859.2284,
      "train_tokens_per_second": 16955.095
    },
    {
      "epoch": 0.5307373354966649,
      "grad_norm": 3.653722047805786,
      "learning_rate": 1.3262730959891845e-05,
      "loss": 4.8245,
      "num_input_tokens_seen": 100405760,
      "step": 2944,
      "train_runtime": 5921.7966,
      "train_tokens_per_second": 16955.287
    },
    {
      "epoch": 0.5365062195781504,
      "grad_norm": 4.200023651123047,
      "learning_rate": 1.3406940063091484e-05,
      "loss": 4.9109,
      "num_input_tokens_seen": 101537440,
      "step": 2976,
      "train_runtime": 5987.707,
      "train_tokens_per_second": 16957.65
    },
    {
      "epoch": 0.5422751036596358,
      "grad_norm": 3.765045642852783,
      "learning_rate": 1.3551149166291125e-05,
      "loss": 4.8547,
      "num_input_tokens_seen": 102607648,
      "step": 3008,
      "train_runtime": 6050.8074,
      "train_tokens_per_second": 16957.679
    },
    {
      "epoch": 0.5480439877411213,
      "grad_norm": 3.9172635078430176,
      "learning_rate": 1.3695358269490762e-05,
      "loss": 4.8726,
      "num_input_tokens_seen": 103672544,
      "step": 3040,
      "train_runtime": 6113.6261,
      "train_tokens_per_second": 16957.619
    },
    {
      "epoch": 0.5538128718226069,
      "grad_norm": 3.6448819637298584,
      "learning_rate": 1.38395673726904e-05,
      "loss": 4.8518,
      "num_input_tokens_seen": 104776704,
      "step": 3072,
      "train_runtime": 6178.3561,
      "train_tokens_per_second": 16958.67
    },
    {
      "epoch": 0.5595817559040923,
      "grad_norm": 4.124131679534912,
      "learning_rate": 1.3983776475890041e-05,
      "loss": 4.8733,
      "num_input_tokens_seen": 105830240,
      "step": 3104,
      "train_runtime": 6240.6616,
      "train_tokens_per_second": 16958.176
    },
    {
      "epoch": 0.5653506399855778,
      "grad_norm": 3.85402250289917,
      "learning_rate": 1.412798557908968e-05,
      "loss": 4.7869,
      "num_input_tokens_seen": 106959808,
      "step": 3136,
      "train_runtime": 6306.4718,
      "train_tokens_per_second": 16960.324
    },
    {
      "epoch": 0.5711195240670632,
      "grad_norm": 3.6787469387054443,
      "learning_rate": 1.427219468228932e-05,
      "loss": 4.7836,
      "num_input_tokens_seen": 108061728,
      "step": 3168,
      "train_runtime": 6370.954,
      "train_tokens_per_second": 16961.624
    },
    {
      "epoch": 0.5768884081485488,
      "grad_norm": 3.8257017135620117,
      "learning_rate": 1.441640378548896e-05,
      "loss": 4.8174,
      "num_input_tokens_seen": 109158368,
      "step": 3200,
      "train_runtime": 6435.1472,
      "train_tokens_per_second": 16962.839
    },
    {
      "epoch": 0.5826572922300343,
      "grad_norm": 3.448932409286499,
      "learning_rate": 1.4560612888688598e-05,
      "loss": 4.7953,
      "num_input_tokens_seen": 110253216,
      "step": 3232,
      "train_runtime": 6499.1442,
      "train_tokens_per_second": 16964.267
    },
    {
      "epoch": 0.5884261763115197,
      "grad_norm": 3.471904993057251,
      "learning_rate": 1.4704821991888239e-05,
      "loss": 4.815,
      "num_input_tokens_seen": 111313504,
      "step": 3264,
      "train_runtime": 6561.4677,
      "train_tokens_per_second": 16964.726
    },
    {
      "epoch": 0.5941950603930052,
      "grad_norm": 3.717144250869751,
      "learning_rate": 1.4849031095087877e-05,
      "loss": 4.913,
      "num_input_tokens_seen": 112450752,
      "step": 3296,
      "train_runtime": 6627.4842,
      "train_tokens_per_second": 16967.336
    },
    {
      "epoch": 0.5999639444744908,
      "grad_norm": 3.6922662258148193,
      "learning_rate": 1.4993240198287518e-05,
      "loss": 4.89,
      "num_input_tokens_seen": 113567392,
      "step": 3328,
      "train_runtime": 6692.683,
      "train_tokens_per_second": 16968.889
    },
    {
      "epoch": 0.6057328285559762,
      "grad_norm": 3.935722827911377,
      "learning_rate": 1.5137449301487157e-05,
      "loss": 4.7987,
      "num_input_tokens_seen": 114651328,
      "step": 3360,
      "train_runtime": 6756.2433,
      "train_tokens_per_second": 16969.686
    },
    {
      "epoch": 0.6115017126374617,
      "grad_norm": 4.094447135925293,
      "learning_rate": 1.5281658404686797e-05,
      "loss": 4.8519,
      "num_input_tokens_seen": 115722304,
      "step": 3392,
      "train_runtime": 6819.1985,
      "train_tokens_per_second": 16970.074
    },
    {
      "epoch": 0.6172705967189471,
      "grad_norm": 3.6364219188690186,
      "learning_rate": 1.5425867507886438e-05,
      "loss": 4.8534,
      "num_input_tokens_seen": 116797856,
      "step": 3424,
      "train_runtime": 6882.4616,
      "train_tokens_per_second": 16970.361
    },
    {
      "epoch": 0.6230394808004327,
      "grad_norm": 3.423572063446045,
      "learning_rate": 1.5570076611086075e-05,
      "loss": 4.9249,
      "num_input_tokens_seen": 117938240,
      "step": 3456,
      "train_runtime": 6948.6872,
      "train_tokens_per_second": 16972.737
    },
    {
      "epoch": 0.6288083648819182,
      "grad_norm": 3.996164560317993,
      "learning_rate": 1.5714285714285715e-05,
      "loss": 4.779,
      "num_input_tokens_seen": 119038784,
      "step": 3488,
      "train_runtime": 7013.0814,
      "train_tokens_per_second": 16973.82
    },
    {
      "epoch": 0.6345772489634036,
      "grad_norm": 4.0297532081604,
      "learning_rate": 1.5858494817485356e-05,
      "loss": 4.8842,
      "num_input_tokens_seen": 120137120,
      "step": 3520,
      "train_runtime": 7077.4013,
      "train_tokens_per_second": 16974.75
    },
    {
      "epoch": 0.6403461330448891,
      "grad_norm": 3.9042210578918457,
      "learning_rate": 1.6002703920684993e-05,
      "loss": 4.8148,
      "num_input_tokens_seen": 121244512,
      "step": 3552,
      "train_runtime": 7142.1308,
      "train_tokens_per_second": 16975.958
    },
    {
      "epoch": 0.6461150171263746,
      "grad_norm": 4.215262413024902,
      "learning_rate": 1.6146913023884633e-05,
      "loss": 4.8728,
      "num_input_tokens_seen": 122323328,
      "step": 3584,
      "train_runtime": 7205.5953,
      "train_tokens_per_second": 16976.158
    },
    {
      "epoch": 0.6518839012078601,
      "grad_norm": 3.9923758506774902,
      "learning_rate": 1.6291122127084274e-05,
      "loss": 4.8996,
      "num_input_tokens_seen": 123457920,
      "step": 3616,
      "train_runtime": 7271.7804,
      "train_tokens_per_second": 16977.674
    },
    {
      "epoch": 0.6576527852893456,
      "grad_norm": 4.360619068145752,
      "learning_rate": 1.6435331230283914e-05,
      "loss": 4.8615,
      "num_input_tokens_seen": 124555840,
      "step": 3648,
      "train_runtime": 7336.1789,
      "train_tokens_per_second": 16978.299
    },
    {
      "epoch": 0.6634216693708311,
      "grad_norm": 3.7114012241363525,
      "learning_rate": 1.657954033348355e-05,
      "loss": 4.8744,
      "num_input_tokens_seen": 125677056,
      "step": 3680,
      "train_runtime": 7401.6309,
      "train_tokens_per_second": 16979.644
    },
    {
      "epoch": 0.6691905534523166,
      "grad_norm": 3.9286558628082275,
      "learning_rate": 1.6723749436683192e-05,
      "loss": 4.879,
      "num_input_tokens_seen": 126749056,
      "step": 3712,
      "train_runtime": 7464.8509,
      "train_tokens_per_second": 16979.449
    },
    {
      "epoch": 0.674959437533802,
      "grad_norm": 3.7468297481536865,
      "learning_rate": 1.6867958539882833e-05,
      "loss": 4.8598,
      "num_input_tokens_seen": 127869760,
      "step": 3744,
      "train_runtime": 7530.3725,
      "train_tokens_per_second": 16980.536
    },
    {
      "epoch": 0.6807283216152875,
      "grad_norm": 3.426130771636963,
      "learning_rate": 1.701216764308247e-05,
      "loss": 4.9228,
      "num_input_tokens_seen": 128955616,
      "step": 3776,
      "train_runtime": 7594.1736,
      "train_tokens_per_second": 16980.862
    },
    {
      "epoch": 0.6864972056967731,
      "grad_norm": 6.526918411254883,
      "learning_rate": 1.715637674628211e-05,
      "loss": 4.8344,
      "num_input_tokens_seen": 130047584,
      "step": 3808,
      "train_runtime": 7658.3325,
      "train_tokens_per_second": 16981.188
    },
    {
      "epoch": 0.6922660897782585,
      "grad_norm": 3.964672088623047,
      "learning_rate": 1.730058584948175e-05,
      "loss": 4.855,
      "num_input_tokens_seen": 131169728,
      "step": 3840,
      "train_runtime": 7723.8118,
      "train_tokens_per_second": 16982.512
    },
    {
      "epoch": 0.698034973859744,
      "grad_norm": 3.922776699066162,
      "learning_rate": 1.744479495268139e-05,
      "loss": 4.8553,
      "num_input_tokens_seen": 132228960,
      "step": 3872,
      "train_runtime": 7786.3216,
      "train_tokens_per_second": 16982.212
    },
    {
      "epoch": 0.7038038579412295,
      "grad_norm": 3.861614465713501,
      "learning_rate": 1.7589004055881028e-05,
      "loss": 4.7963,
      "num_input_tokens_seen": 133323040,
      "step": 3904,
      "train_runtime": 7850.5387,
      "train_tokens_per_second": 16982.661
    },
    {
      "epoch": 0.709572742022715,
      "grad_norm": 3.85001802444458,
      "learning_rate": 1.773321315908067e-05,
      "loss": 4.8552,
      "num_input_tokens_seen": 134407360,
      "step": 3936,
      "train_runtime": 7914.3417,
      "train_tokens_per_second": 16982.759
    },
    {
      "epoch": 0.7153416261042005,
      "grad_norm": 3.992096424102783,
      "learning_rate": 1.787742226228031e-05,
      "loss": 4.9088,
      "num_input_tokens_seen": 135494272,
      "step": 3968,
      "train_runtime": 7978.0363,
      "train_tokens_per_second": 16983.411
    },
    {
      "epoch": 0.721110510185686,
      "grad_norm": 3.853675365447998,
      "learning_rate": 1.8021631365479946e-05,
      "loss": 4.8283,
      "num_input_tokens_seen": 136598912,
      "step": 4000,
      "train_runtime": 8042.4814,
      "train_tokens_per_second": 16984.673
    },
    {
      "epoch": 0.7268793942671714,
      "grad_norm": 4.056074142456055,
      "learning_rate": 1.8165840468679587e-05,
      "loss": 4.8542,
      "num_input_tokens_seen": 137659392,
      "step": 4032,
      "train_runtime": 8104.8037,
      "train_tokens_per_second": 16984.914
    },
    {
      "epoch": 0.732648278348657,
      "grad_norm": 3.5059404373168945,
      "learning_rate": 1.8310049571879227e-05,
      "loss": 4.8664,
      "num_input_tokens_seen": 138717856,
      "step": 4064,
      "train_runtime": 8167.5424,
      "train_tokens_per_second": 16984.039
    },
    {
      "epoch": 0.7384171624301424,
      "grad_norm": 3.501187801361084,
      "learning_rate": 1.8454258675078868e-05,
      "loss": 4.8664,
      "num_input_tokens_seen": 139812064,
      "step": 4096,
      "train_runtime": 8232.0276,
      "train_tokens_per_second": 16983.916
    },
    {
      "epoch": 0.7441860465116279,
      "grad_norm": 3.441481113433838,
      "learning_rate": 1.8598467778278505e-05,
      "loss": 4.8596,
      "num_input_tokens_seen": 140935168,
      "step": 4128,
      "train_runtime": 8297.8266,
      "train_tokens_per_second": 16984.588
    },
    {
      "epoch": 0.7499549305931134,
      "grad_norm": 3.2332985401153564,
      "learning_rate": 1.8742676881478145e-05,
      "loss": 4.85,
      "num_input_tokens_seen": 141980480,
      "step": 4160,
      "train_runtime": 8359.6316,
      "train_tokens_per_second": 16984.059
    },
    {
      "epoch": 0.7557238146745989,
      "grad_norm": 3.6136276721954346,
      "learning_rate": 1.8886885984677786e-05,
      "loss": 4.8018,
      "num_input_tokens_seen": 143039552,
      "step": 4192,
      "train_runtime": 8422.4818,
      "train_tokens_per_second": 16983.065
    },
    {
      "epoch": 0.7614926987560844,
      "grad_norm": 3.3986711502075195,
      "learning_rate": 1.9031095087877426e-05,
      "loss": 4.811,
      "num_input_tokens_seen": 144118016,
      "step": 4224,
      "train_runtime": 8486.0306,
      "train_tokens_per_second": 16982.972
    },
    {
      "epoch": 0.7672615828375698,
      "grad_norm": 3.7936530113220215,
      "learning_rate": 1.9175304191077064e-05,
      "loss": 4.9081,
      "num_input_tokens_seen": 145197728,
      "step": 4256,
      "train_runtime": 8549.9126,
      "train_tokens_per_second": 16982.364
    },
    {
      "epoch": 0.7730304669190553,
      "grad_norm": 3.9310429096221924,
      "learning_rate": 1.9319513294276704e-05,
      "loss": 4.85,
      "num_input_tokens_seen": 146319872,
      "step": 4288,
      "train_runtime": 8615.4258,
      "train_tokens_per_second": 16983.475
    },
    {
      "epoch": 0.7787993510005409,
      "grad_norm": 3.426068067550659,
      "learning_rate": 1.946372239747634e-05,
      "loss": 4.871,
      "num_input_tokens_seen": 147439360,
      "step": 4320,
      "train_runtime": 8680.8004,
      "train_tokens_per_second": 16984.535
    },
    {
      "epoch": 0.7845682350820263,
      "grad_norm": 3.288440465927124,
      "learning_rate": 1.960793150067598e-05,
      "loss": 4.8927,
      "num_input_tokens_seen": 148550720,
      "step": 4352,
      "train_runtime": 8746.1104,
      "train_tokens_per_second": 16984.775
    },
    {
      "epoch": 0.7903371191635118,
      "grad_norm": 3.4574618339538574,
      "learning_rate": 1.975214060387562e-05,
      "loss": 4.8964,
      "num_input_tokens_seen": 149648992,
      "step": 4384,
      "train_runtime": 8810.8271,
      "train_tokens_per_second": 16984.67
    },
    {
      "epoch": 0.7961060032449973,
      "grad_norm": 4.888607025146484,
      "learning_rate": 1.989634970707526e-05,
      "loss": 4.8644,
      "num_input_tokens_seen": 150708704,
      "step": 4416,
      "train_runtime": 8873.3872,
      "train_tokens_per_second": 16984.349
    },
    {
      "epoch": 0.8018748873264828,
      "grad_norm": 3.5967752933502197,
      "learning_rate": 1.999999749399389e-05,
      "loss": 4.861,
      "num_input_tokens_seen": 151815680,
      "step": 4448,
      "train_runtime": 8938.238,
      "train_tokens_per_second": 16984.967
    },
    {
      "epoch": 0.8076437714079683,
      "grad_norm": 3.8835184574127197,
      "learning_rate": 1.999994799268158e-05,
      "loss": 4.8563,
      "num_input_tokens_seen": 152942208,
      "step": 4480,
      "train_runtime": 9003.9441,
      "train_tokens_per_second": 16986.135
    },
    {
      "epoch": 0.8134126554894537,
      "grad_norm": 3.721308708190918,
      "learning_rate": 1.9999835129994926e-05,
      "loss": 4.9136,
      "num_input_tokens_seen": 154033664,
      "step": 4512,
      "train_runtime": 9068.0401,
      "train_tokens_per_second": 16986.434
    },
    {
      "epoch": 0.8191815395709392,
      "grad_norm": 3.9621145725250244,
      "learning_rate": 1.9999658906649047e-05,
      "loss": 4.8437,
      "num_input_tokens_seen": 155050912,
      "step": 4544,
      "train_runtime": 9128.4168,
      "train_tokens_per_second": 16985.521
    },
    {
      "epoch": 0.8249504236524248,
      "grad_norm": 3.7863333225250244,
      "learning_rate": 1.999941932376052e-05,
      "loss": 4.8476,
      "num_input_tokens_seen": 156143168,
      "step": 4576,
      "train_runtime": 9192.5734,
      "train_tokens_per_second": 16985.795
    },
    {
      "epoch": 0.8307193077339102,
      "grad_norm": 3.5559887886047363,
      "learning_rate": 1.9999116382847393e-05,
      "loss": 4.8704,
      "num_input_tokens_seen": 157214528,
      "step": 4608,
      "train_runtime": 9256.0109,
      "train_tokens_per_second": 16985.128
    },
    {
      "epoch": 0.8364881918153957,
      "grad_norm": 3.993807077407837,
      "learning_rate": 1.999875008582914e-05,
      "loss": 4.841,
      "num_input_tokens_seen": 158303584,
      "step": 4640,
      "train_runtime": 9320.3597,
      "train_tokens_per_second": 16984.708
    },
    {
      "epoch": 0.8422570758968811,
      "grad_norm": 4.25840425491333,
      "learning_rate": 1.9998320435026685e-05,
      "loss": 4.8342,
      "num_input_tokens_seen": 159382048,
      "step": 4672,
      "train_runtime": 9384.0919,
      "train_tokens_per_second": 16984.28
    },
    {
      "epoch": 0.8480259599783667,
      "grad_norm": 3.4912567138671875,
      "learning_rate": 1.9997827433162374e-05,
      "loss": 4.896,
      "num_input_tokens_seen": 160449312,
      "step": 4704,
      "train_runtime": 9447.1459,
      "train_tokens_per_second": 16983.893
    },
    {
      "epoch": 0.8537948440598522,
      "grad_norm": 3.7736403942108154,
      "learning_rate": 1.999727108335994e-05,
      "loss": 4.898,
      "num_input_tokens_seen": 161565952,
      "step": 4736,
      "train_runtime": 9512.5568,
      "train_tokens_per_second": 16984.493
    },
    {
      "epoch": 0.8595637281413376,
      "grad_norm": 3.6171774864196777,
      "learning_rate": 1.999665138914452e-05,
      "loss": 4.85,
      "num_input_tokens_seen": 162675104,
      "step": 4768,
      "train_runtime": 9577.1699,
      "train_tokens_per_second": 16985.718
    },
    {
      "epoch": 0.8653326122228231,
      "grad_norm": 3.9216480255126953,
      "learning_rate": 1.9995968354442602e-05,
      "loss": 4.8757,
      "num_input_tokens_seen": 163729024,
      "step": 4800,
      "train_runtime": 9639.6954,
      "train_tokens_per_second": 16984.875
    },
    {
      "epoch": 0.8711014963043087,
      "grad_norm": 3.1727004051208496,
      "learning_rate": 1.9995221983582003e-05,
      "loss": 4.8501,
      "num_input_tokens_seen": 164820032,
      "step": 4832,
      "train_runtime": 9703.8023,
      "train_tokens_per_second": 16985.098
    },
    {
      "epoch": 0.8768703803857941,
      "grad_norm": 3.5492935180664062,
      "learning_rate": 1.9994412281291858e-05,
      "loss": 4.8053,
      "num_input_tokens_seen": 165871552,
      "step": 4864,
      "train_runtime": 9766.1869,
      "train_tokens_per_second": 16984.27
    },
    {
      "epoch": 0.8826392644672796,
      "grad_norm": 3.5912294387817383,
      "learning_rate": 1.9993539252702584e-05,
      "loss": 4.844,
      "num_input_tokens_seen": 166958880,
      "step": 4896,
      "train_runtime": 9830.402,
      "train_tokens_per_second": 16983.932
    },
    {
      "epoch": 0.8884081485487652,
      "grad_norm": 3.436453342437744,
      "learning_rate": 1.9992602903345837e-05,
      "loss": 4.8341,
      "num_input_tokens_seen": 167996064,
      "step": 4928,
      "train_runtime": 9892.2067,
      "train_tokens_per_second": 16982.668
    },
    {
      "epoch": 0.8941770326302506,
      "grad_norm": 3.947066068649292,
      "learning_rate": 1.9991603239154482e-05,
      "loss": 4.8211,
      "num_input_tokens_seen": 169080800,
      "step": 4960,
      "train_runtime": 9956.0752,
      "train_tokens_per_second": 16982.676
    },
    {
      "epoch": 0.8999459167117361,
      "grad_norm": 3.3316235542297363,
      "learning_rate": 1.9990540266462566e-05,
      "loss": 4.8353,
      "num_input_tokens_seen": 170198048,
      "step": 4992,
      "train_runtime": 10021.3766,
      "train_tokens_per_second": 16983.5
    },
    {
      "epoch": 0.9057148007932215,
      "grad_norm": 3.797623634338379,
      "learning_rate": 1.998941399200526e-05,
      "loss": 4.8709,
      "num_input_tokens_seen": 171297568,
      "step": 5024,
      "train_runtime": 10085.8436,
      "train_tokens_per_second": 16983.96
    },
    {
      "epoch": 0.9114836848747071,
      "grad_norm": 3.967582941055298,
      "learning_rate": 1.9988224422918836e-05,
      "loss": 4.8371,
      "num_input_tokens_seen": 172401056,
      "step": 5056,
      "train_runtime": 10150.4139,
      "train_tokens_per_second": 16984.633
    },
    {
      "epoch": 0.9172525689561926,
      "grad_norm": 3.485846757888794,
      "learning_rate": 1.9986971566740605e-05,
      "loss": 4.824,
      "num_input_tokens_seen": 173527616,
      "step": 5088,
      "train_runtime": 10215.987,
      "train_tokens_per_second": 16985.889
    },
    {
      "epoch": 0.923021453037678,
      "grad_norm": 3.397831678390503,
      "learning_rate": 1.9985655431408877e-05,
      "loss": 4.8511,
      "num_input_tokens_seen": 174596896,
      "step": 5120,
      "train_runtime": 10278.9532,
      "train_tokens_per_second": 16985.864
    },
    {
      "epoch": 0.9287903371191635,
      "grad_norm": 3.5220389366149902,
      "learning_rate": 1.998427602526291e-05,
      "loss": 4.8798,
      "num_input_tokens_seen": 175700864,
      "step": 5152,
      "train_runtime": 10343.6191,
      "train_tokens_per_second": 16986.401
    },
    {
      "epoch": 0.934559221200649,
      "grad_norm": 3.3263018131256104,
      "learning_rate": 1.998283335704285e-05,
      "loss": 4.8739,
      "num_input_tokens_seen": 176796672,
      "step": 5184,
      "train_runtime": 10407.8666,
      "train_tokens_per_second": 16986.831
    },
    {
      "epoch": 0.9403281052821345,
      "grad_norm": 3.433643102645874,
      "learning_rate": 1.9981327435889702e-05,
      "loss": 4.7794,
      "num_input_tokens_seen": 177861728,
      "step": 5216,
      "train_runtime": 10470.7968,
      "train_tokens_per_second": 16986.456
    },
    {
      "epoch": 0.94609698936362,
      "grad_norm": 4.29701566696167,
      "learning_rate": 1.9979758271345232e-05,
      "loss": 4.9041,
      "num_input_tokens_seen": 178977280,
      "step": 5248,
      "train_runtime": 10536.1549,
      "train_tokens_per_second": 16986.964
    },
    {
      "epoch": 0.9518658734451054,
      "grad_norm": 3.6963319778442383,
      "learning_rate": 1.9978125873351927e-05,
      "loss": 4.8428,
      "num_input_tokens_seen": 180058688,
      "step": 5280,
      "train_runtime": 10599.9046,
      "train_tokens_per_second": 16986.822
    },
    {
      "epoch": 0.957634757526591,
      "grad_norm": 4.15185022354126,
      "learning_rate": 1.997643025225295e-05,
      "loss": 4.8889,
      "num_input_tokens_seen": 181156032,
      "step": 5312,
      "train_runtime": 10664.3743,
      "train_tokens_per_second": 16987.029
    },
    {
      "epoch": 0.9634036416080765,
      "grad_norm": 3.7578907012939453,
      "learning_rate": 1.997467141879204e-05,
      "loss": 4.835,
      "num_input_tokens_seen": 182233440,
      "step": 5344,
      "train_runtime": 10727.7794,
      "train_tokens_per_second": 16987.061
    },
    {
      "epoch": 0.9691725256895619,
      "grad_norm": 3.650197744369507,
      "learning_rate": 1.9972849384113467e-05,
      "loss": 4.8519,
      "num_input_tokens_seen": 183353152,
      "step": 5376,
      "train_runtime": 10793.2292,
      "train_tokens_per_second": 16987.794
    },
    {
      "epoch": 0.9749414097710474,
      "grad_norm": 3.447173595428467,
      "learning_rate": 1.997096415976195e-05,
      "loss": 4.7588,
      "num_input_tokens_seen": 184405568,
      "step": 5408,
      "train_runtime": 10855.4667,
      "train_tokens_per_second": 16987.346
    },
    {
      "epoch": 0.9807102938525329,
      "grad_norm": 3.180900812149048,
      "learning_rate": 1.9969015757682592e-05,
      "loss": 4.8235,
      "num_input_tokens_seen": 185491520,
      "step": 5440,
      "train_runtime": 10919.3376,
      "train_tokens_per_second": 16987.433
    },
    {
      "epoch": 0.9864791779340184,
      "grad_norm": 3.501553773880005,
      "learning_rate": 1.9967004190220806e-05,
      "loss": 4.8704,
      "num_input_tokens_seen": 186629600,
      "step": 5472,
      "train_runtime": 10985.7881,
      "train_tokens_per_second": 16988.276
    },
    {
      "epoch": 0.9922480620155039,
      "grad_norm": 3.282925605773926,
      "learning_rate": 1.996492947012222e-05,
      "loss": 4.7898,
      "num_input_tokens_seen": 187717504,
      "step": 5504,
      "train_runtime": 11049.7774,
      "train_tokens_per_second": 16988.352
    },
    {
      "epoch": 0.9980169460969893,
      "grad_norm": 3.1253163814544678,
      "learning_rate": 1.996279161053262e-05,
      "loss": 4.8257,
      "num_input_tokens_seen": 188843136,
      "step": 5536,
      "train_runtime": 11115.3738,
      "train_tokens_per_second": 16989.364
    },
    {
      "epoch": 1.0,
      "eval_accuracy": 0.015967114021037333,
      "eval_class_accuracy": 0.46712114593305315,
      "eval_exact_token_accuracy": 0.0,
      "eval_f1": 0.031394168173780296,
      "eval_loss": 2.73325514793396,
      "eval_mean_iou": 0.14481354450589734,
      "eval_mean_iou_25": 0.14481354450589734,
      "eval_precision": 0.03278260308023382,
      "eval_prediction_target_similarity": 7.553852477357928e-05,
      "eval_recall": 0.030121253801006467,
      "eval_recall_25": 0.0780937747749746,
      "eval_runtime": 1277.9982,
      "eval_samples_per_second": 3.875,
      "eval_sequence_accuracy": 0.0,
      "eval_steps_per_second": 0.121,
      "eval_top3_token_accuracy": 0.05340932309627533,
      "eval_top5_token_accuracy": 0.08841782808303833,
      "num_input_tokens_seen": 189185350,
      "step": 5547
    },
    {
      "epoch": 1.0037858301784748,
      "grad_norm": 3.841376781463623,
      "learning_rate": 1.9960590624997842e-05,
      "loss": 4.8158,
      "num_input_tokens_seen": 189894502,
      "step": 5568,
      "train_runtime": 12457.4604,
      "train_tokens_per_second": 15243.436
    },
    {
      "epoch": 1.0095547142599604,
      "grad_norm": 3.9971373081207275,
      "learning_rate": 1.995832652746371e-05,
      "loss": 4.8586,
      "num_input_tokens_seen": 190968838,
      "step": 5600,
      "train_runtime": 12520.4429,
      "train_tokens_per_second": 15252.562
    },
    {
      "epoch": 1.015323598341446,
      "grad_norm": 3.317399263381958,
      "learning_rate": 1.9955999332275932e-05,
      "loss": 4.8722,
      "num_input_tokens_seen": 192026726,
      "step": 5632,
      "train_runtime": 12582.6255,
      "train_tokens_per_second": 15261.261
    },
    {
      "epoch": 1.0210924824229313,
      "grad_norm": 3.8126466274261475,
      "learning_rate": 1.995360905418001e-05,
      "loss": 4.8085,
      "num_input_tokens_seen": 193126022,
      "step": 5664,
      "train_runtime": 12647.011,
      "train_tokens_per_second": 15270.487
    },
    {
      "epoch": 1.0268613665044168,
      "grad_norm": 3.254185438156128,
      "learning_rate": 1.995115570832115e-05,
      "loss": 4.7269,
      "num_input_tokens_seen": 194179430,
      "step": 5696,
      "train_runtime": 12708.9302,
      "train_tokens_per_second": 15278.975
    },
    {
      "epoch": 1.0326302505859022,
      "grad_norm": 3.7427756786346436,
      "learning_rate": 1.994863931024418e-05,
      "loss": 4.8168,
      "num_input_tokens_seen": 195308166,
      "step": 5728,
      "train_runtime": 12774.3757,
      "train_tokens_per_second": 15289.058
    },
    {
      "epoch": 1.0383991346673878,
      "grad_norm": 3.6290676593780518,
      "learning_rate": 1.9946059875893423e-05,
      "loss": 4.8136,
      "num_input_tokens_seen": 196380902,
      "step": 5760,
      "train_runtime": 12837.2153,
      "train_tokens_per_second": 15297.781
    },
    {
      "epoch": 1.0441680187488733,
      "grad_norm": 3.825223207473755,
      "learning_rate": 1.994341742161261e-05,
      "loss": 4.7722,
      "num_input_tokens_seen": 197483270,
      "step": 5792,
      "train_runtime": 12901.522,
      "train_tokens_per_second": 15306.975
    },
    {
      "epoch": 1.0499369028303587,
      "grad_norm": 3.367959976196289,
      "learning_rate": 1.9940711964144787e-05,
      "loss": 4.8608,
      "num_input_tokens_seen": 198532486,
      "step": 5824,
      "train_runtime": 12963.2371,
      "train_tokens_per_second": 15315.039
    },
    {
      "epoch": 1.0557057869118442,
      "grad_norm": 4.943643569946289,
      "learning_rate": 1.9937943520632188e-05,
      "loss": 4.8072,
      "num_input_tokens_seen": 199599238,
      "step": 5856,
      "train_runtime": 13025.8083,
      "train_tokens_per_second": 15323.367
    },
    {
      "epoch": 1.0614746709933298,
      "grad_norm": 3.399486541748047,
      "learning_rate": 1.9935112108616147e-05,
      "loss": 4.8247,
      "num_input_tokens_seen": 200677510,
      "step": 5888,
      "train_runtime": 13089.0072,
      "train_tokens_per_second": 15331.759
    },
    {
      "epoch": 1.0672435550748152,
      "grad_norm": 3.6218137741088867,
      "learning_rate": 1.993221774603697e-05,
      "loss": 4.8511,
      "num_input_tokens_seen": 201800038,
      "step": 5920,
      "train_runtime": 13154.2059,
      "train_tokens_per_second": 15341.104
    },
    {
      "epoch": 1.0730124391563007,
      "grad_norm": 3.3650333881378174,
      "learning_rate": 1.9929260451233838e-05,
      "loss": 4.7921,
      "num_input_tokens_seen": 202907366,
      "step": 5952,
      "train_runtime": 13218.6324,
      "train_tokens_per_second": 15350.103
    },
    {
      "epoch": 1.078781323237786,
      "grad_norm": 3.5959296226501465,
      "learning_rate": 1.9926240242944664e-05,
      "loss": 4.8359,
      "num_input_tokens_seen": 204011878,
      "step": 5984,
      "train_runtime": 13282.991,
      "train_tokens_per_second": 15358.881
    },
    {
      "epoch": 1.0845502073192717,
      "grad_norm": 3.400754451751709,
      "learning_rate": 1.992315714030601e-05,
      "loss": 4.8569,
      "num_input_tokens_seen": 205152902,
      "step": 6016,
      "train_runtime": 13349.4126,
      "train_tokens_per_second": 15367.935
    },
    {
      "epoch": 1.0903190914007572,
      "grad_norm": 4.987976551055908,
      "learning_rate": 1.9920011162852935e-05,
      "loss": 4.9142,
      "num_input_tokens_seen": 206285446,
      "step": 6048,
      "train_runtime": 13415.4361,
      "train_tokens_per_second": 15376.723
    },
    {
      "epoch": 1.0960879754822426,
      "grad_norm": 3.3017992973327637,
      "learning_rate": 1.9916802330518898e-05,
      "loss": 4.8926,
      "num_input_tokens_seen": 207374246,
      "step": 6080,
      "train_runtime": 13479.0919,
      "train_tokens_per_second": 15384.883
    },
    {
      "epoch": 1.1018568595637281,
      "grad_norm": 3.483048915863037,
      "learning_rate": 1.9913530663635593e-05,
      "loss": 4.8415,
      "num_input_tokens_seen": 208448838,
      "step": 6112,
      "train_runtime": 13542.2925,
      "train_tokens_per_second": 15392.434
    },
    {
      "epoch": 1.1076257436452137,
      "grad_norm": 3.51377272605896,
      "learning_rate": 1.991019618293287e-05,
      "loss": 4.8988,
      "num_input_tokens_seen": 209524390,
      "step": 6144,
      "train_runtime": 13605.5942,
      "train_tokens_per_second": 15399.871
    },
    {
      "epoch": 1.113394627726699,
      "grad_norm": 3.503357410430908,
      "learning_rate": 1.9906798909538566e-05,
      "loss": 4.8223,
      "num_input_tokens_seen": 210615718,
      "step": 6176,
      "train_runtime": 13669.4003,
      "train_tokens_per_second": 15407.824
    },
    {
      "epoch": 1.1191635118081846,
      "grad_norm": 3.3150854110717773,
      "learning_rate": 1.9903338864978377e-05,
      "loss": 4.8096,
      "num_input_tokens_seen": 211704134,
      "step": 6208,
      "train_runtime": 13732.5035,
      "train_tokens_per_second": 15416.281
    },
    {
      "epoch": 1.1249323958896702,
      "grad_norm": 5.2570953369140625,
      "learning_rate": 1.989981607117574e-05,
      "loss": 4.8456,
      "num_input_tokens_seen": 212831526,
      "step": 6240,
      "train_runtime": 13797.7447,
      "train_tokens_per_second": 15425.095
    },
    {
      "epoch": 1.1307012799711555,
      "grad_norm": 3.31638240814209,
      "learning_rate": 1.9896230550451675e-05,
      "loss": 4.8537,
      "num_input_tokens_seen": 213915846,
      "step": 6272,
      "train_runtime": 13861.1004,
      "train_tokens_per_second": 15432.818
    },
    {
      "epoch": 1.1364701640526411,
      "grad_norm": 3.3040647506713867,
      "learning_rate": 1.9892582325524656e-05,
      "loss": 4.8376,
      "num_input_tokens_seen": 215009670,
      "step": 6304,
      "train_runtime": 13925.0296,
      "train_tokens_per_second": 15440.518
    },
    {
      "epoch": 1.1422390481341265,
      "grad_norm": 3.466109037399292,
      "learning_rate": 1.9888871419510454e-05,
      "loss": 4.8295,
      "num_input_tokens_seen": 216068518,
      "step": 6336,
      "train_runtime": 13987.4821,
      "train_tokens_per_second": 15447.277
    },
    {
      "epoch": 1.148007932215612,
      "grad_norm": 3.583068370819092,
      "learning_rate": 1.9885097855922e-05,
      "loss": 4.8034,
      "num_input_tokens_seen": 217172134,
      "step": 6368,
      "train_runtime": 14052.0973,
      "train_tokens_per_second": 15454.784
    },
    {
      "epoch": 1.1537768162970976,
      "grad_norm": 3.567913055419922,
      "learning_rate": 1.988126165866924e-05,
      "loss": 4.8644,
      "num_input_tokens_seen": 218259238,
      "step": 6400,
      "train_runtime": 14116.0531,
      "train_tokens_per_second": 15461.775
    },
    {
      "epoch": 1.159545700378583,
      "grad_norm": 4.005767345428467,
      "learning_rate": 1.9877362852058973e-05,
      "loss": 4.8051,
      "num_input_tokens_seen": 219361606,
      "step": 6432,
      "train_runtime": 14180.5034,
      "train_tokens_per_second": 15469.24
    },
    {
      "epoch": 1.1653145844600685,
      "grad_norm": 3.6864805221557617,
      "learning_rate": 1.98734014607947e-05,
      "loss": 4.8745,
      "num_input_tokens_seen": 220532134,
      "step": 6464,
      "train_runtime": 14248.2404,
      "train_tokens_per_second": 15477.85
    },
    {
      "epoch": 1.1710834685415539,
      "grad_norm": 3.7279343605041504,
      "learning_rate": 1.9869377509976474e-05,
      "loss": 4.762,
      "num_input_tokens_seen": 221629766,
      "step": 6496,
      "train_runtime": 14312.321,
      "train_tokens_per_second": 15485.243
    },
    {
      "epoch": 1.1768523526230394,
      "grad_norm": 3.197563409805298,
      "learning_rate": 1.9865291025100727e-05,
      "loss": 4.8277,
      "num_input_tokens_seen": 222718662,
      "step": 6528,
      "train_runtime": 14375.87,
      "train_tokens_per_second": 15492.535
    },
    {
      "epoch": 1.182621236704525,
      "grad_norm": 3.7520174980163574,
      "learning_rate": 1.986114203206013e-05,
      "loss": 4.872,
      "num_input_tokens_seen": 223857734,
      "step": 6560,
      "train_runtime": 14441.8773,
      "train_tokens_per_second": 15500.598
    },
    {
      "epoch": 1.1883901207860104,
      "grad_norm": 3.4962093830108643,
      "learning_rate": 1.9856930557143407e-05,
      "loss": 4.75,
      "num_input_tokens_seen": 224914822,
      "step": 6592,
      "train_runtime": 14504.4684,
      "train_tokens_per_second": 15506.588
    },
    {
      "epoch": 1.194159004867496,
      "grad_norm": 3.1234593391418457,
      "learning_rate": 1.985265662703518e-05,
      "loss": 4.8357,
      "num_input_tokens_seen": 226021158,
      "step": 6624,
      "train_runtime": 14569.2637,
      "train_tokens_per_second": 15513.561
    },
    {
      "epoch": 1.1999278889489815,
      "grad_norm": 4.8574957847595215,
      "learning_rate": 1.98483202688158e-05,
      "loss": 4.8653,
      "num_input_tokens_seen": 227105862,
      "step": 6656,
      "train_runtime": 14632.8232,
      "train_tokens_per_second": 15520.304
    },
    {
      "epoch": 1.2056967730304669,
      "grad_norm": 3.2610127925872803,
      "learning_rate": 1.9843921509961172e-05,
      "loss": 4.8589,
      "num_input_tokens_seen": 228148870,
      "step": 6688,
      "train_runtime": 14694.3686,
      "train_tokens_per_second": 15526.279
    },
    {
      "epoch": 1.2114656571119524,
      "grad_norm": 3.368269681930542,
      "learning_rate": 1.983946037834258e-05,
      "loss": 4.8055,
      "num_input_tokens_seen": 229235494,
      "step": 6720,
      "train_runtime": 14757.8187,
      "train_tokens_per_second": 15533.156
    },
    {
      "epoch": 1.217234541193438,
      "grad_norm": 3.47998046875,
      "learning_rate": 1.9834936902226513e-05,
      "loss": 4.8434,
      "num_input_tokens_seen": 230339078,
      "step": 6752,
      "train_runtime": 14822.0849,
      "train_tokens_per_second": 15540.262
    },
    {
      "epoch": 1.2230034252749233,
      "grad_norm": 3.353074550628662,
      "learning_rate": 1.983035111027449e-05,
      "loss": 4.8189,
      "num_input_tokens_seen": 231415814,
      "step": 6784,
      "train_runtime": 14885.1699,
      "train_tokens_per_second": 15546.737
    },
    {
      "epoch": 1.228772309356409,
      "grad_norm": 3.5964322090148926,
      "learning_rate": 1.9825703031542864e-05,
      "loss": 4.8746,
      "num_input_tokens_seen": 232494470,
      "step": 6816,
      "train_runtime": 14948.2397,
      "train_tokens_per_second": 15553.301
    },
    {
      "epoch": 1.2345411934378943,
      "grad_norm": 3.265256404876709,
      "learning_rate": 1.9820992695482663e-05,
      "loss": 4.8151,
      "num_input_tokens_seen": 233583718,
      "step": 6848,
      "train_runtime": 15011.8832,
      "train_tokens_per_second": 15559.921
    },
    {
      "epoch": 1.2403100775193798,
      "grad_norm": 3.5675923824310303,
      "learning_rate": 1.981622013193937e-05,
      "loss": 4.8184,
      "num_input_tokens_seen": 234651238,
      "step": 6880,
      "train_runtime": 15074.945,
      "train_tokens_per_second": 15565.645
    },
    {
      "epoch": 1.2460789616008654,
      "grad_norm": 3.5973193645477295,
      "learning_rate": 1.9811385371152765e-05,
      "loss": 4.8111,
      "num_input_tokens_seen": 235735494,
      "step": 6912,
      "train_runtime": 15138.5208,
      "train_tokens_per_second": 15571.898
    },
    {
      "epoch": 1.2518478456823507,
      "grad_norm": 4.043084144592285,
      "learning_rate": 1.9806488443756717e-05,
      "loss": 4.8217,
      "num_input_tokens_seen": 236880262,
      "step": 6944,
      "train_runtime": 15204.7347,
      "train_tokens_per_second": 15579.375
    },
    {
      "epoch": 1.2576167297638363,
      "grad_norm": 3.383133888244629,
      "learning_rate": 1.980152938077899e-05,
      "loss": 4.8574,
      "num_input_tokens_seen": 238025190,
      "step": 6976,
      "train_runtime": 15271.0143,
      "train_tokens_per_second": 15586.731
    },
    {
      "epoch": 1.2633856138453217,
      "grad_norm": 3.960707664489746,
      "learning_rate": 1.9796508213641053e-05,
      "loss": 4.7904,
      "num_input_tokens_seen": 239089030,
      "step": 7008,
      "train_runtime": 15333.3769,
      "train_tokens_per_second": 15592.719
    },
    {
      "epoch": 1.2691544979268072,
      "grad_norm": 6.396266460418701,
      "learning_rate": 1.979142497415788e-05,
      "loss": 4.8696,
      "num_input_tokens_seen": 240230054,
      "step": 7040,
      "train_runtime": 15399.3381,
      "train_tokens_per_second": 15600.025
    },
    {
      "epoch": 1.2749233820082928,
      "grad_norm": 3.3140382766723633,
      "learning_rate": 1.9786279694537733e-05,
      "loss": 4.8451,
      "num_input_tokens_seen": 241347462,
      "step": 7072,
      "train_runtime": 15464.3165,
      "train_tokens_per_second": 15606.733
    },
    {
      "epoch": 1.2806922660897784,
      "grad_norm": 2.972419261932373,
      "learning_rate": 1.978107240738199e-05,
      "loss": 4.8164,
      "num_input_tokens_seen": 242467174,
      "step": 7104,
      "train_runtime": 15529.3698,
      "train_tokens_per_second": 15613.459
    },
    {
      "epoch": 1.2864611501712637,
      "grad_norm": 3.4024059772491455,
      "learning_rate": 1.9775803145684906e-05,
      "loss": 4.8121,
      "num_input_tokens_seen": 243536902,
      "step": 7136,
      "train_runtime": 15592.0172,
      "train_tokens_per_second": 15619.333
    },
    {
      "epoch": 1.2922300342527493,
      "grad_norm": 3.7519757747650146,
      "learning_rate": 1.977047194283342e-05,
      "loss": 4.8543,
      "num_input_tokens_seen": 244634214,
      "step": 7168,
      "train_runtime": 15656.0074,
      "train_tokens_per_second": 15625.581
    },
    {
      "epoch": 1.2979989183342346,
      "grad_norm": 3.2128164768218994,
      "learning_rate": 1.9765078832606944e-05,
      "loss": 4.8705,
      "num_input_tokens_seen": 245725542,
      "step": 7200,
      "train_runtime": 15719.7846,
      "train_tokens_per_second": 15631.61
    },
    {
      "epoch": 1.3037678024157202,
      "grad_norm": 3.858163833618164,
      "learning_rate": 1.975962384917714e-05,
      "loss": 4.7793,
      "num_input_tokens_seen": 246821222,
      "step": 7232,
      "train_runtime": 15783.9514,
      "train_tokens_per_second": 15637.48
    },
    {
      "epoch": 1.3095366864972058,
      "grad_norm": 3.1296544075012207,
      "learning_rate": 1.9754107027107718e-05,
      "loss": 4.8588,
      "num_input_tokens_seen": 247896806,
      "step": 7264,
      "train_runtime": 15847.3274,
      "train_tokens_per_second": 15642.815
    },
    {
      "epoch": 1.3153055705786911,
      "grad_norm": 3.4613566398620605,
      "learning_rate": 1.9748528401354194e-05,
      "loss": 4.8233,
      "num_input_tokens_seen": 248995942,
      "step": 7296,
      "train_runtime": 15911.9283,
      "train_tokens_per_second": 15648.383
    },
    {
      "epoch": 1.3210744546601767,
      "grad_norm": 3.3450849056243896,
      "learning_rate": 1.9742888007263698e-05,
      "loss": 4.7968,
      "num_input_tokens_seen": 250098886,
      "step": 7328,
      "train_runtime": 15976.463,
      "train_tokens_per_second": 15654.209
    },
    {
      "epoch": 1.326843338741662,
      "grad_norm": 3.600023031234741,
      "learning_rate": 1.973718588057473e-05,
      "loss": 4.8404,
      "num_input_tokens_seen": 251208198,
      "step": 7360,
      "train_runtime": 16041.3872,
      "train_tokens_per_second": 15660.005
    },
    {
      "epoch": 1.3326122228231476,
      "grad_norm": 3.241856336593628,
      "learning_rate": 1.973142205741693e-05,
      "loss": 4.7914,
      "num_input_tokens_seen": 252310150,
      "step": 7392,
      "train_runtime": 16105.7042,
      "train_tokens_per_second": 15665.888
    },
    {
      "epoch": 1.3383811069046332,
      "grad_norm": 3.154062032699585,
      "learning_rate": 1.9725596574310865e-05,
      "loss": 4.8448,
      "num_input_tokens_seen": 253385638,
      "step": 7424,
      "train_runtime": 16168.5541,
      "train_tokens_per_second": 15671.509
    },
    {
      "epoch": 1.3441499909861188,
      "grad_norm": 3.720827341079712,
      "learning_rate": 1.971970946816779e-05,
      "loss": 4.7781,
      "num_input_tokens_seen": 254484678,
      "step": 7456,
      "train_runtime": 16232.5568,
      "train_tokens_per_second": 15677.424
    },
    {
      "epoch": 1.349918875067604,
      "grad_norm": 3.0943562984466553,
      "learning_rate": 1.971376077628941e-05,
      "loss": 4.8666,
      "num_input_tokens_seen": 255577958,
      "step": 7488,
      "train_runtime": 16296.7091,
      "train_tokens_per_second": 15682.796
    },
    {
      "epoch": 1.3556877591490897,
      "grad_norm": 3.660078287124634,
      "learning_rate": 1.9707750536367657e-05,
      "loss": 4.7796,
      "num_input_tokens_seen": 256649126,
      "step": 7520,
      "train_runtime": 16359.6049,
      "train_tokens_per_second": 15687.978
    },
    {
      "epoch": 1.361456643230575,
      "grad_norm": 3.2728219032287598,
      "learning_rate": 1.970167878648443e-05,
      "loss": 4.8079,
      "num_input_tokens_seen": 257687494,
      "step": 7552,
      "train_runtime": 16420.6515,
      "train_tokens_per_second": 15692.891
    },
    {
      "epoch": 1.3672255273120606,
      "grad_norm": 3.518278121948242,
      "learning_rate": 1.9695545565111374e-05,
      "loss": 4.8127,
      "num_input_tokens_seen": 258732358,
      "step": 7584,
      "train_runtime": 16482.4447,
      "train_tokens_per_second": 15697.45
    },
    {
      "epoch": 1.3729944113935462,
      "grad_norm": 3.4276180267333984,
      "learning_rate": 1.9689350911109616e-05,
      "loss": 4.8292,
      "num_input_tokens_seen": 259805382,
      "step": 7616,
      "train_runtime": 16545.6229,
      "train_tokens_per_second": 15702.363
    },
    {
      "epoch": 1.3787632954750315,
      "grad_norm": 2.9606363773345947,
      "learning_rate": 1.968309486372955e-05,
      "loss": 4.8337,
      "num_input_tokens_seen": 260895398,
      "step": 7648,
      "train_runtime": 16609.6175,
      "train_tokens_per_second": 15707.49
    },
    {
      "epoch": 1.384532179556517,
      "grad_norm": 3.1743459701538086,
      "learning_rate": 1.9676777462610556e-05,
      "loss": 4.8238,
      "num_input_tokens_seen": 261966022,
      "step": 7680,
      "train_runtime": 16672.7198,
      "train_tokens_per_second": 15712.255
    },
    {
      "epoch": 1.3903010636380024,
      "grad_norm": 3.4028468132019043,
      "learning_rate": 1.9670398747780756e-05,
      "loss": 4.8191,
      "num_input_tokens_seen": 263088710,
      "step": 7712,
      "train_runtime": 16738.2265,
      "train_tokens_per_second": 15717.837
    },
    {
      "epoch": 1.396069947719488,
      "grad_norm": 3.2447569370269775,
      "learning_rate": 1.9663958759656784e-05,
      "loss": 4.86,
      "num_input_tokens_seen": 264174310,
      "step": 7744,
      "train_runtime": 16802.0014,
      "train_tokens_per_second": 15722.788
    },
    {
      "epoch": 1.4018388318009736,
      "grad_norm": 3.6134960651397705,
      "learning_rate": 1.9657457539043496e-05,
      "loss": 4.8024,
      "num_input_tokens_seen": 265265766,
      "step": 7776,
      "train_runtime": 16865.8716,
      "train_tokens_per_second": 15727.961
    },
    {
      "epoch": 1.407607715882459,
      "grad_norm": 3.41519832611084,
      "learning_rate": 1.9650895127133735e-05,
      "loss": 4.8364,
      "num_input_tokens_seen": 266366022,
      "step": 7808,
      "train_runtime": 16930.0414,
      "train_tokens_per_second": 15733.336
    },
    {
      "epoch": 1.4133765999639445,
      "grad_norm": 3.0414178371429443,
      "learning_rate": 1.964427156550806e-05,
      "loss": 4.8576,
      "num_input_tokens_seen": 267455974,
      "step": 7840,
      "train_runtime": 16993.5243,
      "train_tokens_per_second": 15738.7
    },
    {
      "epoch": 1.4191454840454298,
      "grad_norm": 3.7269585132598877,
      "learning_rate": 1.9637586896134487e-05,
      "loss": 4.8571,
      "num_input_tokens_seen": 268570150,
      "step": 7872,
      "train_runtime": 17058.4646,
      "train_tokens_per_second": 15744.099
    },
    {
      "epoch": 1.4249143681269154,
      "grad_norm": 3.528916120529175,
      "learning_rate": 1.9630841161368224e-05,
      "loss": 4.9192,
      "num_input_tokens_seen": 269695622,
      "step": 7904,
      "train_runtime": 17123.8064,
      "train_tokens_per_second": 15749.747
    },
    {
      "epoch": 1.430683252208401,
      "grad_norm": 3.244499683380127,
      "learning_rate": 1.9624034403951388e-05,
      "loss": 4.8255,
      "num_input_tokens_seen": 270765030,
      "step": 7936,
      "train_runtime": 17186.3433,
      "train_tokens_per_second": 15754.662
    },
    {
      "epoch": 1.4364521362898865,
      "grad_norm": 3.3232874870300293,
      "learning_rate": 1.961716666701276e-05,
      "loss": 4.8093,
      "num_input_tokens_seen": 271851302,
      "step": 7968,
      "train_runtime": 17249.6312,
      "train_tokens_per_second": 15759.833
    },
    {
      "epoch": 1.442221020371372,
      "grad_norm": 3.8782310485839844,
      "learning_rate": 1.9610237994067488e-05,
      "loss": 4.8031,
      "num_input_tokens_seen": 272940742,
      "step": 8000,
      "train_runtime": 17313.3645,
      "train_tokens_per_second": 15764.743
    },
    {
      "epoch": 1.4479899044528575,
      "grad_norm": 3.0311694145202637,
      "learning_rate": 1.9603248429016828e-05,
      "loss": 4.865,
      "num_input_tokens_seen": 274033222,
      "step": 8032,
      "train_runtime": 17377.3143,
      "train_tokens_per_second": 15769.596
    },
    {
      "epoch": 1.4537587885343428,
      "grad_norm": 3.265305757522583,
      "learning_rate": 1.9596198016147855e-05,
      "loss": 4.8447,
      "num_input_tokens_seen": 275142502,
      "step": 8064,
      "train_runtime": 17442.3002,
      "train_tokens_per_second": 15774.439
    },
    {
      "epoch": 1.4595276726158284,
      "grad_norm": 3.091909885406494,
      "learning_rate": 1.958908680013318e-05,
      "loss": 4.815,
      "num_input_tokens_seen": 276204710,
      "step": 8096,
      "train_runtime": 17504.9564,
      "train_tokens_per_second": 15778.657
    },
    {
      "epoch": 1.465296556697314,
      "grad_norm": 2.7012667655944824,
      "learning_rate": 1.9581914826030685e-05,
      "loss": 4.8204,
      "num_input_tokens_seen": 277308870,
      "step": 8128,
      "train_runtime": 17569.1698,
      "train_tokens_per_second": 15783.835
    },
    {
      "epoch": 1.4710654407787993,
      "grad_norm": 3.609816312789917,
      "learning_rate": 1.957468213928322e-05,
      "loss": 4.8397,
      "num_input_tokens_seen": 278386278,
      "step": 8160,
      "train_runtime": 17632.1343,
      "train_tokens_per_second": 15788.575
    },
    {
      "epoch": 1.4768343248602849,
      "grad_norm": 3.278559446334839,
      "learning_rate": 1.9567388785718315e-05,
      "loss": 4.8408,
      "num_input_tokens_seen": 279483494,
      "step": 8192,
      "train_runtime": 17696.4935,
      "train_tokens_per_second": 15793.157
    },
    {
      "epoch": 1.4826032089417702,
      "grad_norm": 3.826197624206543,
      "learning_rate": 1.9560034811547907e-05,
      "loss": 4.7844,
      "num_input_tokens_seen": 280580902,
      "step": 8224,
      "train_runtime": 17760.8081,
      "train_tokens_per_second": 15797.755
    },
    {
      "epoch": 1.4883720930232558,
      "grad_norm": 3.586574077606201,
      "learning_rate": 1.9552620263368027e-05,
      "loss": 4.7997,
      "num_input_tokens_seen": 281701350,
      "step": 8256,
      "train_runtime": 17826.072,
      "train_tokens_per_second": 15802.772
    },
    {
      "epoch": 1.4941409771047414,
      "grad_norm": 3.644909143447876,
      "learning_rate": 1.9545145188158513e-05,
      "loss": 4.8719,
      "num_input_tokens_seen": 282767462,
      "step": 8288,
      "train_runtime": 17888.6572,
      "train_tokens_per_second": 15807.081
    },
    {
      "epoch": 1.4999098611862267,
      "grad_norm": 2.8953328132629395,
      "learning_rate": 1.9537609633282725e-05,
      "loss": 4.8254,
      "num_input_tokens_seen": 283875334,
      "step": 8320,
      "train_runtime": 17953.3317,
      "train_tokens_per_second": 15811.847
    },
    {
      "epoch": 1.5056787452677123,
      "grad_norm": 3.4295907020568848,
      "learning_rate": 1.953001364648721e-05,
      "loss": 4.771,
      "num_input_tokens_seen": 284945958,
      "step": 8352,
      "train_runtime": 18016.1186,
      "train_tokens_per_second": 15816.168
    },
    {
      "epoch": 1.5114476293491976,
      "grad_norm": 3.121865749359131,
      "learning_rate": 1.9522357275901444e-05,
      "loss": 4.8353,
      "num_input_tokens_seen": 286070310,
      "step": 8384,
      "train_runtime": 18081.4565,
      "train_tokens_per_second": 15821.198
    },
    {
      "epoch": 1.5172165134306832,
      "grad_norm": 2.995149612426758,
      "learning_rate": 1.9514640570037487e-05,
      "loss": 4.7929,
      "num_input_tokens_seen": 287144166,
      "step": 8416,
      "train_runtime": 18144.3732,
      "train_tokens_per_second": 15825.521
    },
    {
      "epoch": 1.5229853975121688,
      "grad_norm": 3.218637704849243,
      "learning_rate": 1.9506863577789708e-05,
      "loss": 4.809,
      "num_input_tokens_seen": 288234278,
      "step": 8448,
      "train_runtime": 18208.0439,
      "train_tokens_per_second": 15830.052
    },
    {
      "epoch": 1.5287542815936543,
      "grad_norm": 3.3537933826446533,
      "learning_rate": 1.949902634843446e-05,
      "loss": 4.7887,
      "num_input_tokens_seen": 289298214,
      "step": 8480,
      "train_runtime": 18270.4944,
      "train_tokens_per_second": 15834.175
    },
    {
      "epoch": 1.5345231656751397,
      "grad_norm": 3.418565511703491,
      "learning_rate": 1.949112893162975e-05,
      "loss": 4.7729,
      "num_input_tokens_seen": 290359686,
      "step": 8512,
      "train_runtime": 18333.1655,
      "train_tokens_per_second": 15837.946
    },
    {
      "epoch": 1.540292049756625,
      "grad_norm": 3.3247838020324707,
      "learning_rate": 1.9483171377414968e-05,
      "loss": 4.8214,
      "num_input_tokens_seen": 291418918,
      "step": 8544,
      "train_runtime": 18395.7469,
      "train_tokens_per_second": 15841.646
    },
    {
      "epoch": 1.5460609338381106,
      "grad_norm": 3.374870777130127,
      "learning_rate": 1.9475153736210525e-05,
      "loss": 4.8261,
      "num_input_tokens_seen": 292507078,
      "step": 8576,
      "train_runtime": 18459.7121,
      "train_tokens_per_second": 15845.701
    },
    {
      "epoch": 1.5518298179195962,
      "grad_norm": 3.3869996070861816,
      "learning_rate": 1.946707605881757e-05,
      "loss": 4.7933,
      "num_input_tokens_seen": 293601062,
      "step": 8608,
      "train_runtime": 18523.9972,
      "train_tokens_per_second": 15849.768
    },
    {
      "epoch": 1.5575987020010817,
      "grad_norm": 3.2061474323272705,
      "learning_rate": 1.945893839641764e-05,
      "loss": 4.8213,
      "num_input_tokens_seen": 294703718,
      "step": 8640,
      "train_runtime": 18588.6235,
      "train_tokens_per_second": 15853.983
    },
    {
      "epoch": 1.5633675860825673,
      "grad_norm": 3.185854911804199,
      "learning_rate": 1.945074080057235e-05,
      "loss": 4.8841,
      "num_input_tokens_seen": 295796998,
      "step": 8672,
      "train_runtime": 18652.6213,
      "train_tokens_per_second": 15858.2
    },
    {
      "epoch": 1.5691364701640527,
      "grad_norm": 3.056278944015503,
      "learning_rate": 1.9442483323223062e-05,
      "loss": 4.8465,
      "num_input_tokens_seen": 296925958,
      "step": 8704,
      "train_runtime": 18718.1107,
      "train_tokens_per_second": 15863.03
    },
    {
      "epoch": 1.574905354245538,
      "grad_norm": 2.9617063999176025,
      "learning_rate": 1.9434166016690562e-05,
      "loss": 4.7871,
      "num_input_tokens_seen": 298000294,
      "step": 8736,
      "train_runtime": 18780.939,
      "train_tokens_per_second": 15867.167
    },
    {
      "epoch": 1.5806742383270236,
      "grad_norm": 3.154205799102783,
      "learning_rate": 1.9425788933674724e-05,
      "loss": 4.8296,
      "num_input_tokens_seen": 299084134,
      "step": 8768,
      "train_runtime": 18844.2059,
      "train_tokens_per_second": 15871.411
    },
    {
      "epoch": 1.5864431224085092,
      "grad_norm": 3.006692886352539,
      "learning_rate": 1.9417352127254164e-05,
      "loss": 4.8271,
      "num_input_tokens_seen": 300195910,
      "step": 8800,
      "train_runtime": 18908.9122,
      "train_tokens_per_second": 15875.895
    },
    {
      "epoch": 1.5922120064899947,
      "grad_norm": 3.245248317718506,
      "learning_rate": 1.9408855650885938e-05,
      "loss": 4.8415,
      "num_input_tokens_seen": 301298566,
      "step": 8832,
      "train_runtime": 18973.1179,
      "train_tokens_per_second": 15880.287
    },
    {
      "epoch": 1.59798089057148,
      "grad_norm": 3.5184638500213623,
      "learning_rate": 1.9400299558405154e-05,
      "loss": 4.8384,
      "num_input_tokens_seen": 302356422,
      "step": 8864,
      "train_runtime": 19035.319,
      "train_tokens_per_second": 15883.969
    },
    {
      "epoch": 1.6037497746529654,
      "grad_norm": 3.1579785346984863,
      "learning_rate": 1.9391683904024684e-05,
      "loss": 4.8598,
      "num_input_tokens_seen": 303438566,
      "step": 8896,
      "train_runtime": 19098.8334,
      "train_tokens_per_second": 15887.806
    },
    {
      "epoch": 1.609518658734451,
      "grad_norm": 3.3713104724884033,
      "learning_rate": 1.9383008742334778e-05,
      "loss": 4.8427,
      "num_input_tokens_seen": 304566950,
      "step": 8928,
      "train_runtime": 19164.6339,
      "train_tokens_per_second": 15892.135
    },
    {
      "epoch": 1.6152875428159366,
      "grad_norm": 3.085603713989258,
      "learning_rate": 1.9374274128302734e-05,
      "loss": 4.8482,
      "num_input_tokens_seen": 305667142,
      "step": 8960,
      "train_runtime": 19229.1278,
      "train_tokens_per_second": 15896.048
    },
    {
      "epoch": 1.6210564268974221,
      "grad_norm": 3.206111431121826,
      "learning_rate": 1.936548011727256e-05,
      "loss": 4.8529,
      "num_input_tokens_seen": 306790342,
      "step": 8992,
      "train_runtime": 19294.8158,
      "train_tokens_per_second": 15900.144
    },
    {
      "epoch": 1.6268253109789075,
      "grad_norm": 3.561109781265259,
      "learning_rate": 1.9356626764964604e-05,
      "loss": 4.8525,
      "num_input_tokens_seen": 307872166,
      "step": 9024,
      "train_runtime": 19358.438,
      "train_tokens_per_second": 15903.771
    },
    {
      "epoch": 1.632594195060393,
      "grad_norm": 3.438708782196045,
      "learning_rate": 1.9347714127475222e-05,
      "loss": 4.8566,
      "num_input_tokens_seen": 308956230,
      "step": 9056,
      "train_runtime": 19422.1234,
      "train_tokens_per_second": 15907.438
    },
    {
      "epoch": 1.6383630791418784,
      "grad_norm": 3.61970853805542,
      "learning_rate": 1.9338742261276396e-05,
      "loss": 4.7923,
      "num_input_tokens_seen": 310009702,
      "step": 9088,
      "train_runtime": 19484.4533,
      "train_tokens_per_second": 15910.618
    },
    {
      "epoch": 1.644131963223364,
      "grad_norm": 3.474029541015625,
      "learning_rate": 1.9329711223215403e-05,
      "loss": 4.7779,
      "num_input_tokens_seen": 311146726,
      "step": 9120,
      "train_runtime": 19550.7759,
      "train_tokens_per_second": 15914.802
    },
    {
      "epoch": 1.6499008473048495,
      "grad_norm": 3.543256998062134,
      "learning_rate": 1.932062107051444e-05,
      "loss": 4.8194,
      "num_input_tokens_seen": 312253830,
      "step": 9152,
      "train_runtime": 19615.5467,
      "train_tokens_per_second": 15918.691
    },
    {
      "epoch": 1.655669731386335,
      "grad_norm": 3.386363983154297,
      "learning_rate": 1.9311471860770257e-05,
      "loss": 4.8272,
      "num_input_tokens_seen": 313320806,
      "step": 9184,
      "train_runtime": 19678.271,
      "train_tokens_per_second": 15922.172
    },
    {
      "epoch": 1.6614386154678205,
      "grad_norm": 3.2472450733184814,
      "learning_rate": 1.9302263651953814e-05,
      "loss": 4.8166,
      "num_input_tokens_seen": 314447046,
      "step": 9216,
      "train_runtime": 19744.1173,
      "train_tokens_per_second": 15926.113
    },
    {
      "epoch": 1.6672074995493058,
      "grad_norm": 3.24613881111145,
      "learning_rate": 1.9292996502409887e-05,
      "loss": 4.8821,
      "num_input_tokens_seen": 315538310,
      "step": 9248,
      "train_runtime": 19808.1634,
      "train_tokens_per_second": 15929.711
    },
    {
      "epoch": 1.6729763836307914,
      "grad_norm": 2.936152935028076,
      "learning_rate": 1.9283670470856712e-05,
      "loss": 4.7948,
      "num_input_tokens_seen": 316621062,
      "step": 9280,
      "train_runtime": 19871.8153,
      "train_tokens_per_second": 15933.173
    },
    {
      "epoch": 1.678745267712277,
      "grad_norm": 3.4068868160247803,
      "learning_rate": 1.927428561638562e-05,
      "loss": 4.8163,
      "num_input_tokens_seen": 317676774,
      "step": 9312,
      "train_runtime": 19934.2478,
      "train_tokens_per_second": 15936.231
    },
    {
      "epoch": 1.6845141517937625,
      "grad_norm": 3.10905385017395,
      "learning_rate": 1.926484199846064e-05,
      "loss": 4.8265,
      "num_input_tokens_seen": 318731878,
      "step": 9344,
      "train_runtime": 19996.5292,
      "train_tokens_per_second": 15939.36
    },
    {
      "epoch": 1.6902830358752479,
      "grad_norm": 3.369109869003296,
      "learning_rate": 1.9255339676918155e-05,
      "loss": 4.8845,
      "num_input_tokens_seen": 319840262,
      "step": 9376,
      "train_runtime": 20060.9238,
      "train_tokens_per_second": 15943.446
    },
    {
      "epoch": 1.6960519199567332,
      "grad_norm": 3.5334057807922363,
      "learning_rate": 1.9245778711966484e-05,
      "loss": 4.8232,
      "num_input_tokens_seen": 320972326,
      "step": 9408,
      "train_runtime": 20126.5581,
      "train_tokens_per_second": 15947.701
    },
    {
      "epoch": 1.7018208040382188,
      "grad_norm": 3.1388776302337646,
      "learning_rate": 1.9236159164185534e-05,
      "loss": 4.8058,
      "num_input_tokens_seen": 322049478,
      "step": 9440,
      "train_runtime": 20189.8362,
      "train_tokens_per_second": 15951.069
    },
    {
      "epoch": 1.7075896881197044,
      "grad_norm": 2.937199831008911,
      "learning_rate": 1.9226481094526398e-05,
      "loss": 4.853,
      "num_input_tokens_seen": 323182182,
      "step": 9472,
      "train_runtime": 20255.6461,
      "train_tokens_per_second": 15955.165
    },
    {
      "epoch": 1.71335857220119,
      "grad_norm": 3.3626623153686523,
      "learning_rate": 1.9216744564310973e-05,
      "loss": 4.7517,
      "num_input_tokens_seen": 324239462,
      "step": 9504,
      "train_runtime": 20317.731,
      "train_tokens_per_second": 15958.448
    },
    {
      "epoch": 1.7191274562826753,
      "grad_norm": 3.4637014865875244,
      "learning_rate": 1.9206949635231574e-05,
      "loss": 4.8265,
      "num_input_tokens_seen": 325335686,
      "step": 9536,
      "train_runtime": 20381.4453,
      "train_tokens_per_second": 15962.346
    },
    {
      "epoch": 1.7248963403641608,
      "grad_norm": 3.6957433223724365,
      "learning_rate": 1.9197096369350545e-05,
      "loss": 4.8045,
      "num_input_tokens_seen": 326429670,
      "step": 9568,
      "train_runtime": 20445.133,
      "train_tokens_per_second": 15966.131
    },
    {
      "epoch": 1.7306652244456462,
      "grad_norm": 3.1471574306488037,
      "learning_rate": 1.918718482909985e-05,
      "loss": 4.8266,
      "num_input_tokens_seen": 327511494,
      "step": 9600,
      "train_runtime": 20508.2835,
      "train_tokens_per_second": 15969.718
    },
    {
      "epoch": 1.7364341085271318,
      "grad_norm": 3.139302968978882,
      "learning_rate": 1.9177215077280705e-05,
      "loss": 4.7937,
      "num_input_tokens_seen": 328559206,
      "step": 9632,
      "train_runtime": 20569.8218,
      "train_tokens_per_second": 15972.876
    },
    {
      "epoch": 1.7422029926086173,
      "grad_norm": 3.551828145980835,
      "learning_rate": 1.916718717706315e-05,
      "loss": 4.8102,
      "num_input_tokens_seen": 329652838,
      "step": 9664,
      "train_runtime": 20633.643,
      "train_tokens_per_second": 15976.473
    },
    {
      "epoch": 1.747971876690103,
      "grad_norm": 3.1053547859191895,
      "learning_rate": 1.9157101191985675e-05,
      "loss": 4.8307,
      "num_input_tokens_seen": 330770566,
      "step": 9696,
      "train_runtime": 20698.5717,
      "train_tokens_per_second": 15980.357
    },
    {
      "epoch": 1.7537407607715882,
      "grad_norm": 3.0200486183166504,
      "learning_rate": 1.914695718595479e-05,
      "loss": 4.8194,
      "num_input_tokens_seen": 331900262,
      "step": 9728,
      "train_runtime": 20764.0165,
      "train_tokens_per_second": 15984.396
    },
    {
      "epoch": 1.7595096448530736,
      "grad_norm": 3.4050893783569336,
      "learning_rate": 1.9136755223244657e-05,
      "loss": 4.8949,
      "num_input_tokens_seen": 333011206,
      "step": 9760,
      "train_runtime": 20828.5683,
      "train_tokens_per_second": 15988.195
    },
    {
      "epoch": 1.7652785289345592,
      "grad_norm": 3.0287978649139404,
      "learning_rate": 1.9126495368496642e-05,
      "loss": 4.8208,
      "num_input_tokens_seen": 334083334,
      "step": 9792,
      "train_runtime": 20891.3952,
      "train_tokens_per_second": 15991.432
    },
    {
      "epoch": 1.7710474130160447,
      "grad_norm": 3.6826324462890625,
      "learning_rate": 1.9116177686718937e-05,
      "loss": 4.8041,
      "num_input_tokens_seen": 335169158,
      "step": 9824,
      "train_runtime": 20954.8368,
      "train_tokens_per_second": 15994.835
    },
    {
      "epoch": 1.7768162970975303,
      "grad_norm": 3.1220815181732178,
      "learning_rate": 1.9105802243286133e-05,
      "loss": 4.8015,
      "num_input_tokens_seen": 336263686,
      "step": 9856,
      "train_runtime": 21018.6236,
      "train_tokens_per_second": 15998.369
    },
    {
      "epoch": 1.7825851811790157,
      "grad_norm": 3.494704008102417,
      "learning_rate": 1.9095369103938802e-05,
      "loss": 4.8332,
      "num_input_tokens_seen": 337359654,
      "step": 9888,
      "train_runtime": 21082.6267,
      "train_tokens_per_second": 16001.785
    },
    {
      "epoch": 1.7883540652605012,
      "grad_norm": 3.117496967315674,
      "learning_rate": 1.90848783347831e-05,
      "loss": 4.8924,
      "num_input_tokens_seen": 338428742,
      "step": 9920,
      "train_runtime": 21145.2915,
      "train_tokens_per_second": 16004.922
    },
    {
      "epoch": 1.7941229493419866,
      "grad_norm": 3.7799832820892334,
      "learning_rate": 1.9074330002290313e-05,
      "loss": 4.8381,
      "num_input_tokens_seen": 339524006,
      "step": 9952,
      "train_runtime": 21209.12,
      "train_tokens_per_second": 16008.397
    },
    {
      "epoch": 1.7998918334234721,
      "grad_norm": 3.2405126094818115,
      "learning_rate": 1.906372417329649e-05,
      "loss": 4.8412,
      "num_input_tokens_seen": 340615270,
      "step": 9984,
      "train_runtime": 21272.8244,
      "train_tokens_per_second": 16011.756
    },
    {
      "epoch": 1.8056607175049577,
      "grad_norm": 3.5178327560424805,
      "learning_rate": 1.9053060915001967e-05,
      "loss": 4.8978,
      "num_input_tokens_seen": 341768230,
      "step": 10016,
      "train_runtime": 21339.4599,
      "train_tokens_per_second": 16015.786
    },
    {
      "epoch": 1.8114296015864433,
      "grad_norm": 3.041790008544922,
      "learning_rate": 1.904234029497096e-05,
      "loss": 4.8497,
      "num_input_tokens_seen": 342828358,
      "step": 10048,
      "train_runtime": 21401.6423,
      "train_tokens_per_second": 16018.787
    },
    {
      "epoch": 1.8171984856679286,
      "grad_norm": 3.4345014095306396,
      "learning_rate": 1.9031562381131143e-05,
      "loss": 4.8402,
      "num_input_tokens_seen": 343934630,
      "step": 10080,
      "train_runtime": 21466.0719,
      "train_tokens_per_second": 16022.243
    },
    {
      "epoch": 1.822967369749414,
      "grad_norm": 3.8350391387939453,
      "learning_rate": 1.902072724177322e-05,
      "loss": 4.8507,
      "num_input_tokens_seen": 345004454,
      "step": 10112,
      "train_runtime": 21528.8315,
      "train_tokens_per_second": 16025.229
    },
    {
      "epoch": 1.8287362538308995,
      "grad_norm": 3.370708465576172,
      "learning_rate": 1.9009834945550484e-05,
      "loss": 4.7937,
      "num_input_tokens_seen": 346053894,
      "step": 10144,
      "train_runtime": 21590.8758,
      "train_tokens_per_second": 16027.784
    },
    {
      "epoch": 1.8345051379123851,
      "grad_norm": 3.0951733589172363,
      "learning_rate": 1.899888556147837e-05,
      "loss": 4.8202,
      "num_input_tokens_seen": 347139206,
      "step": 10176,
      "train_runtime": 21654.428,
      "train_tokens_per_second": 16030.865
    },
    {
      "epoch": 1.8402740219938707,
      "grad_norm": 3.2940001487731934,
      "learning_rate": 1.8987879158934044e-05,
      "loss": 4.8372,
      "num_input_tokens_seen": 348264454,
      "step": 10208,
      "train_runtime": 21719.7469,
      "train_tokens_per_second": 16034.462
    },
    {
      "epoch": 1.846042906075356,
      "grad_norm": 3.5861783027648926,
      "learning_rate": 1.897681580765595e-05,
      "loss": 4.8646,
      "num_input_tokens_seen": 349356582,
      "step": 10240,
      "train_runtime": 21783.8307,
      "train_tokens_per_second": 16037.426
    },
    {
      "epoch": 1.8518117901568414,
      "grad_norm": 3.4709222316741943,
      "learning_rate": 1.8965695577743365e-05,
      "loss": 4.8484,
      "num_input_tokens_seen": 350482118,
      "step": 10272,
      "train_runtime": 21849.064,
      "train_tokens_per_second": 16041.059
    },
    {
      "epoch": 1.857580674238327,
      "grad_norm": 3.102905511856079,
      "learning_rate": 1.8954518539655962e-05,
      "loss": 4.8899,
      "num_input_tokens_seen": 351568518,
      "step": 10304,
      "train_runtime": 21912.9913,
      "train_tokens_per_second": 16043.84
    },
    {
      "epoch": 1.8633495583198125,
      "grad_norm": 3.5500094890594482,
      "learning_rate": 1.8943284764213357e-05,
      "loss": 4.8219,
      "num_input_tokens_seen": 352621958,
      "step": 10336,
      "train_runtime": 21975.1277,
      "train_tokens_per_second": 16046.412
    },
    {
      "epoch": 1.869118442401298,
      "grad_norm": 3.3337790966033936,
      "learning_rate": 1.8931994322594668e-05,
      "loss": 4.7385,
      "num_input_tokens_seen": 353645158,
      "step": 10368,
      "train_runtime": 22035.5719,
      "train_tokens_per_second": 16048.83
    },
    {
      "epoch": 1.8748873264827834,
      "grad_norm": 3.4226818084716797,
      "learning_rate": 1.8920647286338056e-05,
      "loss": 4.7625,
      "num_input_tokens_seen": 354726278,
      "step": 10400,
      "train_runtime": 22098.8277,
      "train_tokens_per_second": 16051.814
    },
    {
      "epoch": 1.880656210564269,
      "grad_norm": 3.4827651977539062,
      "learning_rate": 1.8909243727340273e-05,
      "loss": 4.8598,
      "num_input_tokens_seen": 355876646,
      "step": 10432,
      "train_runtime": 22165.3221,
      "train_tokens_per_second": 16055.559
    },
    {
      "epoch": 1.8864250946457544,
      "grad_norm": 3.4325249195098877,
      "learning_rate": 1.8897783717856215e-05,
      "loss": 4.8264,
      "num_input_tokens_seen": 356924678,
      "step": 10464,
      "train_runtime": 22226.865,
      "train_tokens_per_second": 16058.256
    },
    {
      "epoch": 1.89219397872724,
      "grad_norm": 3.4796721935272217,
      "learning_rate": 1.8886267330498455e-05,
      "loss": 4.7886,
      "num_input_tokens_seen": 357948262,
      "step": 10496,
      "train_runtime": 22287.7885,
      "train_tokens_per_second": 16060.286
    },
    {
      "epoch": 1.8979628628087255,
      "grad_norm": 3.527951240539551,
      "learning_rate": 1.8874694638236782e-05,
      "loss": 4.8262,
      "num_input_tokens_seen": 359075462,
      "step": 10528,
      "train_runtime": 22353.661,
      "train_tokens_per_second": 16063.385
    },
    {
      "epoch": 1.903731746890211,
      "grad_norm": 3.444775104522705,
      "learning_rate": 1.886306571439775e-05,
      "loss": 4.8075,
      "num_input_tokens_seen": 360184774,
      "step": 10560,
      "train_runtime": 22418.5591,
      "train_tokens_per_second": 16066.366
    },
    {
      "epoch": 1.9095006309716964,
      "grad_norm": 3.3695685863494873,
      "learning_rate": 1.8851380632664198e-05,
      "loss": 4.8977,
      "num_input_tokens_seen": 361294118,
      "step": 10592,
      "train_runtime": 22483.1501,
      "train_tokens_per_second": 16069.551
    },
    {
      "epoch": 1.9152695150531818,
      "grad_norm": 3.940575122833252,
      "learning_rate": 1.88396394670748e-05,
      "loss": 4.7777,
      "num_input_tokens_seen": 362368006,
      "step": 10624,
      "train_runtime": 22546.0661,
      "train_tokens_per_second": 16072.338
    },
    {
      "epoch": 1.9210383991346673,
      "grad_norm": 3.2835066318511963,
      "learning_rate": 1.8827842292023578e-05,
      "loss": 4.9055,
      "num_input_tokens_seen": 363458310,
      "step": 10656,
      "train_runtime": 22609.6412,
      "train_tokens_per_second": 16075.368
    },
    {
      "epoch": 1.926807283216153,
      "grad_norm": 3.57904314994812,
      "learning_rate": 1.881598918225944e-05,
      "loss": 4.8967,
      "num_input_tokens_seen": 364570822,
      "step": 10688,
      "train_runtime": 22674.3898,
      "train_tokens_per_second": 16078.528
    },
    {
      "epoch": 1.9325761672976385,
      "grad_norm": 3.0948615074157715,
      "learning_rate": 1.880408021288572e-05,
      "loss": 4.8413,
      "num_input_tokens_seen": 365640902,
      "step": 10720,
      "train_runtime": 22737.1274,
      "train_tokens_per_second": 16081.227
    },
    {
      "epoch": 1.9383450513791238,
      "grad_norm": 3.2657253742218018,
      "learning_rate": 1.8792115459359666e-05,
      "loss": 4.8498,
      "num_input_tokens_seen": 366720838,
      "step": 10752,
      "train_runtime": 22800.2994,
      "train_tokens_per_second": 16084.036
    },
    {
      "epoch": 1.9441139354606092,
      "grad_norm": 3.441110849380493,
      "learning_rate": 1.8780094997492e-05,
      "loss": 4.8255,
      "num_input_tokens_seen": 367826694,
      "step": 10784,
      "train_runtime": 22864.6523,
      "train_tokens_per_second": 16087.133
    },
    {
      "epoch": 1.9498828195420947,
      "grad_norm": 3.2778372764587402,
      "learning_rate": 1.8768018903446415e-05,
      "loss": 4.7985,
      "num_input_tokens_seen": 368915206,
      "step": 10816,
      "train_runtime": 22928.3051,
      "train_tokens_per_second": 16089.947
    },
    {
      "epoch": 1.9556517036235803,
      "grad_norm": 3.4600894451141357,
      "learning_rate": 1.8755887253739103e-05,
      "loss": 4.8123,
      "num_input_tokens_seen": 370028166,
      "step": 10848,
      "train_runtime": 22992.9702,
      "train_tokens_per_second": 16093.1
    },
    {
      "epoch": 1.9614205877050659,
      "grad_norm": 3.7156145572662354,
      "learning_rate": 1.874370012523826e-05,
      "loss": 4.8622,
      "num_input_tokens_seen": 371124838,
      "step": 10880,
      "train_runtime": 23056.9048,
      "train_tokens_per_second": 16096.039
    },
    {
      "epoch": 1.9671894717865512,
      "grad_norm": 2.6852283477783203,
      "learning_rate": 1.8731457595163612e-05,
      "loss": 4.8071,
      "num_input_tokens_seen": 372213638,
      "step": 10912,
      "train_runtime": 23120.5463,
      "train_tokens_per_second": 16098.825
    },
    {
      "epoch": 1.9729583558680368,
      "grad_norm": 3.6780905723571777,
      "learning_rate": 1.8719159741085914e-05,
      "loss": 4.8374,
      "num_input_tokens_seen": 373330310,
      "step": 10944,
      "train_runtime": 23185.439,
      "train_tokens_per_second": 16101.93
    },
    {
      "epoch": 1.9787272399495222,
      "grad_norm": 3.0676581859588623,
      "learning_rate": 1.870680664092646e-05,
      "loss": 4.7672,
      "num_input_tokens_seen": 374433574,
      "step": 10976,
      "train_runtime": 23249.6992,
      "train_tokens_per_second": 16104.878
    },
    {
      "epoch": 1.9844961240310077,
      "grad_norm": 3.440450668334961,
      "learning_rate": 1.869439837295661e-05,
      "loss": 4.9121,
      "num_input_tokens_seen": 375548902,
      "step": 11008,
      "train_runtime": 23314.5429,
      "train_tokens_per_second": 16107.925
    },
    {
      "epoch": 1.9902650081124933,
      "grad_norm": 3.1507225036621094,
      "learning_rate": 1.8681935015797246e-05,
      "loss": 4.873,
      "num_input_tokens_seen": 376672102,
      "step": 11040,
      "train_runtime": 23379.6706,
      "train_tokens_per_second": 16111.095
    },
    {
      "epoch": 1.9960338921939789,
      "grad_norm": 3.3599352836608887,
      "learning_rate": 1.8669416648418343e-05,
      "loss": 4.8467,
      "num_input_tokens_seen": 377747078,
      "step": 11072,
      "train_runtime": 23442.1608,
      "train_tokens_per_second": 16114.004
    },
    {
      "epoch": 2.0,
      "eval_accuracy": 0.016872920701179264,
      "eval_class_accuracy": 0.46796748711791986,
      "eval_exact_token_accuracy": 0.0,
      "eval_f1": 0.033148860262899406,
      "eval_loss": 2.713247060775757,
      "eval_mean_iou": 0.14842971231045185,
      "eval_mean_iou_25": 0.14842971231045185,
      "eval_precision": 0.03461263843391495,
      "eval_prediction_target_similarity": 9.308982852516524e-05,
      "eval_recall": 0.03180687283161619,
      "eval_recall_25": 0.08052832523425267,
      "eval_runtime": 1278.108,
      "eval_samples_per_second": 3.874,
      "eval_sequence_accuracy": 0.0,
      "eval_steps_per_second": 0.121,
      "eval_top3_token_accuracy": 0.05769391357898712,
      "eval_top5_token_accuracy": 0.10684457421302795,
      "num_input_tokens_seen": 378492642,
      "step": 11094
    },
    {
      "epoch": 2.0018027762754644,
      "grad_norm": 3.468554973602295,
      "learning_rate": 1.8656843350138394e-05,
      "loss": 4.8073,
      "num_input_tokens_seen": 378839746,
      "step": 11104,
      "train_runtime": 24785.86,
      "train_tokens_per_second": 15284.511
    },
    {
      "epoch": 2.0075716603569496,
      "grad_norm": 3.2270212173461914,
      "learning_rate": 1.8644215200623966e-05,
      "loss": 4.8957,
      "num_input_tokens_seen": 379957634,
      "step": 11136,
      "train_runtime": 24851.1796,
      "train_tokens_per_second": 15289.32
    },
    {
      "epoch": 2.013340544438435,
      "grad_norm": 3.3108742237091064,
      "learning_rate": 1.8631532279889166e-05,
      "loss": 4.8198,
      "num_input_tokens_seen": 381062818,
      "step": 11168,
      "train_runtime": 24915.9562,
      "train_tokens_per_second": 15293.927
    },
    {
      "epoch": 2.0191094285199207,
      "grad_norm": 3.1885883808135986,
      "learning_rate": 1.861879466829514e-05,
      "loss": 4.8417,
      "num_input_tokens_seen": 382158978,
      "step": 11200,
      "train_runtime": 24979.8291,
      "train_tokens_per_second": 15298.703
    },
    {
      "epoch": 2.0248783126014063,
      "grad_norm": 3.8441436290740967,
      "learning_rate": 1.8606002446549563e-05,
      "loss": 4.7939,
      "num_input_tokens_seen": 383222626,
      "step": 11232,
      "train_runtime": 25042.3574,
      "train_tokens_per_second": 15302.977
    },
    {
      "epoch": 2.030647196682892,
      "grad_norm": 3.4187674522399902,
      "learning_rate": 1.8593155695706134e-05,
      "loss": 4.8274,
      "num_input_tokens_seen": 384320130,
      "step": 11264,
      "train_runtime": 25106.3727,
      "train_tokens_per_second": 15307.672
    },
    {
      "epoch": 2.036416080764377,
      "grad_norm": 3.1727030277252197,
      "learning_rate": 1.8580254497164057e-05,
      "loss": 4.8643,
      "num_input_tokens_seen": 385421282,
      "step": 11296,
      "train_runtime": 25170.426,
      "train_tokens_per_second": 15312.466
    },
    {
      "epoch": 2.0421849648458625,
      "grad_norm": 3.364508867263794,
      "learning_rate": 1.856729893266752e-05,
      "loss": 4.8417,
      "num_input_tokens_seen": 386533058,
      "step": 11328,
      "train_runtime": 25235.5093,
      "train_tokens_per_second": 15317.03
    },
    {
      "epoch": 2.047953848927348,
      "grad_norm": 3.271899938583374,
      "learning_rate": 1.855428908430519e-05,
      "loss": 4.7574,
      "num_input_tokens_seen": 387580514,
      "step": 11360,
      "train_runtime": 25297.5699,
      "train_tokens_per_second": 15320.86
    },
    {
      "epoch": 2.0537227330088337,
      "grad_norm": 3.2817232608795166,
      "learning_rate": 1.854122503450968e-05,
      "loss": 4.7484,
      "num_input_tokens_seen": 388653602,
      "step": 11392,
      "train_runtime": 25360.276,
      "train_tokens_per_second": 15325.291
    },
    {
      "epoch": 2.0594916170903192,
      "grad_norm": 3.5996592044830322,
      "learning_rate": 1.852810686605704e-05,
      "loss": 4.7991,
      "num_input_tokens_seen": 389746882,
      "step": 11424,
      "train_runtime": 25424.4427,
      "train_tokens_per_second": 15329.614
    },
    {
      "epoch": 2.0652605011718044,
      "grad_norm": 3.3787591457366943,
      "learning_rate": 1.8514934662066214e-05,
      "loss": 4.7306,
      "num_input_tokens_seen": 390844066,
      "step": 11456,
      "train_runtime": 25488.8845,
      "train_tokens_per_second": 15333.902
    },
    {
      "epoch": 2.07102938525329,
      "grad_norm": 3.095088243484497,
      "learning_rate": 1.8501708505998538e-05,
      "loss": 4.7997,
      "num_input_tokens_seen": 391946498,
      "step": 11488,
      "train_runtime": 25553.4593,
      "train_tokens_per_second": 15338.295
    },
    {
      "epoch": 2.0767982693347755,
      "grad_norm": 3.4296875,
      "learning_rate": 1.8488428481657185e-05,
      "loss": 4.803,
      "num_input_tokens_seen": 393061922,
      "step": 11520,
      "train_runtime": 25618.3537,
      "train_tokens_per_second": 15342.981
    },
    {
      "epoch": 2.082567153416261,
      "grad_norm": 3.5616397857666016,
      "learning_rate": 1.8475094673186657e-05,
      "loss": 4.8149,
      "num_input_tokens_seen": 394176354,
      "step": 11552,
      "train_runtime": 25683.3159,
      "train_tokens_per_second": 15347.565
    },
    {
      "epoch": 2.0883360374977467,
      "grad_norm": 3.4414913654327393,
      "learning_rate": 1.8461707165072233e-05,
      "loss": 4.7818,
      "num_input_tokens_seen": 395259522,
      "step": 11584,
      "train_runtime": 25746.4125,
      "train_tokens_per_second": 15352.023
    },
    {
      "epoch": 2.0941049215792322,
      "grad_norm": 3.344379186630249,
      "learning_rate": 1.8448266042139447e-05,
      "loss": 4.8276,
      "num_input_tokens_seen": 396344002,
      "step": 11616,
      "train_runtime": 25809.5336,
      "train_tokens_per_second": 15356.496
    },
    {
      "epoch": 2.0998738056607174,
      "grad_norm": 3.266462802886963,
      "learning_rate": 1.843477138955354e-05,
      "loss": 4.7668,
      "num_input_tokens_seen": 397434242,
      "step": 11648,
      "train_runtime": 25872.9019,
      "train_tokens_per_second": 15361.023
    },
    {
      "epoch": 2.105642689742203,
      "grad_norm": 3.3427441120147705,
      "learning_rate": 1.8421223292818936e-05,
      "loss": 4.8075,
      "num_input_tokens_seen": 398558434,
      "step": 11680,
      "train_runtime": 25938.0057,
      "train_tokens_per_second": 15365.809
    },
    {
      "epoch": 2.1114115738236885,
      "grad_norm": 3.285161018371582,
      "learning_rate": 1.840762183777868e-05,
      "loss": 4.7665,
      "num_input_tokens_seen": 399625986,
      "step": 11712,
      "train_runtime": 26000.6497,
      "train_tokens_per_second": 15369.846
    },
    {
      "epoch": 2.117180457905174,
      "grad_norm": 3.443773031234741,
      "learning_rate": 1.839396711061391e-05,
      "loss": 4.839,
      "num_input_tokens_seen": 400727458,
      "step": 11744,
      "train_runtime": 26064.5484,
      "train_tokens_per_second": 15374.426
    },
    {
      "epoch": 2.1229493419866596,
      "grad_norm": 3.034687042236328,
      "learning_rate": 1.8380259197843304e-05,
      "loss": 4.7845,
      "num_input_tokens_seen": 401801378,
      "step": 11776,
      "train_runtime": 26127.8182,
      "train_tokens_per_second": 15378.298
    },
    {
      "epoch": 2.1287182260681448,
      "grad_norm": 3.452768325805664,
      "learning_rate": 1.8366498186322534e-05,
      "loss": 4.74,
      "num_input_tokens_seen": 402870306,
      "step": 11808,
      "train_runtime": 26190.8079,
      "train_tokens_per_second": 15382.126
    },
    {
      "epoch": 2.1344871101496303,
      "grad_norm": 3.828434705734253,
      "learning_rate": 1.8352684163243714e-05,
      "loss": 4.815,
      "num_input_tokens_seen": 403977634,
      "step": 11840,
      "train_runtime": 26255.5306,
      "train_tokens_per_second": 15386.382
    },
    {
      "epoch": 2.140255994231116,
      "grad_norm": 3.843691110610962,
      "learning_rate": 1.8338817216134847e-05,
      "loss": 4.7948,
      "num_input_tokens_seen": 405045602,
      "step": 11872,
      "train_runtime": 26318.0735,
      "train_tokens_per_second": 15390.397
    },
    {
      "epoch": 2.1460248783126015,
      "grad_norm": 3.284762382507324,
      "learning_rate": 1.8324897432859267e-05,
      "loss": 4.835,
      "num_input_tokens_seen": 406139746,
      "step": 11904,
      "train_runtime": 26381.7145,
      "train_tokens_per_second": 15394.744
    },
    {
      "epoch": 2.151793762394087,
      "grad_norm": 3.4411888122558594,
      "learning_rate": 1.83109249016151e-05,
      "loss": 4.8595,
      "num_input_tokens_seen": 407227714,
      "step": 11936,
      "train_runtime": 26445.0195,
      "train_tokens_per_second": 15399.032
    },
    {
      "epoch": 2.157562646475572,
      "grad_norm": 3.3125131130218506,
      "learning_rate": 1.8296899710934687e-05,
      "loss": 4.8003,
      "num_input_tokens_seen": 408328098,
      "step": 11968,
      "train_runtime": 26509.01,
      "train_tokens_per_second": 15403.37
    },
    {
      "epoch": 2.1633315305570577,
      "grad_norm": 3.158552646636963,
      "learning_rate": 1.8282821949684018e-05,
      "loss": 4.7747,
      "num_input_tokens_seen": 409422946,
      "step": 12000,
      "train_runtime": 26572.6943,
      "train_tokens_per_second": 15407.656
    },
    {
      "epoch": 2.1691004146385433,
      "grad_norm": 3.0812339782714844,
      "learning_rate": 1.826869170706219e-05,
      "loss": 4.784,
      "num_input_tokens_seen": 410498594,
      "step": 12032,
      "train_runtime": 26635.4086,
      "train_tokens_per_second": 15411.763
    },
    {
      "epoch": 2.174869298720029,
      "grad_norm": 3.0808286666870117,
      "learning_rate": 1.825450907260083e-05,
      "loss": 4.827,
      "num_input_tokens_seen": 411617890,
      "step": 12064,
      "train_runtime": 26700.2004,
      "train_tokens_per_second": 15416.285
    },
    {
      "epoch": 2.1806381828015144,
      "grad_norm": 3.445492744445801,
      "learning_rate": 1.8240274136163526e-05,
      "loss": 4.8009,
      "num_input_tokens_seen": 412718722,
      "step": 12096,
      "train_runtime": 26764.43,
      "train_tokens_per_second": 15420.419
    },
    {
      "epoch": 2.186407066883,
      "grad_norm": 3.447911500930786,
      "learning_rate": 1.8225986987945257e-05,
      "loss": 4.7902,
      "num_input_tokens_seen": 413731138,
      "step": 12128,
      "train_runtime": 26824.2626,
      "train_tokens_per_second": 15423.766
    },
    {
      "epoch": 2.192175950964485,
      "grad_norm": 3.508444309234619,
      "learning_rate": 1.8211647718471834e-05,
      "loss": 4.8438,
      "num_input_tokens_seen": 414803202,
      "step": 12160,
      "train_runtime": 26887.2864,
      "train_tokens_per_second": 15427.485
    },
    {
      "epoch": 2.1979448350459707,
      "grad_norm": 3.779958724975586,
      "learning_rate": 1.8197256418599304e-05,
      "loss": 4.7703,
      "num_input_tokens_seen": 415908354,
      "step": 12192,
      "train_runtime": 26952.0908,
      "train_tokens_per_second": 15431.395
    },
    {
      "epoch": 2.2037137191274563,
      "grad_norm": 3.7972428798675537,
      "learning_rate": 1.81828131795134e-05,
      "loss": 4.8141,
      "num_input_tokens_seen": 416972290,
      "step": 12224,
      "train_runtime": 27014.8777,
      "train_tokens_per_second": 15434.913
    },
    {
      "epoch": 2.209482603208942,
      "grad_norm": 3.3046772480010986,
      "learning_rate": 1.8168318092728948e-05,
      "loss": 4.775,
      "num_input_tokens_seen": 418060642,
      "step": 12256,
      "train_runtime": 27078.7771,
      "train_tokens_per_second": 15438.682
    },
    {
      "epoch": 2.2152514872904274,
      "grad_norm": 3.2824478149414062,
      "learning_rate": 1.8153771250089286e-05,
      "loss": 4.7181,
      "num_input_tokens_seen": 419121602,
      "step": 12288,
      "train_runtime": 27141.4937,
      "train_tokens_per_second": 15442.098
    },
    {
      "epoch": 2.2210203713719125,
      "grad_norm": 3.281395673751831,
      "learning_rate": 1.8139172743765683e-05,
      "loss": 4.8503,
      "num_input_tokens_seen": 420251522,
      "step": 12320,
      "train_runtime": 27206.8413,
      "train_tokens_per_second": 15446.538
    },
    {
      "epoch": 2.226789255453398,
      "grad_norm": 3.278738260269165,
      "learning_rate": 1.812452266625677e-05,
      "loss": 4.8222,
      "num_input_tokens_seen": 421343810,
      "step": 12352,
      "train_runtime": 27270.3582,
      "train_tokens_per_second": 15450.615
    },
    {
      "epoch": 2.2325581395348837,
      "grad_norm": 3.1983351707458496,
      "learning_rate": 1.8109821110387927e-05,
      "loss": 4.8211,
      "num_input_tokens_seen": 422416674,
      "step": 12384,
      "train_runtime": 27333.2787,
      "train_tokens_per_second": 15454.299
    },
    {
      "epoch": 2.2383270236163693,
      "grad_norm": 3.738922595977783,
      "learning_rate": 1.8095068169310724e-05,
      "loss": 4.7814,
      "num_input_tokens_seen": 423507202,
      "step": 12416,
      "train_runtime": 27396.9229,
      "train_tokens_per_second": 15458.203
    },
    {
      "epoch": 2.244095907697855,
      "grad_norm": 3.3476314544677734,
      "learning_rate": 1.8080263936502302e-05,
      "loss": 4.8492,
      "num_input_tokens_seen": 424620162,
      "step": 12448,
      "train_runtime": 27461.6596,
      "train_tokens_per_second": 15462.291
    },
    {
      "epoch": 2.2498647917793404,
      "grad_norm": 3.2234630584716797,
      "learning_rate": 1.806540850576481e-05,
      "loss": 4.7673,
      "num_input_tokens_seen": 425688482,
      "step": 12480,
      "train_runtime": 27524.1156,
      "train_tokens_per_second": 15466.019
    },
    {
      "epoch": 2.2556336758608255,
      "grad_norm": 3.9680585861206055,
      "learning_rate": 1.805050197122478e-05,
      "loss": 4.7411,
      "num_input_tokens_seen": 426733346,
      "step": 12512,
      "train_runtime": 27585.3836,
      "train_tokens_per_second": 15469.545
    },
    {
      "epoch": 2.261402559942311,
      "grad_norm": 3.595560312271118,
      "learning_rate": 1.8035544427332564e-05,
      "loss": 4.7634,
      "num_input_tokens_seen": 427776514,
      "step": 12544,
      "train_runtime": 27646.5799,
      "train_tokens_per_second": 15473.036
    },
    {
      "epoch": 2.2671714440237967,
      "grad_norm": 3.510361433029175,
      "learning_rate": 1.80205359688617e-05,
      "loss": 4.7508,
      "num_input_tokens_seen": 428894498,
      "step": 12576,
      "train_runtime": 27711.332,
      "train_tokens_per_second": 15477.224
    },
    {
      "epoch": 2.2729403281052822,
      "grad_norm": 3.2013587951660156,
      "learning_rate": 1.8005476690908343e-05,
      "loss": 4.8197,
      "num_input_tokens_seen": 430013122,
      "step": 12608,
      "train_runtime": 27776.1346,
      "train_tokens_per_second": 15481.388
    },
    {
      "epoch": 2.278709212186768,
      "grad_norm": 3.4004428386688232,
      "learning_rate": 1.7990366688890646e-05,
      "loss": 4.7721,
      "num_input_tokens_seen": 431121122,
      "step": 12640,
      "train_runtime": 27840.3515,
      "train_tokens_per_second": 15485.477
    },
    {
      "epoch": 2.284478096268253,
      "grad_norm": 3.3988499641418457,
      "learning_rate": 1.797520605854815e-05,
      "loss": 4.7553,
      "num_input_tokens_seen": 432205218,
      "step": 12672,
      "train_runtime": 27903.4429,
      "train_tokens_per_second": 15489.315
    },
    {
      "epoch": 2.2902469803497385,
      "grad_norm": 3.432121992111206,
      "learning_rate": 1.7959994895941197e-05,
      "loss": 4.7792,
      "num_input_tokens_seen": 433311842,
      "step": 12704,
      "train_runtime": 27967.6275,
      "train_tokens_per_second": 15493.336
    },
    {
      "epoch": 2.296015864431224,
      "grad_norm": 3.2715048789978027,
      "learning_rate": 1.79447332974503e-05,
      "loss": 4.7424,
      "num_input_tokens_seen": 434367330,
      "step": 12736,
      "train_runtime": 28029.457,
      "train_tokens_per_second": 15496.816
    },
    {
      "epoch": 2.3017847485127096,
      "grad_norm": 2.963762044906616,
      "learning_rate": 1.7929421359775556e-05,
      "loss": 4.7533,
      "num_input_tokens_seen": 435429794,
      "step": 12768,
      "train_runtime": 28091.592,
      "train_tokens_per_second": 15500.36
    },
    {
      "epoch": 2.307553632594195,
      "grad_norm": 3.4580347537994385,
      "learning_rate": 1.7914059179936005e-05,
      "loss": 4.825,
      "num_input_tokens_seen": 436518722,
      "step": 12800,
      "train_runtime": 28154.9604,
      "train_tokens_per_second": 15504.15
    },
    {
      "epoch": 2.3133225166756803,
      "grad_norm": 3.327214479446411,
      "learning_rate": 1.7898646855269036e-05,
      "loss": 4.8054,
      "num_input_tokens_seen": 437599778,
      "step": 12832,
      "train_runtime": 28217.9734,
      "train_tokens_per_second": 15507.839
    },
    {
      "epoch": 2.319091400757166,
      "grad_norm": 3.580523729324341,
      "learning_rate": 1.7883184483429765e-05,
      "loss": 4.815,
      "num_input_tokens_seen": 438661314,
      "step": 12864,
      "train_runtime": 28279.8789,
      "train_tokens_per_second": 15511.428
    },
    {
      "epoch": 2.3248602848386515,
      "grad_norm": 3.532899856567383,
      "learning_rate": 1.7867672162390414e-05,
      "loss": 4.7729,
      "num_input_tokens_seen": 439771682,
      "step": 12896,
      "train_runtime": 28344.0577,
      "train_tokens_per_second": 15515.481
    },
    {
      "epoch": 2.330629168920137,
      "grad_norm": 3.843994617462158,
      "learning_rate": 1.7852109990439697e-05,
      "loss": 4.8393,
      "num_input_tokens_seen": 440870466,
      "step": 12928,
      "train_runtime": 28407.7184,
      "train_tokens_per_second": 15519.39
    },
    {
      "epoch": 2.3363980530016226,
      "grad_norm": 3.2288103103637695,
      "learning_rate": 1.7836498066182176e-05,
      "loss": 4.8351,
      "num_input_tokens_seen": 441959970,
      "step": 12960,
      "train_runtime": 28471.2822,
      "train_tokens_per_second": 15523.009
    },
    {
      "epoch": 2.3421669370831077,
      "grad_norm": 3.1715989112854004,
      "learning_rate": 1.7820836488537676e-05,
      "loss": 4.7846,
      "num_input_tokens_seen": 443019234,
      "step": 12992,
      "train_runtime": 28533.7099,
      "train_tokens_per_second": 15526.17
    },
    {
      "epoch": 2.3479358211645933,
      "grad_norm": 3.2307491302490234,
      "learning_rate": 1.7805125356740613e-05,
      "loss": 4.7784,
      "num_input_tokens_seen": 444128802,
      "step": 13024,
      "train_runtime": 28598.3238,
      "train_tokens_per_second": 15529.889
    },
    {
      "epoch": 2.353704705246079,
      "grad_norm": 3.3719308376312256,
      "learning_rate": 1.77893647703394e-05,
      "loss": 4.7558,
      "num_input_tokens_seen": 445193346,
      "step": 13056,
      "train_runtime": 28660.4735,
      "train_tokens_per_second": 15533.356
    },
    {
      "epoch": 2.3594735893275645,
      "grad_norm": 3.670384645462036,
      "learning_rate": 1.777355482919579e-05,
      "loss": 4.8257,
      "num_input_tokens_seen": 446323426,
      "step": 13088,
      "train_runtime": 28726.4148,
      "train_tokens_per_second": 15537.039
    },
    {
      "epoch": 2.36524247340905,
      "grad_norm": 3.6692888736724854,
      "learning_rate": 1.7757695633484277e-05,
      "loss": 4.8025,
      "num_input_tokens_seen": 447427266,
      "step": 13120,
      "train_runtime": 28791.0773,
      "train_tokens_per_second": 15540.484
    },
    {
      "epoch": 2.3710113574905356,
      "grad_norm": 3.0041160583496094,
      "learning_rate": 1.7741787283691413e-05,
      "loss": 4.8396,
      "num_input_tokens_seen": 448569730,
      "step": 13152,
      "train_runtime": 28857.5446,
      "train_tokens_per_second": 15544.279
    },
    {
      "epoch": 2.3767802415720207,
      "grad_norm": 3.2724838256835938,
      "learning_rate": 1.7725829880615223e-05,
      "loss": 4.8306,
      "num_input_tokens_seen": 449679330,
      "step": 13184,
      "train_runtime": 28922.1642,
      "train_tokens_per_second": 15547.914
    },
    {
      "epoch": 2.3825491256535063,
      "grad_norm": 3.282518148422241,
      "learning_rate": 1.7709823525364526e-05,
      "loss": 4.7939,
      "num_input_tokens_seen": 450802242,
      "step": 13216,
      "train_runtime": 28987.108,
      "train_tokens_per_second": 15551.818
    },
    {
      "epoch": 2.388318009734992,
      "grad_norm": 3.0607781410217285,
      "learning_rate": 1.769376831935832e-05,
      "loss": 4.7882,
      "num_input_tokens_seen": 451901922,
      "step": 13248,
      "train_runtime": 29051.0069,
      "train_tokens_per_second": 15555.465
    },
    {
      "epoch": 2.3940868938164774,
      "grad_norm": 3.230024814605713,
      "learning_rate": 1.7677664364325124e-05,
      "loss": 4.7404,
      "num_input_tokens_seen": 452961890,
      "step": 13280,
      "train_runtime": 29113.0303,
      "train_tokens_per_second": 15558.734
    },
    {
      "epoch": 2.399855777897963,
      "grad_norm": 3.7594852447509766,
      "learning_rate": 1.766151176230234e-05,
      "loss": 4.7721,
      "num_input_tokens_seen": 454054498,
      "step": 13312,
      "train_runtime": 29176.5736,
      "train_tokens_per_second": 15562.297
    },
    {
      "epoch": 2.4056246619794486,
      "grad_norm": 3.5373871326446533,
      "learning_rate": 1.764531061563561e-05,
      "loss": 4.8004,
      "num_input_tokens_seen": 455144194,
      "step": 13344,
      "train_runtime": 29239.9451,
      "train_tokens_per_second": 15565.836
    },
    {
      "epoch": 2.4113935460609337,
      "grad_norm": 3.8250112533569336,
      "learning_rate": 1.762906102697816e-05,
      "loss": 4.8412,
      "num_input_tokens_seen": 456269026,
      "step": 13376,
      "train_runtime": 29305.0177,
      "train_tokens_per_second": 15569.655
    },
    {
      "epoch": 2.4171624301424193,
      "grad_norm": 3.331615686416626,
      "learning_rate": 1.761276309929014e-05,
      "loss": 4.7865,
      "num_input_tokens_seen": 457374466,
      "step": 13408,
      "train_runtime": 29369.1691,
      "train_tokens_per_second": 15573.286
    },
    {
      "epoch": 2.422931314223905,
      "grad_norm": 3.3927764892578125,
      "learning_rate": 1.7596416935838016e-05,
      "loss": 4.8004,
      "num_input_tokens_seen": 458459970,
      "step": 13440,
      "train_runtime": 29432.303,
      "train_tokens_per_second": 15576.762
    },
    {
      "epoch": 2.4287001983053904,
      "grad_norm": 3.5234830379486084,
      "learning_rate": 1.7580022640193856e-05,
      "loss": 4.8172,
      "num_input_tokens_seen": 459542466,
      "step": 13472,
      "train_runtime": 29495.3558,
      "train_tokens_per_second": 15580.163
    },
    {
      "epoch": 2.434469082386876,
      "grad_norm": 3.6478989124298096,
      "learning_rate": 1.7563580316234707e-05,
      "loss": 4.8192,
      "num_input_tokens_seen": 460657218,
      "step": 13504,
      "train_runtime": 29559.7774,
      "train_tokens_per_second": 15583.92
    },
    {
      "epoch": 2.440237966468361,
      "grad_norm": 3.1556758880615234,
      "learning_rate": 1.754709006814194e-05,
      "loss": 4.6988,
      "num_input_tokens_seen": 461770882,
      "step": 13536,
      "train_runtime": 29624.0703,
      "train_tokens_per_second": 15587.692
    },
    {
      "epoch": 2.4460068505498467,
      "grad_norm": 3.2956807613372803,
      "learning_rate": 1.7530552000400572e-05,
      "loss": 4.738,
      "num_input_tokens_seen": 462854946,
      "step": 13568,
      "train_runtime": 29687.7437,
      "train_tokens_per_second": 15590.775
    },
    {
      "epoch": 2.4517757346313322,
      "grad_norm": 3.423912286758423,
      "learning_rate": 1.7513966217798624e-05,
      "loss": 4.8053,
      "num_input_tokens_seen": 463983234,
      "step": 13600,
      "train_runtime": 29753.0844,
      "train_tokens_per_second": 15594.458
    },
    {
      "epoch": 2.457544618712818,
      "grad_norm": 3.303903579711914,
      "learning_rate": 1.7497332825426435e-05,
      "loss": 4.7515,
      "num_input_tokens_seen": 465046210,
      "step": 13632,
      "train_runtime": 29815.3378,
      "train_tokens_per_second": 15597.55
    },
    {
      "epoch": 2.4633135027943034,
      "grad_norm": 3.501619338989258,
      "learning_rate": 1.748065192867602e-05,
      "loss": 4.7949,
      "num_input_tokens_seen": 466115842,
      "step": 13664,
      "train_runtime": 29878.0253,
      "train_tokens_per_second": 15600.624
    },
    {
      "epoch": 2.4690823868757885,
      "grad_norm": 3.4068100452423096,
      "learning_rate": 1.7463923633240378e-05,
      "loss": 4.8291,
      "num_input_tokens_seen": 467215778,
      "step": 13696,
      "train_runtime": 29942.5882,
      "train_tokens_per_second": 15603.721
    },
    {
      "epoch": 2.474851270957274,
      "grad_norm": 3.5598526000976562,
      "learning_rate": 1.7447148045112838e-05,
      "loss": 4.8054,
      "num_input_tokens_seen": 468314018,
      "step": 13728,
      "train_runtime": 30006.948,
      "train_tokens_per_second": 15606.853
    },
    {
      "epoch": 2.4806201550387597,
      "grad_norm": 3.7765655517578125,
      "learning_rate": 1.743032527058639e-05,
      "loss": 4.8451,
      "num_input_tokens_seen": 469404482,
      "step": 13760,
      "train_runtime": 30070.9343,
      "train_tokens_per_second": 15609.907
    },
    {
      "epoch": 2.4863890391202452,
      "grad_norm": 3.485823154449463,
      "learning_rate": 1.7413455416252992e-05,
      "loss": 4.7981,
      "num_input_tokens_seen": 470500194,
      "step": 13792,
      "train_runtime": 30135.3224,
      "train_tokens_per_second": 15612.914
    },
    {
      "epoch": 2.492157923201731,
      "grad_norm": 3.264247179031372,
      "learning_rate": 1.7396538589002923e-05,
      "loss": 4.8177,
      "num_input_tokens_seen": 471605154,
      "step": 13824,
      "train_runtime": 30199.9537,
      "train_tokens_per_second": 15616.089
    },
    {
      "epoch": 2.497926807283216,
      "grad_norm": 3.2174904346466064,
      "learning_rate": 1.7379574896024077e-05,
      "loss": 4.7316,
      "num_input_tokens_seen": 472687458,
      "step": 13856,
      "train_runtime": 30263.5167,
      "train_tokens_per_second": 15619.053
    },
    {
      "epoch": 2.5036956913647015,
      "grad_norm": 3.5689098834991455,
      "learning_rate": 1.736256444480131e-05,
      "loss": 4.8323,
      "num_input_tokens_seen": 473759778,
      "step": 13888,
      "train_runtime": 30326.8772,
      "train_tokens_per_second": 15621.779
    },
    {
      "epoch": 2.509464575446187,
      "grad_norm": 3.190373659133911,
      "learning_rate": 1.7345507343115735e-05,
      "loss": 4.7922,
      "num_input_tokens_seen": 474840898,
      "step": 13920,
      "train_runtime": 30390.3761,
      "train_tokens_per_second": 15624.713
    },
    {
      "epoch": 2.5152334595276726,
      "grad_norm": 3.542760133743286,
      "learning_rate": 1.7328403699044053e-05,
      "loss": 4.7973,
      "num_input_tokens_seen": 475920322,
      "step": 13952,
      "train_runtime": 30453.7563,
      "train_tokens_per_second": 15627.639
    },
    {
      "epoch": 2.521002343609158,
      "grad_norm": 3.5077402591705322,
      "learning_rate": 1.7311253620957868e-05,
      "loss": 4.763,
      "num_input_tokens_seen": 476987426,
      "step": 13984,
      "train_runtime": 30516.6122,
      "train_tokens_per_second": 15630.419
    },
    {
      "epoch": 2.5267712276906433,
      "grad_norm": 3.609607696533203,
      "learning_rate": 1.7294057217523e-05,
      "loss": 4.876,
      "num_input_tokens_seen": 478074146,
      "step": 14016,
      "train_runtime": 30580.4404,
      "train_tokens_per_second": 15633.331
    },
    {
      "epoch": 2.5325401117721293,
      "grad_norm": 3.14735746383667,
      "learning_rate": 1.727681459769879e-05,
      "loss": 4.7561,
      "num_input_tokens_seen": 479182146,
      "step": 14048,
      "train_runtime": 30644.8364,
      "train_tokens_per_second": 15636.636
    },
    {
      "epoch": 2.5383089958536145,
      "grad_norm": 3.262284994125366,
      "learning_rate": 1.7259525870737413e-05,
      "loss": 4.775,
      "num_input_tokens_seen": 480284514,
      "step": 14080,
      "train_runtime": 30709.1104,
      "train_tokens_per_second": 15639.806
    },
    {
      "epoch": 2.5440778799351,
      "grad_norm": 3.78977108001709,
      "learning_rate": 1.7242191146183187e-05,
      "loss": 4.816,
      "num_input_tokens_seen": 481384418,
      "step": 14112,
      "train_runtime": 30773.468,
      "train_tokens_per_second": 15642.839
    },
    {
      "epoch": 2.5498467640165856,
      "grad_norm": 3.420468330383301,
      "learning_rate": 1.722481053387189e-05,
      "loss": 4.767,
      "num_input_tokens_seen": 482470082,
      "step": 14144,
      "train_runtime": 30837.01,
      "train_tokens_per_second": 15645.813
    },
    {
      "epoch": 2.555615648098071,
      "grad_norm": 2.891084909439087,
      "learning_rate": 1.720738414393003e-05,
      "loss": 4.7519,
      "num_input_tokens_seen": 483547234,
      "step": 14176,
      "train_runtime": 30900.0009,
      "train_tokens_per_second": 15648.777
    },
    {
      "epoch": 2.5613845321795568,
      "grad_norm": 3.316148042678833,
      "learning_rate": 1.718991208677419e-05,
      "loss": 4.7994,
      "num_input_tokens_seen": 484610434,
      "step": 14208,
      "train_runtime": 30962.3234,
      "train_tokens_per_second": 15651.617
    },
    {
      "epoch": 2.567153416261042,
      "grad_norm": 3.1591265201568604,
      "learning_rate": 1.7172394473110302e-05,
      "loss": 4.7846,
      "num_input_tokens_seen": 485644994,
      "step": 14240,
      "train_runtime": 31023.4717,
      "train_tokens_per_second": 15654.115
    },
    {
      "epoch": 2.5729223003425274,
      "grad_norm": 2.9113166332244873,
      "learning_rate": 1.7154831413932952e-05,
      "loss": 4.8051,
      "num_input_tokens_seen": 486705602,
      "step": 14272,
      "train_runtime": 31085.6817,
      "train_tokens_per_second": 15656.906
    },
    {
      "epoch": 2.578691184424013,
      "grad_norm": 3.5169012546539307,
      "learning_rate": 1.713722302052467e-05,
      "loss": 4.7266,
      "num_input_tokens_seen": 487816354,
      "step": 14304,
      "train_runtime": 31150.4198,
      "train_tokens_per_second": 15660.025
    },
    {
      "epoch": 2.5844600685054986,
      "grad_norm": 3.040200710296631,
      "learning_rate": 1.711956940445524e-05,
      "loss": 4.8633,
      "num_input_tokens_seen": 488959234,
      "step": 14336,
      "train_runtime": 31216.4799,
      "train_tokens_per_second": 15663.497
    },
    {
      "epoch": 2.590228952586984,
      "grad_norm": 3.8032662868499756,
      "learning_rate": 1.710187067758098e-05,
      "loss": 4.7693,
      "num_input_tokens_seen": 490002050,
      "step": 14368,
      "train_runtime": 31277.7233,
      "train_tokens_per_second": 15666.167
    },
    {
      "epoch": 2.5959978366684693,
      "grad_norm": 3.44339656829834,
      "learning_rate": 1.7084126952044046e-05,
      "loss": 4.7691,
      "num_input_tokens_seen": 491058210,
      "step": 14400,
      "train_runtime": 31339.7025,
      "train_tokens_per_second": 15668.885
    },
    {
      "epoch": 2.601766720749955,
      "grad_norm": 3.5905134677886963,
      "learning_rate": 1.70663383402717e-05,
      "loss": 4.8458,
      "num_input_tokens_seen": 492172034,
      "step": 14432,
      "train_runtime": 31404.3697,
      "train_tokens_per_second": 15672.088
    },
    {
      "epoch": 2.6075356048314404,
      "grad_norm": 3.307187080383301,
      "learning_rate": 1.704850495497561e-05,
      "loss": 4.7894,
      "num_input_tokens_seen": 493286402,
      "step": 14464,
      "train_runtime": 31469.1294,
      "train_tokens_per_second": 15675.248
    },
    {
      "epoch": 2.613304488912926,
      "grad_norm": 3.1869804859161377,
      "learning_rate": 1.7030626909151156e-05,
      "loss": 4.781,
      "num_input_tokens_seen": 494364354,
      "step": 14496,
      "train_runtime": 31532.2295,
      "train_tokens_per_second": 15678.065
    },
    {
      "epoch": 2.6190733729944116,
      "grad_norm": 3.518170118331909,
      "learning_rate": 1.7012704316076678e-05,
      "loss": 4.7865,
      "num_input_tokens_seen": 495441026,
      "step": 14528,
      "train_runtime": 31595.1557,
      "train_tokens_per_second": 15680.917
    },
    {
      "epoch": 2.6248422570758967,
      "grad_norm": 3.3287415504455566,
      "learning_rate": 1.699473728931278e-05,
      "loss": 4.752,
      "num_input_tokens_seen": 496525442,
      "step": 14560,
      "train_runtime": 31658.3517,
      "train_tokens_per_second": 15683.869
    },
    {
      "epoch": 2.6306111411573823,
      "grad_norm": 3.549618721008301,
      "learning_rate": 1.6976725942701603e-05,
      "loss": 4.8223,
      "num_input_tokens_seen": 497635202,
      "step": 14592,
      "train_runtime": 31722.8544,
      "train_tokens_per_second": 15686.962
    },
    {
      "epoch": 2.636380025238868,
      "grad_norm": 3.337446928024292,
      "learning_rate": 1.6958670390366113e-05,
      "loss": 4.8176,
      "num_input_tokens_seen": 498676674,
      "step": 14624,
      "train_runtime": 31784.0183,
      "train_tokens_per_second": 15689.541
    },
    {
      "epoch": 2.6421489093203534,
      "grad_norm": 3.0332226753234863,
      "learning_rate": 1.694057074670936e-05,
      "loss": 4.7831,
      "num_input_tokens_seen": 499791714,
      "step": 14656,
      "train_runtime": 31848.6953,
      "train_tokens_per_second": 15692.69
    },
    {
      "epoch": 2.647917793401839,
      "grad_norm": 3.258985996246338,
      "learning_rate": 1.6922427126413768e-05,
      "loss": 4.7668,
      "num_input_tokens_seen": 500877410,
      "step": 14688,
      "train_runtime": 31912.0118,
      "train_tokens_per_second": 15695.576
    },
    {
      "epoch": 2.653686677483324,
      "grad_norm": 3.40651798248291,
      "learning_rate": 1.690423964444042e-05,
      "loss": 4.8143,
      "num_input_tokens_seen": 501955906,
      "step": 14720,
      "train_runtime": 31975.0774,
      "train_tokens_per_second": 15698.348
    },
    {
      "epoch": 2.6594555615648097,
      "grad_norm": 3.4536848068237305,
      "learning_rate": 1.6886008416028285e-05,
      "loss": 4.7847,
      "num_input_tokens_seen": 503031586,
      "step": 14752,
      "train_runtime": 32037.9265,
      "train_tokens_per_second": 15701.128
    },
    {
      "epoch": 2.6652244456462952,
      "grad_norm": 3.9042882919311523,
      "learning_rate": 1.6867733556693544e-05,
      "loss": 4.7431,
      "num_input_tokens_seen": 504119554,
      "step": 14784,
      "train_runtime": 32101.4486,
      "train_tokens_per_second": 15703.95
    },
    {
      "epoch": 2.670993329727781,
      "grad_norm": 3.1403098106384277,
      "learning_rate": 1.6849415182228814e-05,
      "loss": 4.7425,
      "num_input_tokens_seen": 505253762,
      "step": 14816,
      "train_runtime": 32167.1881,
      "train_tokens_per_second": 15707.116
    },
    {
      "epoch": 2.6767622138092664,
      "grad_norm": 3.486340284347534,
      "learning_rate": 1.683105340870244e-05,
      "loss": 4.7836,
      "num_input_tokens_seen": 506347714,
      "step": 14848,
      "train_runtime": 32230.9273,
      "train_tokens_per_second": 15709.995
    },
    {
      "epoch": 2.6825310978907515,
      "grad_norm": 3.242480993270874,
      "learning_rate": 1.681264835245774e-05,
      "loss": 4.8584,
      "num_input_tokens_seen": 507462754,
      "step": 14880,
      "train_runtime": 32295.7087,
      "train_tokens_per_second": 15713.009
    },
    {
      "epoch": 2.6882999819722375,
      "grad_norm": 3.3287830352783203,
      "learning_rate": 1.6794200130112297e-05,
      "loss": 4.7769,
      "num_input_tokens_seen": 508529890,
      "step": 14912,
      "train_runtime": 32358.2335,
      "train_tokens_per_second": 15715.626
    },
    {
      "epoch": 2.6940688660537226,
      "grad_norm": 3.2287113666534424,
      "learning_rate": 1.6775708858557184e-05,
      "loss": 4.7737,
      "num_input_tokens_seen": 509625314,
      "step": 14944,
      "train_runtime": 32421.9469,
      "train_tokens_per_second": 15718.529
    },
    {
      "epoch": 2.699837750135208,
      "grad_norm": 3.037113666534424,
      "learning_rate": 1.675717465495625e-05,
      "loss": 4.8153,
      "num_input_tokens_seen": 510673250,
      "step": 14976,
      "train_runtime": 32483.5213,
      "train_tokens_per_second": 15720.994
    },
    {
      "epoch": 2.705606634216694,
      "grad_norm": 3.16595196723938,
      "learning_rate": 1.6738597636745367e-05,
      "loss": 4.8323,
      "num_input_tokens_seen": 511843170,
      "step": 15008,
      "train_runtime": 32550.9213,
      "train_tokens_per_second": 15724.383
    },
    {
      "epoch": 2.7113755182981794,
      "grad_norm": 3.204251766204834,
      "learning_rate": 1.671997792163169e-05,
      "loss": 4.7223,
      "num_input_tokens_seen": 512876322,
      "step": 15040,
      "train_runtime": 32611.7974,
      "train_tokens_per_second": 15726.711
    },
    {
      "epoch": 2.717144402379665,
      "grad_norm": 3.2147159576416016,
      "learning_rate": 1.6701315627592904e-05,
      "loss": 4.8249,
      "num_input_tokens_seen": 513959810,
      "step": 15072,
      "train_runtime": 32674.9616,
      "train_tokens_per_second": 15729.469
    },
    {
      "epoch": 2.72291328646115,
      "grad_norm": 3.0851519107818604,
      "learning_rate": 1.6682610872876484e-05,
      "loss": 4.7485,
      "num_input_tokens_seen": 515077538,
      "step": 15104,
      "train_runtime": 32739.7434,
      "train_tokens_per_second": 15732.485
    },
    {
      "epoch": 2.7286821705426356,
      "grad_norm": 3.5235331058502197,
      "learning_rate": 1.6663863775998942e-05,
      "loss": 4.8245,
      "num_input_tokens_seen": 516163330,
      "step": 15136,
      "train_runtime": 32803.1126,
      "train_tokens_per_second": 15735.194
    },
    {
      "epoch": 2.734451054624121,
      "grad_norm": 3.5074806213378906,
      "learning_rate": 1.664507445574508e-05,
      "loss": 4.8143,
      "num_input_tokens_seen": 517289410,
      "step": 15168,
      "train_runtime": 32868.4513,
      "train_tokens_per_second": 15738.174
    },
    {
      "epoch": 2.7402199387056068,
      "grad_norm": 3.189572811126709,
      "learning_rate": 1.6626243031167236e-05,
      "loss": 4.7728,
      "num_input_tokens_seen": 518412130,
      "step": 15200,
      "train_runtime": 32933.6041,
      "train_tokens_per_second": 15741.13
    },
    {
      "epoch": 2.7459888227870923,
      "grad_norm": 3.6484272480010986,
      "learning_rate": 1.6607369621584515e-05,
      "loss": 4.797,
      "num_input_tokens_seen": 519563074,
      "step": 15232,
      "train_runtime": 33000.1569,
      "train_tokens_per_second": 15744.261
    },
    {
      "epoch": 2.7517577068685775,
      "grad_norm": 4.347327709197998,
      "learning_rate": 1.6588454346582063e-05,
      "loss": 4.7604,
      "num_input_tokens_seen": 520631746,
      "step": 15264,
      "train_runtime": 33062.7153,
      "train_tokens_per_second": 15746.793
    },
    {
      "epoch": 2.757526590950063,
      "grad_norm": 3.17333722114563,
      "learning_rate": 1.656949732601029e-05,
      "loss": 4.7458,
      "num_input_tokens_seen": 521754594,
      "step": 15296,
      "train_runtime": 33127.6872,
      "train_tokens_per_second": 15749.804
    },
    {
      "epoch": 2.7632954750315486,
      "grad_norm": 3.361931562423706,
      "learning_rate": 1.6550498679984092e-05,
      "loss": 4.7437,
      "num_input_tokens_seen": 522815458,
      "step": 15328,
      "train_runtime": 33189.9906,
      "train_tokens_per_second": 15752.203
    },
    {
      "epoch": 2.769064359113034,
      "grad_norm": 3.5736141204833984,
      "learning_rate": 1.6531458528882145e-05,
      "loss": 4.6896,
      "num_input_tokens_seen": 523895778,
      "step": 15360,
      "train_runtime": 33253.2254,
      "train_tokens_per_second": 15754.736
    },
    {
      "epoch": 2.7748332431945197,
      "grad_norm": 3.532297372817993,
      "learning_rate": 1.6512376993346084e-05,
      "loss": 4.7934,
      "num_input_tokens_seen": 524973026,
      "step": 15392,
      "train_runtime": 33316.2719,
      "train_tokens_per_second": 15757.256
    },
    {
      "epoch": 2.780602127276005,
      "grad_norm": 3.5516674518585205,
      "learning_rate": 1.649325419427976e-05,
      "loss": 4.8388,
      "num_input_tokens_seen": 526065346,
      "step": 15424,
      "train_runtime": 33379.9238,
      "train_tokens_per_second": 15759.933
    },
    {
      "epoch": 2.7863710113574904,
      "grad_norm": 3.2327845096588135,
      "learning_rate": 1.64740902528485e-05,
      "loss": 4.746,
      "num_input_tokens_seen": 527096930,
      "step": 15456,
      "train_runtime": 33440.7212,
      "train_tokens_per_second": 15762.128
    },
    {
      "epoch": 2.792139895438976,
      "grad_norm": 3.6199302673339844,
      "learning_rate": 1.64548852904783e-05,
      "loss": 4.7873,
      "num_input_tokens_seen": 528231042,
      "step": 15488,
      "train_runtime": 33506.2122,
      "train_tokens_per_second": 15765.167
    },
    {
      "epoch": 2.7979087795204616,
      "grad_norm": 2.9213647842407227,
      "learning_rate": 1.643563942885507e-05,
      "loss": 4.7383,
      "num_input_tokens_seen": 529330306,
      "step": 15520,
      "train_runtime": 33570.3118,
      "train_tokens_per_second": 15767.81
    },
    {
      "epoch": 2.803677663601947,
      "grad_norm": 3.4969496726989746,
      "learning_rate": 1.6416352789923865e-05,
      "loss": 4.7903,
      "num_input_tokens_seen": 530447682,
      "step": 15552,
      "train_runtime": 33635.2074,
      "train_tokens_per_second": 15770.608
    },
    {
      "epoch": 2.8094465476834323,
      "grad_norm": 3.276737928390503,
      "learning_rate": 1.6397025495888125e-05,
      "loss": 4.7592,
      "num_input_tokens_seen": 531529218,
      "step": 15584,
      "train_runtime": 33698.2316,
      "train_tokens_per_second": 15773.208
    },
    {
      "epoch": 2.815215431764918,
      "grad_norm": 3.1328630447387695,
      "learning_rate": 1.637765766920887e-05,
      "loss": 4.7518,
      "num_input_tokens_seen": 532587810,
      "step": 15616,
      "train_runtime": 33760.3911,
      "train_tokens_per_second": 15775.523
    },
    {
      "epoch": 2.8209843158464034,
      "grad_norm": 3.3733508586883545,
      "learning_rate": 1.635824943260395e-05,
      "loss": 4.7587,
      "num_input_tokens_seen": 533635458,
      "step": 15648,
      "train_runtime": 33822.0158,
      "train_tokens_per_second": 15777.754
    },
    {
      "epoch": 2.826753199927889,
      "grad_norm": 3.201986312866211,
      "learning_rate": 1.633880090904727e-05,
      "loss": 4.7527,
      "num_input_tokens_seen": 534738850,
      "step": 15680,
      "train_runtime": 33886.2253,
      "train_tokens_per_second": 15780.419
    },
    {
      "epoch": 2.8325220840093746,
      "grad_norm": 3.294613838195801,
      "learning_rate": 1.631931222176797e-05,
      "loss": 4.7554,
      "num_input_tokens_seen": 535847490,
      "step": 15712,
      "train_runtime": 33951.3869,
      "train_tokens_per_second": 15782.786
    },
    {
      "epoch": 2.8382909680908597,
      "grad_norm": 3.119830846786499,
      "learning_rate": 1.6299783494249718e-05,
      "loss": 4.8257,
      "num_input_tokens_seen": 536938082,
      "step": 15744,
      "train_runtime": 34015.613,
      "train_tokens_per_second": 15785.048
    },
    {
      "epoch": 2.8440598521723452,
      "grad_norm": 3.2075188159942627,
      "learning_rate": 1.628021485022983e-05,
      "loss": 4.8376,
      "num_input_tokens_seen": 537996002,
      "step": 15776,
      "train_runtime": 34077.8307,
      "train_tokens_per_second": 15787.273
    },
    {
      "epoch": 2.849828736253831,
      "grad_norm": 3.542630434036255,
      "learning_rate": 1.6260606413698594e-05,
      "loss": 4.8004,
      "num_input_tokens_seen": 539129794,
      "step": 15808,
      "train_runtime": 34143.6233,
      "train_tokens_per_second": 15790.058
    },
    {
      "epoch": 2.8555976203353164,
      "grad_norm": 3.5401690006256104,
      "learning_rate": 1.6240958308898392e-05,
      "loss": 4.7725,
      "num_input_tokens_seen": 540212066,
      "step": 15840,
      "train_runtime": 34207.4541,
      "train_tokens_per_second": 15792.232
    },
    {
      "epoch": 2.861366504416802,
      "grad_norm": 3.225186824798584,
      "learning_rate": 1.6221270660322967e-05,
      "loss": 4.7933,
      "num_input_tokens_seen": 541313026,
      "step": 15872,
      "train_runtime": 34272.1481,
      "train_tokens_per_second": 15794.546
    },
    {
      "epoch": 2.867135388498287,
      "grad_norm": 3.2544546127319336,
      "learning_rate": 1.6201543592716612e-05,
      "loss": 4.6824,
      "num_input_tokens_seen": 542368130,
      "step": 15904,
      "train_runtime": 34334.7182,
      "train_tokens_per_second": 15796.493
    },
    {
      "epoch": 2.872904272579773,
      "grad_norm": 3.3351120948791504,
      "learning_rate": 1.6181777231073395e-05,
      "loss": 4.7676,
      "num_input_tokens_seen": 543441954,
      "step": 15936,
      "train_runtime": 34397.7785,
      "train_tokens_per_second": 15798.751
    },
    {
      "epoch": 2.8786731566612582,
      "grad_norm": 3.591552495956421,
      "learning_rate": 1.6161971700636346e-05,
      "loss": 4.8059,
      "num_input_tokens_seen": 544531266,
      "step": 15968,
      "train_runtime": 34461.6974,
      "train_tokens_per_second": 15801.058
    },
    {
      "epoch": 2.884442040742744,
      "grad_norm": 3.586257219314575,
      "learning_rate": 1.6142127126896682e-05,
      "loss": 4.7458,
      "num_input_tokens_seen": 545600098,
      "step": 16000,
      "train_runtime": 34524.9893,
      "train_tokens_per_second": 15803.049
    },
    {
      "epoch": 2.8902109248242294,
      "grad_norm": 3.5001378059387207,
      "learning_rate": 1.6122243635593e-05,
      "loss": 4.787,
      "num_input_tokens_seen": 546643586,
      "step": 16032,
      "train_runtime": 34587.0287,
      "train_tokens_per_second": 15804.873
    },
    {
      "epoch": 2.895979808905715,
      "grad_norm": 3.1976823806762695,
      "learning_rate": 1.6102321352710493e-05,
      "loss": 4.7336,
      "num_input_tokens_seen": 547739202,
      "step": 16064,
      "train_runtime": 34651.2927,
      "train_tokens_per_second": 15807.179
    },
    {
      "epoch": 2.9017486929872005,
      "grad_norm": 3.561603307723999,
      "learning_rate": 1.608236040448014e-05,
      "loss": 4.8217,
      "num_input_tokens_seen": 548828994,
      "step": 16096,
      "train_runtime": 34714.7346,
      "train_tokens_per_second": 15809.684
    },
    {
      "epoch": 2.9075175770686856,
      "grad_norm": 3.3650875091552734,
      "learning_rate": 1.606236091737791e-05,
      "loss": 4.7656,
      "num_input_tokens_seen": 549926850,
      "step": 16128,
      "train_runtime": 34779.1351,
      "train_tokens_per_second": 15811.976
    },
    {
      "epoch": 2.913286461150171,
      "grad_norm": 3.5196545124053955,
      "learning_rate": 1.6042323018123957e-05,
      "loss": 4.7765,
      "num_input_tokens_seen": 551002690,
      "step": 16160,
      "train_runtime": 34841.7963,
      "train_tokens_per_second": 15814.417
    },
    {
      "epoch": 2.9190553452316568,
      "grad_norm": 3.1350550651550293,
      "learning_rate": 1.602224683368183e-05,
      "loss": 4.7799,
      "num_input_tokens_seen": 552108802,
      "step": 16192,
      "train_runtime": 34906.043,
      "train_tokens_per_second": 15816.998
    },
    {
      "epoch": 2.9248242293131423,
      "grad_norm": 3.6716806888580322,
      "learning_rate": 1.600213249125765e-05,
      "loss": 4.7489,
      "num_input_tokens_seen": 553173666,
      "step": 16224,
      "train_runtime": 34968.5441,
      "train_tokens_per_second": 15819.179
    },
    {
      "epoch": 2.930593113394628,
      "grad_norm": 3.217423915863037,
      "learning_rate": 1.5981980118299328e-05,
      "loss": 4.7681,
      "num_input_tokens_seen": 554253666,
      "step": 16256,
      "train_runtime": 35031.6805,
      "train_tokens_per_second": 15821.498
    },
    {
      "epoch": 2.936361997476113,
      "grad_norm": 3.070343494415283,
      "learning_rate": 1.5961789842495717e-05,
      "loss": 4.7202,
      "num_input_tokens_seen": 555320994,
      "step": 16288,
      "train_runtime": 35094.6915,
      "train_tokens_per_second": 15823.504
    },
    {
      "epoch": 2.9421308815575986,
      "grad_norm": 3.5749495029449463,
      "learning_rate": 1.594156179177585e-05,
      "loss": 4.7954,
      "num_input_tokens_seen": 556410914,
      "step": 16320,
      "train_runtime": 35158.9491,
      "train_tokens_per_second": 15825.584
    },
    {
      "epoch": 2.947899765639084,
      "grad_norm": 3.360161542892456,
      "learning_rate": 1.592129609430811e-05,
      "loss": 4.8089,
      "num_input_tokens_seen": 557516162,
      "step": 16352,
      "train_runtime": 35223.8099,
      "train_tokens_per_second": 15827.821
    },
    {
      "epoch": 2.9536686497205697,
      "grad_norm": 3.369171142578125,
      "learning_rate": 1.5900992878499405e-05,
      "loss": 4.7824,
      "num_input_tokens_seen": 558624578,
      "step": 16384,
      "train_runtime": 35288.5316,
      "train_tokens_per_second": 15830.202
    },
    {
      "epoch": 2.9594375338020553,
      "grad_norm": 2.9405815601348877,
      "learning_rate": 1.5880652272994366e-05,
      "loss": 4.7842,
      "num_input_tokens_seen": 559688322,
      "step": 16416,
      "train_runtime": 35351.1638,
      "train_tokens_per_second": 15832.246
    },
    {
      "epoch": 2.9652064178835404,
      "grad_norm": 3.242603063583374,
      "learning_rate": 1.586027440667454e-05,
      "loss": 4.7503,
      "num_input_tokens_seen": 560736002,
      "step": 16448,
      "train_runtime": 35412.6768,
      "train_tokens_per_second": 15834.33
    },
    {
      "epoch": 2.970975301965026,
      "grad_norm": 3.4766647815704346,
      "learning_rate": 1.583985940865756e-05,
      "loss": 4.7581,
      "num_input_tokens_seen": 561861634,
      "step": 16480,
      "train_runtime": 35477.9505,
      "train_tokens_per_second": 15836.925
    },
    {
      "epoch": 2.9767441860465116,
      "grad_norm": 3.2937026023864746,
      "learning_rate": 1.581940740829633e-05,
      "loss": 4.834,
      "num_input_tokens_seen": 562968802,
      "step": 16512,
      "train_runtime": 35542.2948,
      "train_tokens_per_second": 15839.405
    },
    {
      "epoch": 2.982513070127997,
      "grad_norm": 3.40773868560791,
      "learning_rate": 1.579891853517821e-05,
      "loss": 4.7353,
      "num_input_tokens_seen": 564066658,
      "step": 16544,
      "train_runtime": 35606.2608,
      "train_tokens_per_second": 15841.783
    },
    {
      "epoch": 2.9882819542094827,
      "grad_norm": 3.4626107215881348,
      "learning_rate": 1.577839291912419e-05,
      "loss": 4.7638,
      "num_input_tokens_seen": 565142722,
      "step": 16576,
      "train_runtime": 35669.1337,
      "train_tokens_per_second": 15844.027
    },
    {
      "epoch": 2.994050838290968,
      "grad_norm": 3.118180513381958,
      "learning_rate": 1.575783069018807e-05,
      "loss": 4.8265,
      "num_input_tokens_seen": 566264066,
      "step": 16608,
      "train_runtime": 35734.1104,
      "train_tokens_per_second": 15846.598
    },
    {
      "epoch": 2.9998197223724534,
      "grad_norm": 2.995995283126831,
      "learning_rate": 1.5737231978655637e-05,
      "loss": 4.7582,
      "num_input_tokens_seen": 567358370,
      "step": 16640,
      "train_runtime": 35797.8597,
      "train_tokens_per_second": 15848.947
    },
    {
      "epoch": 3.0,
      "eval_accuracy": 0.018046800539783366,
      "eval_class_accuracy": 0.472241191188317,
      "eval_exact_token_accuracy": 0.0,
      "eval_f1": 0.03541312110180683,
      "eval_loss": 2.697479486465454,
      "eval_mean_iou": 0.1527707128652486,
      "eval_mean_iou_25": 0.1527707128652486,
      "eval_precision": 0.03697971900918995,
      "eval_prediction_target_similarity": 7.736400026382984e-05,
      "eval_recall": 0.03397694754730255,
      "eval_recall_25": 0.08543555072078392,
      "eval_runtime": 1274.8518,
      "eval_samples_per_second": 3.884,
      "eval_sequence_accuracy": 0.0,
      "eval_steps_per_second": 0.122,
      "eval_top3_token_accuracy": 0.06044773757457733,
      "eval_top5_token_accuracy": 0.11378675699234009,
      "num_input_tokens_seen": 567392122,
      "step": 16641
    },
    {
      "epoch": 3.005588606453939,
      "grad_norm": 3.7403409481048584,
      "learning_rate": 1.571659691504384e-05,
      "loss": 4.7538,
      "num_input_tokens_seen": 568404602,
      "step": 16672,
      "train_runtime": 37136.1106,
      "train_tokens_per_second": 15305.981
    },
    {
      "epoch": 3.0113574905354246,
      "grad_norm": 3.277402639389038,
      "learning_rate": 1.5695925630099954e-05,
      "loss": 4.7132,
      "num_input_tokens_seen": 569509594,
      "step": 16704,
      "train_runtime": 37200.4353,
      "train_tokens_per_second": 15309.219
    },
    {
      "epoch": 3.01712637461691,
      "grad_norm": 3.5067012310028076,
      "learning_rate": 1.5675218254800763e-05,
      "loss": 4.7368,
      "num_input_tokens_seen": 570583994,
      "step": 16736,
      "train_runtime": 37263.2712,
      "train_tokens_per_second": 15312.236
    },
    {
      "epoch": 3.0228952586983957,
      "grad_norm": 3.2583329677581787,
      "learning_rate": 1.5654474920351726e-05,
      "loss": 4.7351,
      "num_input_tokens_seen": 571680954,
      "step": 16768,
      "train_runtime": 37327.2192,
      "train_tokens_per_second": 15315.391
    },
    {
      "epoch": 3.028664142779881,
      "grad_norm": 3.483611583709717,
      "learning_rate": 1.5633695758186146e-05,
      "loss": 4.7943,
      "num_input_tokens_seen": 572854522,
      "step": 16800,
      "train_runtime": 37394.8205,
      "train_tokens_per_second": 15319.087
    },
    {
      "epoch": 3.0344330268613664,
      "grad_norm": 3.1267383098602295,
      "learning_rate": 1.5612880899964334e-05,
      "loss": 4.7719,
      "num_input_tokens_seen": 573941370,
      "step": 16832,
      "train_runtime": 37458.2938,
      "train_tokens_per_second": 15322.144
    },
    {
      "epoch": 3.040201910942852,
      "grad_norm": 3.4471991062164307,
      "learning_rate": 1.5592030477572776e-05,
      "loss": 4.7514,
      "num_input_tokens_seen": 575011354,
      "step": 16864,
      "train_runtime": 37520.948,
      "train_tokens_per_second": 15325.075
    },
    {
      "epoch": 3.0459707950243375,
      "grad_norm": 3.632723569869995,
      "learning_rate": 1.5571144623123302e-05,
      "loss": 4.7711,
      "num_input_tokens_seen": 576101658,
      "step": 16896,
      "train_runtime": 37584.6025,
      "train_tokens_per_second": 15328.13
    },
    {
      "epoch": 3.051739679105823,
      "grad_norm": 3.470853328704834,
      "learning_rate": 1.5550223468952248e-05,
      "loss": 4.7025,
      "num_input_tokens_seen": 577155738,
      "step": 16928,
      "train_runtime": 37646.3249,
      "train_tokens_per_second": 15330.998
    },
    {
      "epoch": 3.0575085631873082,
      "grad_norm": 4.48854923248291,
      "learning_rate": 1.5529267147619608e-05,
      "loss": 4.7984,
      "num_input_tokens_seen": 578260122,
      "step": 16960,
      "train_runtime": 37710.5513,
      "train_tokens_per_second": 15334.173
    },
    {
      "epoch": 3.063277447268794,
      "grad_norm": 3.301002264022827,
      "learning_rate": 1.5508275791908202e-05,
      "loss": 4.7617,
      "num_input_tokens_seen": 579376506,
      "step": 16992,
      "train_runtime": 37775.3082,
      "train_tokens_per_second": 15337.44
    },
    {
      "epoch": 3.0690463313502794,
      "grad_norm": 3.2440547943115234,
      "learning_rate": 1.5487249534822844e-05,
      "loss": 4.722,
      "num_input_tokens_seen": 580432314,
      "step": 17024,
      "train_runtime": 37837.2101,
      "train_tokens_per_second": 15340.251
    },
    {
      "epoch": 3.074815215431765,
      "grad_norm": 3.377685308456421,
      "learning_rate": 1.5466188509589476e-05,
      "loss": 4.74,
      "num_input_tokens_seen": 581557210,
      "step": 17056,
      "train_runtime": 37902.4918,
      "train_tokens_per_second": 15343.509
    },
    {
      "epoch": 3.0805840995132505,
      "grad_norm": 3.1555063724517822,
      "learning_rate": 1.5445092849654342e-05,
      "loss": 4.7688,
      "num_input_tokens_seen": 582638490,
      "step": 17088,
      "train_runtime": 37965.8109,
      "train_tokens_per_second": 15346.399
    },
    {
      "epoch": 3.086352983594736,
      "grad_norm": 3.4058151245117188,
      "learning_rate": 1.542396268868314e-05,
      "loss": 4.8044,
      "num_input_tokens_seen": 583735738,
      "step": 17120,
      "train_runtime": 38029.8176,
      "train_tokens_per_second": 15349.422
    },
    {
      "epoch": 3.092121867676221,
      "grad_norm": 3.4060826301574707,
      "learning_rate": 1.540279816056017e-05,
      "loss": 4.7501,
      "num_input_tokens_seen": 584824698,
      "step": 17152,
      "train_runtime": 38093.3009,
      "train_tokens_per_second": 15352.429
    },
    {
      "epoch": 3.097890751757707,
      "grad_norm": 3.470864772796631,
      "learning_rate": 1.538159939938749e-05,
      "loss": 4.7373,
      "num_input_tokens_seen": 585930810,
      "step": 17184,
      "train_runtime": 38157.787,
      "train_tokens_per_second": 15355.471
    },
    {
      "epoch": 3.1036596358391924,
      "grad_norm": 3.473496675491333,
      "learning_rate": 1.5360366539484056e-05,
      "loss": 4.7806,
      "num_input_tokens_seen": 587037562,
      "step": 17216,
      "train_runtime": 38222.0938,
      "train_tokens_per_second": 15358.592
    },
    {
      "epoch": 3.109428519920678,
      "grad_norm": 3.751267910003662,
      "learning_rate": 1.53390997153849e-05,
      "loss": 4.8096,
      "num_input_tokens_seen": 588155770,
      "step": 17248,
      "train_runtime": 38286.9522,
      "train_tokens_per_second": 15361.781
    },
    {
      "epoch": 3.1151974040021635,
      "grad_norm": 3.886150598526001,
      "learning_rate": 1.5317799061840224e-05,
      "loss": 4.8093,
      "num_input_tokens_seen": 589239290,
      "step": 17280,
      "train_runtime": 38350.404,
      "train_tokens_per_second": 15364.618
    },
    {
      "epoch": 3.1209662880836486,
      "grad_norm": 3.137505531311035,
      "learning_rate": 1.529646471381461e-05,
      "loss": 4.7609,
      "num_input_tokens_seen": 590306426,
      "step": 17312,
      "train_runtime": 38412.7305,
      "train_tokens_per_second": 15367.468
    },
    {
      "epoch": 3.126735172165134,
      "grad_norm": 3.423372507095337,
      "learning_rate": 1.5275096806486125e-05,
      "loss": 4.8087,
      "num_input_tokens_seen": 591450714,
      "step": 17344,
      "train_runtime": 38478.9199,
      "train_tokens_per_second": 15370.772
    },
    {
      "epoch": 3.1325040562466198,
      "grad_norm": 3.4950063228607178,
      "learning_rate": 1.5253695475245462e-05,
      "loss": 4.7363,
      "num_input_tokens_seen": 592586458,
      "step": 17376,
      "train_runtime": 38544.7628,
      "train_tokens_per_second": 15373.981
    },
    {
      "epoch": 3.1382729403281053,
      "grad_norm": 3.4442033767700195,
      "learning_rate": 1.523226085569511e-05,
      "loss": 4.7366,
      "num_input_tokens_seen": 593653946,
      "step": 17408,
      "train_runtime": 38607.3468,
      "train_tokens_per_second": 15376.709
    },
    {
      "epoch": 3.144041824409591,
      "grad_norm": 3.460569381713867,
      "learning_rate": 1.5210793083648465e-05,
      "loss": 4.7932,
      "num_input_tokens_seen": 594765850,
      "step": 17440,
      "train_runtime": 38671.9108,
      "train_tokens_per_second": 15379.79
    },
    {
      "epoch": 3.149810708491076,
      "grad_norm": 3.145738124847412,
      "learning_rate": 1.518929229512899e-05,
      "loss": 4.7361,
      "num_input_tokens_seen": 595841050,
      "step": 17472,
      "train_runtime": 38734.474,
      "train_tokens_per_second": 15382.707
    },
    {
      "epoch": 3.1555795925725616,
      "grad_norm": 3.449388265609741,
      "learning_rate": 1.5167758626369347e-05,
      "loss": 4.7634,
      "num_input_tokens_seen": 596918170,
      "step": 17504,
      "train_runtime": 38797.2756,
      "train_tokens_per_second": 15385.569
    },
    {
      "epoch": 3.161348476654047,
      "grad_norm": 3.8019509315490723,
      "learning_rate": 1.5146192213810524e-05,
      "loss": 4.758,
      "num_input_tokens_seen": 598038106,
      "step": 17536,
      "train_runtime": 38862.2741,
      "train_tokens_per_second": 15388.654
    },
    {
      "epoch": 3.1671173607355327,
      "grad_norm": 3.232571840286255,
      "learning_rate": 1.5124593194100996e-05,
      "loss": 4.7338,
      "num_input_tokens_seen": 599131290,
      "step": 17568,
      "train_runtime": 38925.9765,
      "train_tokens_per_second": 15391.555
    },
    {
      "epoch": 3.1728862448170183,
      "grad_norm": 3.4061379432678223,
      "learning_rate": 1.5102961704095822e-05,
      "loss": 4.7156,
      "num_input_tokens_seen": 600228154,
      "step": 17600,
      "train_runtime": 38989.8075,
      "train_tokens_per_second": 15394.489
    },
    {
      "epoch": 3.178655128898504,
      "grad_norm": 3.2243521213531494,
      "learning_rate": 1.5081297880855814e-05,
      "loss": 4.7885,
      "num_input_tokens_seen": 601304314,
      "step": 17632,
      "train_runtime": 39052.6016,
      "train_tokens_per_second": 15397.292
    },
    {
      "epoch": 3.184424012979989,
      "grad_norm": 4.911252498626709,
      "learning_rate": 1.5059601861646644e-05,
      "loss": 4.7668,
      "num_input_tokens_seen": 602388378,
      "step": 17664,
      "train_runtime": 39115.8274,
      "train_tokens_per_second": 15400.119
    },
    {
      "epoch": 3.1901928970614746,
      "grad_norm": 3.476929187774658,
      "learning_rate": 1.5037873783937992e-05,
      "loss": 4.6732,
      "num_input_tokens_seen": 603442330,
      "step": 17696,
      "train_runtime": 39177.7374,
      "train_tokens_per_second": 15402.685
    },
    {
      "epoch": 3.19596178114296,
      "grad_norm": 3.607464075088501,
      "learning_rate": 1.5016113785402653e-05,
      "loss": 4.7754,
      "num_input_tokens_seen": 604560282,
      "step": 17728,
      "train_runtime": 39242.5226,
      "train_tokens_per_second": 15405.745
    },
    {
      "epoch": 3.2017306652244457,
      "grad_norm": 3.5850112438201904,
      "learning_rate": 1.499432200391569e-05,
      "loss": 4.7575,
      "num_input_tokens_seen": 605620442,
      "step": 17760,
      "train_runtime": 39304.6139,
      "train_tokens_per_second": 15408.38
    },
    {
      "epoch": 3.2074995493059313,
      "grad_norm": 3.8579702377319336,
      "learning_rate": 1.497249857755354e-05,
      "loss": 4.7536,
      "num_input_tokens_seen": 606682874,
      "step": 17792,
      "train_runtime": 39366.8769,
      "train_tokens_per_second": 15410.998
    },
    {
      "epoch": 3.213268433387417,
      "grad_norm": 3.2536227703094482,
      "learning_rate": 1.4950643644593157e-05,
      "loss": 4.7228,
      "num_input_tokens_seen": 607746650,
      "step": 17824,
      "train_runtime": 39429.1523,
      "train_tokens_per_second": 15413.637
    },
    {
      "epoch": 3.219037317468902,
      "grad_norm": 3.7685279846191406,
      "learning_rate": 1.4928757343511118e-05,
      "loss": 4.7589,
      "num_input_tokens_seen": 608802970,
      "step": 17856,
      "train_runtime": 39491.0408,
      "train_tokens_per_second": 15416.23
    },
    {
      "epoch": 3.2248062015503876,
      "grad_norm": 3.445345401763916,
      "learning_rate": 1.4906839812982747e-05,
      "loss": 4.7976,
      "num_input_tokens_seen": 609893978,
      "step": 17888,
      "train_runtime": 39554.6902,
      "train_tokens_per_second": 15419.005
    },
    {
      "epoch": 3.230575085631873,
      "grad_norm": 3.338313341140747,
      "learning_rate": 1.4884891191881266e-05,
      "loss": 4.6903,
      "num_input_tokens_seen": 610928346,
      "step": 17920,
      "train_runtime": 39615.7319,
      "train_tokens_per_second": 15421.357
    },
    {
      "epoch": 3.2363439697133587,
      "grad_norm": 3.244184732437134,
      "learning_rate": 1.486291161927687e-05,
      "loss": 4.7404,
      "num_input_tokens_seen": 611998874,
      "step": 17952,
      "train_runtime": 39678.4337,
      "train_tokens_per_second": 15423.968
    },
    {
      "epoch": 3.2421128537948443,
      "grad_norm": 3.3460941314697266,
      "learning_rate": 1.4840901234435872e-05,
      "loss": 4.7266,
      "num_input_tokens_seen": 613077370,
      "step": 17984,
      "train_runtime": 39741.6377,
      "train_tokens_per_second": 15426.575
    },
    {
      "epoch": 3.2478817378763294,
      "grad_norm": 3.710031747817993,
      "learning_rate": 1.4818860176819832e-05,
      "loss": 4.8025,
      "num_input_tokens_seen": 614173338,
      "step": 18016,
      "train_runtime": 39805.5607,
      "train_tokens_per_second": 15429.335
    },
    {
      "epoch": 3.253650621957815,
      "grad_norm": 3.2450993061065674,
      "learning_rate": 1.4796788586084638e-05,
      "loss": 4.7073,
      "num_input_tokens_seen": 615245498,
      "step": 18048,
      "train_runtime": 39868.3989,
      "train_tokens_per_second": 15431.909
    },
    {
      "epoch": 3.2594195060393005,
      "grad_norm": 3.2905149459838867,
      "learning_rate": 1.4774686602079653e-05,
      "loss": 4.7857,
      "num_input_tokens_seen": 616382682,
      "step": 18080,
      "train_runtime": 39934.3548,
      "train_tokens_per_second": 15434.898
    },
    {
      "epoch": 3.265188390120786,
      "grad_norm": 3.4262547492980957,
      "learning_rate": 1.4752554364846812e-05,
      "loss": 4.7622,
      "num_input_tokens_seen": 617426618,
      "step": 18112,
      "train_runtime": 39995.8371,
      "train_tokens_per_second": 15437.272
    },
    {
      "epoch": 3.2709572742022717,
      "grad_norm": 3.630246639251709,
      "learning_rate": 1.4730392014619743e-05,
      "loss": 4.727,
      "num_input_tokens_seen": 618508986,
      "step": 18144,
      "train_runtime": 40059.04,
      "train_tokens_per_second": 15439.935
    },
    {
      "epoch": 3.276726158283757,
      "grad_norm": 3.499096632003784,
      "learning_rate": 1.4708199691822869e-05,
      "loss": 4.8557,
      "num_input_tokens_seen": 619674586,
      "step": 18176,
      "train_runtime": 40126.2363,
      "train_tokens_per_second": 15443.128
    },
    {
      "epoch": 3.2824950423652424,
      "grad_norm": 3.3540639877319336,
      "learning_rate": 1.4685977537070532e-05,
      "loss": 4.7989,
      "num_input_tokens_seen": 620781850,
      "step": 18208,
      "train_runtime": 40190.6984,
      "train_tokens_per_second": 15445.909
    },
    {
      "epoch": 3.288263926446728,
      "grad_norm": 3.2436256408691406,
      "learning_rate": 1.4663725691166093e-05,
      "loss": 4.8262,
      "num_input_tokens_seen": 621917498,
      "step": 18240,
      "train_runtime": 40256.4654,
      "train_tokens_per_second": 15448.885
    },
    {
      "epoch": 3.2940328105282135,
      "grad_norm": 3.7965433597564697,
      "learning_rate": 1.4641444295101034e-05,
      "loss": 4.7251,
      "num_input_tokens_seen": 622996442,
      "step": 18272,
      "train_runtime": 40319.2689,
      "train_tokens_per_second": 15451.581
    },
    {
      "epoch": 3.299801694609699,
      "grad_norm": 3.7866287231445312,
      "learning_rate": 1.4619133490054082e-05,
      "loss": 4.8012,
      "num_input_tokens_seen": 624115162,
      "step": 18304,
      "train_runtime": 40384.04,
      "train_tokens_per_second": 15454.5
    },
    {
      "epoch": 3.305570578691184,
      "grad_norm": 3.419278144836426,
      "learning_rate": 1.4596793417390294e-05,
      "loss": 4.8033,
      "num_input_tokens_seen": 625228442,
      "step": 18336,
      "train_runtime": 40448.5815,
      "train_tokens_per_second": 15457.364
    },
    {
      "epoch": 3.3113394627726698,
      "grad_norm": 3.0671377182006836,
      "learning_rate": 1.4574424218660181e-05,
      "loss": 4.7761,
      "num_input_tokens_seen": 626338010,
      "step": 18368,
      "train_runtime": 40513.0164,
      "train_tokens_per_second": 15460.167
    },
    {
      "epoch": 3.3171083468541553,
      "grad_norm": 3.851508617401123,
      "learning_rate": 1.4552026035598796e-05,
      "loss": 4.7718,
      "num_input_tokens_seen": 627429242,
      "step": 18400,
      "train_runtime": 40576.6345,
      "train_tokens_per_second": 15462.821
    },
    {
      "epoch": 3.322877230935641,
      "grad_norm": 3.398439407348633,
      "learning_rate": 1.4529599010124842e-05,
      "loss": 4.716,
      "num_input_tokens_seen": 628543322,
      "step": 18432,
      "train_runtime": 40641.3478,
      "train_tokens_per_second": 15465.612
    },
    {
      "epoch": 3.3286461150171265,
      "grad_norm": 3.38496994972229,
      "learning_rate": 1.4507143284339778e-05,
      "loss": 4.7323,
      "num_input_tokens_seen": 629630010,
      "step": 18464,
      "train_runtime": 40704.7633,
      "train_tokens_per_second": 15468.214
    },
    {
      "epoch": 3.334414999098612,
      "grad_norm": 3.2054443359375,
      "learning_rate": 1.4484659000526902e-05,
      "loss": 4.7458,
      "num_input_tokens_seen": 630737850,
      "step": 18496,
      "train_runtime": 40769.314,
      "train_tokens_per_second": 15470.897
    },
    {
      "epoch": 3.340183883180097,
      "grad_norm": 4.252221584320068,
      "learning_rate": 1.4462146301150473e-05,
      "loss": 4.771,
      "num_input_tokens_seen": 631832506,
      "step": 18528,
      "train_runtime": 40833.151,
      "train_tokens_per_second": 15473.518
    },
    {
      "epoch": 3.3459527672615827,
      "grad_norm": 3.217299699783325,
      "learning_rate": 1.4439605328854787e-05,
      "loss": 4.741,
      "num_input_tokens_seen": 632937050,
      "step": 18560,
      "train_runtime": 40897.297,
      "train_tokens_per_second": 15476.256
    },
    {
      "epoch": 3.3517216513430683,
      "grad_norm": 3.1341962814331055,
      "learning_rate": 1.4417036226463287e-05,
      "loss": 4.7796,
      "num_input_tokens_seen": 634051962,
      "step": 18592,
      "train_runtime": 40961.993,
      "train_tokens_per_second": 15479.031
    },
    {
      "epoch": 3.357490535424554,
      "grad_norm": 3.876305341720581,
      "learning_rate": 1.439443913697765e-05,
      "loss": 4.7959,
      "num_input_tokens_seen": 635121914,
      "step": 18624,
      "train_runtime": 41024.6716,
      "train_tokens_per_second": 15481.462
    },
    {
      "epoch": 3.3632594195060395,
      "grad_norm": 3.2285869121551514,
      "learning_rate": 1.4371814203576886e-05,
      "loss": 4.731,
      "num_input_tokens_seen": 636211290,
      "step": 18656,
      "train_runtime": 41088.1853,
      "train_tokens_per_second": 15484.045
    },
    {
      "epoch": 3.369028303587525,
      "grad_norm": 3.2153635025024414,
      "learning_rate": 1.4349161569616428e-05,
      "loss": 4.6868,
      "num_input_tokens_seen": 637234362,
      "step": 18688,
      "train_runtime": 41148.8983,
      "train_tokens_per_second": 15486.061
    },
    {
      "epoch": 3.37479718766901,
      "grad_norm": 3.994328737258911,
      "learning_rate": 1.4326481378627226e-05,
      "loss": 4.74,
      "num_input_tokens_seen": 638358842,
      "step": 18720,
      "train_runtime": 41214.8087,
      "train_tokens_per_second": 15488.58
    },
    {
      "epoch": 3.3805660717504957,
      "grad_norm": 3.7810323238372803,
      "learning_rate": 1.4303773774314828e-05,
      "loss": 4.7562,
      "num_input_tokens_seen": 639450746,
      "step": 18752,
      "train_runtime": 41279.1736,
      "train_tokens_per_second": 15490.881
    },
    {
      "epoch": 3.3863349558319813,
      "grad_norm": 2.978926658630371,
      "learning_rate": 1.4281038900558492e-05,
      "loss": 4.6659,
      "num_input_tokens_seen": 640540954,
      "step": 18784,
      "train_runtime": 41343.4554,
      "train_tokens_per_second": 15493.164
    },
    {
      "epoch": 3.392103839913467,
      "grad_norm": 3.4041898250579834,
      "learning_rate": 1.4258276901410248e-05,
      "loss": 4.7417,
      "num_input_tokens_seen": 641615770,
      "step": 18816,
      "train_runtime": 41406.9826,
      "train_tokens_per_second": 15495.352
    },
    {
      "epoch": 3.3978727239949524,
      "grad_norm": 3.8583321571350098,
      "learning_rate": 1.4235487921094004e-05,
      "loss": 4.7215,
      "num_input_tokens_seen": 642717946,
      "step": 18848,
      "train_runtime": 41471.8159,
      "train_tokens_per_second": 15497.704
    },
    {
      "epoch": 3.4036416080764376,
      "grad_norm": 3.227367877960205,
      "learning_rate": 1.4212672104004615e-05,
      "loss": 4.7644,
      "num_input_tokens_seen": 643809242,
      "step": 18880,
      "train_runtime": 41535.8803,
      "train_tokens_per_second": 15500.075
    },
    {
      "epoch": 3.409410492157923,
      "grad_norm": 3.3645575046539307,
      "learning_rate": 1.4189829594706994e-05,
      "loss": 4.7564,
      "num_input_tokens_seen": 644921594,
      "step": 18912,
      "train_runtime": 41600.5057,
      "train_tokens_per_second": 15502.734
    },
    {
      "epoch": 3.4151793762394087,
      "grad_norm": 3.3789901733398438,
      "learning_rate": 1.4166960537935169e-05,
      "loss": 4.7409,
      "num_input_tokens_seen": 646038330,
      "step": 18944,
      "train_runtime": 41665.3172,
      "train_tokens_per_second": 15505.422
    },
    {
      "epoch": 3.4209482603208943,
      "grad_norm": 3.074280261993408,
      "learning_rate": 1.4144065078591376e-05,
      "loss": 4.727,
      "num_input_tokens_seen": 647185338,
      "step": 18976,
      "train_runtime": 41731.9798,
      "train_tokens_per_second": 15508.139
    },
    {
      "epoch": 3.42671714440238,
      "grad_norm": 3.3728582859039307,
      "learning_rate": 1.4121143361745151e-05,
      "loss": 4.7103,
      "num_input_tokens_seen": 648262170,
      "step": 19008,
      "train_runtime": 41795.3571,
      "train_tokens_per_second": 15510.387
    },
    {
      "epoch": 3.432486028483865,
      "grad_norm": 4.251126766204834,
      "learning_rate": 1.40981955326324e-05,
      "loss": 4.8027,
      "num_input_tokens_seen": 649383546,
      "step": 19040,
      "train_runtime": 41860.7949,
      "train_tokens_per_second": 15512.929
    },
    {
      "epoch": 3.4382549125653505,
      "grad_norm": 3.034980297088623,
      "learning_rate": 1.4075221736654472e-05,
      "loss": 4.7888,
      "num_input_tokens_seen": 650467642,
      "step": 19072,
      "train_runtime": 41924.7657,
      "train_tokens_per_second": 15515.117
    },
    {
      "epoch": 3.444023796646836,
      "grad_norm": 3.3121228218078613,
      "learning_rate": 1.4052222119377255e-05,
      "loss": 4.7058,
      "num_input_tokens_seen": 651553626,
      "step": 19104,
      "train_runtime": 41988.8799,
      "train_tokens_per_second": 15517.29
    },
    {
      "epoch": 3.4497926807283217,
      "grad_norm": 3.700340747833252,
      "learning_rate": 1.4029196826530247e-05,
      "loss": 4.7472,
      "num_input_tokens_seen": 652637466,
      "step": 19136,
      "train_runtime": 42052.7061,
      "train_tokens_per_second": 15519.512
    },
    {
      "epoch": 3.4555615648098073,
      "grad_norm": 3.3692145347595215,
      "learning_rate": 1.400614600400562e-05,
      "loss": 4.7342,
      "num_input_tokens_seen": 653687354,
      "step": 19168,
      "train_runtime": 42115.082,
      "train_tokens_per_second": 15521.455
    },
    {
      "epoch": 3.4613304488912924,
      "grad_norm": 3.392083168029785,
      "learning_rate": 1.3983069797857316e-05,
      "loss": 4.7995,
      "num_input_tokens_seen": 654749114,
      "step": 19200,
      "train_runtime": 42178.0325,
      "train_tokens_per_second": 15523.463
    },
    {
      "epoch": 3.467099332972778,
      "grad_norm": 3.152796745300293,
      "learning_rate": 1.3959968354300113e-05,
      "loss": 4.8005,
      "num_input_tokens_seen": 655859994,
      "step": 19232,
      "train_runtime": 42243.2766,
      "train_tokens_per_second": 15525.784
    },
    {
      "epoch": 3.4728682170542635,
      "grad_norm": 3.1386938095092773,
      "learning_rate": 1.3936841819708687e-05,
      "loss": 4.7543,
      "num_input_tokens_seen": 656939418,
      "step": 19264,
      "train_runtime": 42306.9246,
      "train_tokens_per_second": 15527.941
    },
    {
      "epoch": 3.478637101135749,
      "grad_norm": 3.4154722690582275,
      "learning_rate": 1.3913690340616706e-05,
      "loss": 4.739,
      "num_input_tokens_seen": 658026618,
      "step": 19296,
      "train_runtime": 42370.7409,
      "train_tokens_per_second": 15530.213
    },
    {
      "epoch": 3.4844059852172347,
      "grad_norm": 3.7197227478027344,
      "learning_rate": 1.3890514063715884e-05,
      "loss": 4.7378,
      "num_input_tokens_seen": 659094298,
      "step": 19328,
      "train_runtime": 42433.8009,
      "train_tokens_per_second": 15532.295
    },
    {
      "epoch": 3.49017486929872,
      "grad_norm": 4.1830573081970215,
      "learning_rate": 1.3867313135855061e-05,
      "loss": 4.7859,
      "num_input_tokens_seen": 660182714,
      "step": 19360,
      "train_runtime": 42497.8116,
      "train_tokens_per_second": 15534.511
    },
    {
      "epoch": 3.4959437533802054,
      "grad_norm": 3.8468711376190186,
      "learning_rate": 1.384408770403927e-05,
      "loss": 4.7509,
      "num_input_tokens_seen": 661291994,
      "step": 19392,
      "train_runtime": 42562.8846,
      "train_tokens_per_second": 15536.823
    },
    {
      "epoch": 3.501712637461691,
      "grad_norm": 3.255506753921509,
      "learning_rate": 1.3820837915428802e-05,
      "loss": 4.6861,
      "num_input_tokens_seen": 662380826,
      "step": 19424,
      "train_runtime": 42627.1915,
      "train_tokens_per_second": 15538.927
    },
    {
      "epoch": 3.5074815215431765,
      "grad_norm": 3.234448194503784,
      "learning_rate": 1.3797563917338283e-05,
      "loss": 4.7337,
      "num_input_tokens_seen": 663479642,
      "step": 19456,
      "train_runtime": 42691.6624,
      "train_tokens_per_second": 15541.199
    },
    {
      "epoch": 3.513250405624662,
      "grad_norm": 3.5795023441314697,
      "learning_rate": 1.3774265857235727e-05,
      "loss": 4.7324,
      "num_input_tokens_seen": 664536730,
      "step": 19488,
      "train_runtime": 42754.1922,
      "train_tokens_per_second": 15543.195
    },
    {
      "epoch": 3.519019289706147,
      "grad_norm": 3.3121256828308105,
      "learning_rate": 1.3750943882741613e-05,
      "loss": 4.7329,
      "num_input_tokens_seen": 665649434,
      "step": 19520,
      "train_runtime": 42819.3029,
      "train_tokens_per_second": 15545.546
    },
    {
      "epoch": 3.524788173787633,
      "grad_norm": 3.6050853729248047,
      "learning_rate": 1.3727598141627947e-05,
      "loss": 4.7066,
      "num_input_tokens_seen": 666723578,
      "step": 19552,
      "train_runtime": 42882.5442,
      "train_tokens_per_second": 15547.668
    },
    {
      "epoch": 3.5305570578691183,
      "grad_norm": 3.569984197616577,
      "learning_rate": 1.370422878181732e-05,
      "loss": 4.7536,
      "num_input_tokens_seen": 667810970,
      "step": 19584,
      "train_runtime": 42946.5298,
      "train_tokens_per_second": 15549.824
    },
    {
      "epoch": 3.536325941950604,
      "grad_norm": 3.6731960773468018,
      "learning_rate": 1.368083595138198e-05,
      "loss": 4.7499,
      "num_input_tokens_seen": 668906266,
      "step": 19616,
      "train_runtime": 43010.9763,
      "train_tokens_per_second": 15551.99
    },
    {
      "epoch": 3.5420948260320895,
      "grad_norm": 3.6239089965820312,
      "learning_rate": 1.3657419798542885e-05,
      "loss": 4.686,
      "num_input_tokens_seen": 669999194,
      "step": 19648,
      "train_runtime": 43075.2578,
      "train_tokens_per_second": 15554.154
    },
    {
      "epoch": 3.547863710113575,
      "grad_norm": 3.200676441192627,
      "learning_rate": 1.363398047166877e-05,
      "loss": 4.7402,
      "num_input_tokens_seen": 671114010,
      "step": 19680,
      "train_runtime": 43140.4591,
      "train_tokens_per_second": 15556.487
    },
    {
      "epoch": 3.5536325941950606,
      "grad_norm": 3.17935848236084,
      "learning_rate": 1.3610518119275202e-05,
      "loss": 4.7542,
      "num_input_tokens_seen": 672192826,
      "step": 19712,
      "train_runtime": 43203.8788,
      "train_tokens_per_second": 15558.622
    },
    {
      "epoch": 3.5594014782765457,
      "grad_norm": 3.105971336364746,
      "learning_rate": 1.3587032890023645e-05,
      "loss": 4.705,
      "num_input_tokens_seen": 673283386,
      "step": 19744,
      "train_runtime": 43267.4756,
      "train_tokens_per_second": 15560.958
    },
    {
      "epoch": 3.5651703623580313,
      "grad_norm": 3.3590292930603027,
      "learning_rate": 1.3563524932720515e-05,
      "loss": 4.7516,
      "num_input_tokens_seen": 674354138,
      "step": 19776,
      "train_runtime": 43330.5833,
      "train_tokens_per_second": 15563.006
    },
    {
      "epoch": 3.570939246439517,
      "grad_norm": 3.5388383865356445,
      "learning_rate": 1.3539994396316233e-05,
      "loss": 4.811,
      "num_input_tokens_seen": 675498106,
      "step": 19808,
      "train_runtime": 43396.9706,
      "train_tokens_per_second": 15565.559
    },
    {
      "epoch": 3.5767081305210024,
      "grad_norm": 3.0247251987457275,
      "learning_rate": 1.3516441429904285e-05,
      "loss": 4.787,
      "num_input_tokens_seen": 676597050,
      "step": 19840,
      "train_runtime": 43461.4764,
      "train_tokens_per_second": 15567.742
    },
    {
      "epoch": 3.582477014602488,
      "grad_norm": 3.8974900245666504,
      "learning_rate": 1.3492866182720282e-05,
      "loss": 4.7709,
      "num_input_tokens_seen": 677689402,
      "step": 19872,
      "train_runtime": 43525.6782,
      "train_tokens_per_second": 15569.876
    },
    {
      "epoch": 3.588245898683973,
      "grad_norm": 3.4112708568573,
      "learning_rate": 1.3469268804141005e-05,
      "loss": 4.7493,
      "num_input_tokens_seen": 678754202,
      "step": 19904,
      "train_runtime": 43588.6005,
      "train_tokens_per_second": 15571.828
    },
    {
      "epoch": 3.5940147827654587,
      "grad_norm": 3.2571825981140137,
      "learning_rate": 1.3445649443683469e-05,
      "loss": 4.6815,
      "num_input_tokens_seen": 679785850,
      "step": 19936,
      "train_runtime": 43649.9064,
      "train_tokens_per_second": 15573.592
    },
    {
      "epoch": 3.5997836668469443,
      "grad_norm": 3.4568779468536377,
      "learning_rate": 1.3422008251003962e-05,
      "loss": 4.7854,
      "num_input_tokens_seen": 680877850,
      "step": 19968,
      "train_runtime": 43714.0043,
      "train_tokens_per_second": 15575.737
    },
    {
      "epoch": 3.60555255092843,
      "grad_norm": 3.3795926570892334,
      "learning_rate": 1.3398345375897111e-05,
      "loss": 4.7971,
      "num_input_tokens_seen": 681979962,
      "step": 20000,
      "train_runtime": 43778.6944,
      "train_tokens_per_second": 15577.896
    },
    {
      "epoch": 3.6113214350099154,
      "grad_norm": 3.834583044052124,
      "learning_rate": 1.3374660968294926e-05,
      "loss": 4.7857,
      "num_input_tokens_seen": 683103002,
      "step": 20032,
      "train_runtime": 43844.2865,
      "train_tokens_per_second": 15580.206
    },
    {
      "epoch": 3.6170903190914006,
      "grad_norm": 3.469505548477173,
      "learning_rate": 1.3350955178265843e-05,
      "loss": 4.7642,
      "num_input_tokens_seen": 684199322,
      "step": 20064,
      "train_runtime": 43908.6313,
      "train_tokens_per_second": 15582.342
    },
    {
      "epoch": 3.622859203172886,
      "grad_norm": 3.0585074424743652,
      "learning_rate": 1.3327228156013794e-05,
      "loss": 4.7056,
      "num_input_tokens_seen": 685301018,
      "step": 20096,
      "train_runtime": 43972.8414,
      "train_tokens_per_second": 15584.643
    },
    {
      "epoch": 3.6286280872543717,
      "grad_norm": 3.7060387134552,
      "learning_rate": 1.3303480051877233e-05,
      "loss": 4.7213,
      "num_input_tokens_seen": 686402778,
      "step": 20128,
      "train_runtime": 44036.7783,
      "train_tokens_per_second": 15587.034
    },
    {
      "epoch": 3.6343969713358573,
      "grad_norm": 3.612546920776367,
      "learning_rate": 1.3279711016328194e-05,
      "loss": 4.7593,
      "num_input_tokens_seen": 687495002,
      "step": 20160,
      "train_runtime": 44100.2163,
      "train_tokens_per_second": 15589.379
    },
    {
      "epoch": 3.640165855417343,
      "grad_norm": 3.1002161502838135,
      "learning_rate": 1.3255921199971333e-05,
      "loss": 4.8185,
      "num_input_tokens_seen": 688616026,
      "step": 20192,
      "train_runtime": 44165.8634,
      "train_tokens_per_second": 15591.59
    },
    {
      "epoch": 3.645934739498828,
      "grad_norm": 3.5075230598449707,
      "learning_rate": 1.3232110753542984e-05,
      "loss": 4.7279,
      "num_input_tokens_seen": 689733562,
      "step": 20224,
      "train_runtime": 44231.2774,
      "train_tokens_per_second": 15593.797
    },
    {
      "epoch": 3.6517036235803135,
      "grad_norm": 3.6522185802459717,
      "learning_rate": 1.3208279827910189e-05,
      "loss": 4.702,
      "num_input_tokens_seen": 690794010,
      "step": 20256,
      "train_runtime": 44293.9324,
      "train_tokens_per_second": 15595.68
    },
    {
      "epoch": 3.657472507661799,
      "grad_norm": 3.4408481121063232,
      "learning_rate": 1.3184428574069757e-05,
      "loss": 4.7505,
      "num_input_tokens_seen": 691916858,
      "step": 20288,
      "train_runtime": 44359.3648,
      "train_tokens_per_second": 15597.988
    },
    {
      "epoch": 3.6632413917432847,
      "grad_norm": 3.1062538623809814,
      "learning_rate": 1.3160557143147292e-05,
      "loss": 4.818,
      "num_input_tokens_seen": 693013466,
      "step": 20320,
      "train_runtime": 44423.3602,
      "train_tokens_per_second": 15600.204
    },
    {
      "epoch": 3.6690102758247702,
      "grad_norm": 3.8424501419067383,
      "learning_rate": 1.313666568639625e-05,
      "loss": 4.7181,
      "num_input_tokens_seen": 694085850,
      "step": 20352,
      "train_runtime": 44486.1115,
      "train_tokens_per_second": 15602.304
    },
    {
      "epoch": 3.6747791599062554,
      "grad_norm": 3.4384384155273438,
      "learning_rate": 1.3112754355196975e-05,
      "loss": 4.7592,
      "num_input_tokens_seen": 695196474,
      "step": 20384,
      "train_runtime": 44550.7633,
      "train_tokens_per_second": 15604.592
    },
    {
      "epoch": 3.6805480439877414,
      "grad_norm": 3.8710007667541504,
      "learning_rate": 1.3088823301055734e-05,
      "loss": 4.7271,
      "num_input_tokens_seen": 696294458,
      "step": 20416,
      "train_runtime": 44614.7722,
      "train_tokens_per_second": 15606.814
    },
    {
      "epoch": 3.6863169280692265,
      "grad_norm": 3.6038832664489746,
      "learning_rate": 1.3064872675603758e-05,
      "loss": 4.8188,
      "num_input_tokens_seen": 697400922,
      "step": 20448,
      "train_runtime": 44679.1242,
      "train_tokens_per_second": 15609.1
    },
    {
      "epoch": 3.692085812150712,
      "grad_norm": 3.3854124546051025,
      "learning_rate": 1.3040902630596295e-05,
      "loss": 4.7726,
      "num_input_tokens_seen": 698549978,
      "step": 20480,
      "train_runtime": 44745.4058,
      "train_tokens_per_second": 15611.658
    },
    {
      "epoch": 3.6978546962321976,
      "grad_norm": 3.514430046081543,
      "learning_rate": 1.3016913317911634e-05,
      "loss": 4.8233,
      "num_input_tokens_seen": 699655450,
      "step": 20512,
      "train_runtime": 44809.3855,
      "train_tokens_per_second": 15614.038
    },
    {
      "epoch": 3.703623580313683,
      "grad_norm": 3.531280517578125,
      "learning_rate": 1.299290488955015e-05,
      "loss": 4.7352,
      "num_input_tokens_seen": 700731578,
      "step": 20544,
      "train_runtime": 44872.3082,
      "train_tokens_per_second": 15616.125
    },
    {
      "epoch": 3.709392464395169,
      "grad_norm": 3.274825096130371,
      "learning_rate": 1.2968877497633331e-05,
      "loss": 4.7263,
      "num_input_tokens_seen": 701850778,
      "step": 20576,
      "train_runtime": 44937.2195,
      "train_tokens_per_second": 15618.474
    },
    {
      "epoch": 3.715161348476654,
      "grad_norm": 3.8136720657348633,
      "learning_rate": 1.2944831294402833e-05,
      "loss": 4.8306,
      "num_input_tokens_seen": 702936538,
      "step": 20608,
      "train_runtime": 45000.7035,
      "train_tokens_per_second": 15620.568
    },
    {
      "epoch": 3.7209302325581395,
      "grad_norm": 3.403876543045044,
      "learning_rate": 1.2920766432219488e-05,
      "loss": 4.7107,
      "num_input_tokens_seen": 704005402,
      "step": 20640,
      "train_runtime": 45063.2992,
      "train_tokens_per_second": 15622.589
    },
    {
      "epoch": 3.726699116639625,
      "grad_norm": 3.5059285163879395,
      "learning_rate": 1.289668306356237e-05,
      "loss": 4.7742,
      "num_input_tokens_seen": 705048474,
      "step": 20672,
      "train_runtime": 45124.6812,
      "train_tokens_per_second": 15624.453
    },
    {
      "epoch": 3.7324680007211106,
      "grad_norm": 3.6917805671691895,
      "learning_rate": 1.2872581341027805e-05,
      "loss": 4.7311,
      "num_input_tokens_seen": 706123578,
      "step": 20704,
      "train_runtime": 45187.6003,
      "train_tokens_per_second": 15626.49
    },
    {
      "epoch": 3.738236884802596,
      "grad_norm": 3.6456117630004883,
      "learning_rate": 1.2848461417328412e-05,
      "loss": 4.7624,
      "num_input_tokens_seen": 707199066,
      "step": 20736,
      "train_runtime": 45250.5081,
      "train_tokens_per_second": 15628.533
    },
    {
      "epoch": 3.7440057688840813,
      "grad_norm": 3.690079927444458,
      "learning_rate": 1.2824323445292135e-05,
      "loss": 4.76,
      "num_input_tokens_seen": 708268442,
      "step": 20768,
      "train_runtime": 45313.1044,
      "train_tokens_per_second": 15630.543
    },
    {
      "epoch": 3.749774652965567,
      "grad_norm": 3.57422137260437,
      "learning_rate": 1.2800167577861284e-05,
      "loss": 4.7241,
      "num_input_tokens_seen": 709337978,
      "step": 20800,
      "train_runtime": 45375.8158,
      "train_tokens_per_second": 15632.512
    },
    {
      "epoch": 3.7555435370470525,
      "grad_norm": 3.515902280807495,
      "learning_rate": 1.2775993968091548e-05,
      "loss": 4.7084,
      "num_input_tokens_seen": 710435290,
      "step": 20832,
      "train_runtime": 45439.7418,
      "train_tokens_per_second": 15634.668
    },
    {
      "epoch": 3.761312421128538,
      "grad_norm": 3.5811610221862793,
      "learning_rate": 1.2751802769151037e-05,
      "loss": 4.6678,
      "num_input_tokens_seen": 711506426,
      "step": 20864,
      "train_runtime": 45502.3289,
      "train_tokens_per_second": 15636.703
    },
    {
      "epoch": 3.7670813052100236,
      "grad_norm": 4.235470294952393,
      "learning_rate": 1.2727594134319305e-05,
      "loss": 4.816,
      "num_input_tokens_seen": 712625562,
      "step": 20896,
      "train_runtime": 45567.3786,
      "train_tokens_per_second": 15638.941
    },
    {
      "epoch": 3.7728501892915087,
      "grad_norm": 3.308051109313965,
      "learning_rate": 1.2703368216986395e-05,
      "loss": 4.7707,
      "num_input_tokens_seen": 713718330,
      "step": 20928,
      "train_runtime": 45631.1321,
      "train_tokens_per_second": 15641.039
    },
    {
      "epoch": 3.7786190733729943,
      "grad_norm": 3.352421760559082,
      "learning_rate": 1.2679125170651842e-05,
      "loss": 4.7012,
      "num_input_tokens_seen": 714767898,
      "step": 20960,
      "train_runtime": 45692.7941,
      "train_tokens_per_second": 15642.902
    },
    {
      "epoch": 3.78438795745448,
      "grad_norm": 3.891735553741455,
      "learning_rate": 1.265486514892372e-05,
      "loss": 4.7886,
      "num_input_tokens_seen": 715857434,
      "step": 20992,
      "train_runtime": 45756.4495,
      "train_tokens_per_second": 15644.952
    },
    {
      "epoch": 3.7901568415359654,
      "grad_norm": 3.4336929321289062,
      "learning_rate": 1.2630588305517662e-05,
      "loss": 4.7801,
      "num_input_tokens_seen": 716953242,
      "step": 21024,
      "train_runtime": 45820.3724,
      "train_tokens_per_second": 15647.041
    },
    {
      "epoch": 3.795925725617451,
      "grad_norm": 3.2055702209472656,
      "learning_rate": 1.2606294794255878e-05,
      "loss": 4.7263,
      "num_input_tokens_seen": 718052154,
      "step": 21056,
      "train_runtime": 45884.3748,
      "train_tokens_per_second": 15649.165
    },
    {
      "epoch": 3.801694609698936,
      "grad_norm": 3.615787982940674,
      "learning_rate": 1.2581984769066203e-05,
      "loss": 4.7582,
      "num_input_tokens_seen": 719120730,
      "step": 21088,
      "train_runtime": 45947.0633,
      "train_tokens_per_second": 15651.071
    },
    {
      "epoch": 3.8074634937804217,
      "grad_norm": 3.2415945529937744,
      "learning_rate": 1.255765838398109e-05,
      "loss": 4.6953,
      "num_input_tokens_seen": 720209562,
      "step": 21120,
      "train_runtime": 46010.6908,
      "train_tokens_per_second": 15653.092
    },
    {
      "epoch": 3.8132323778619073,
      "grad_norm": 3.915395736694336,
      "learning_rate": 1.2533315793136664e-05,
      "loss": 4.707,
      "num_input_tokens_seen": 721311002,
      "step": 21152,
      "train_runtime": 46074.91,
      "train_tokens_per_second": 15655.18
    },
    {
      "epoch": 3.819001261943393,
      "grad_norm": 3.0062172412872314,
      "learning_rate": 1.2508957150771735e-05,
      "loss": 4.7596,
      "num_input_tokens_seen": 722426106,
      "step": 21184,
      "train_runtime": 46139.706,
      "train_tokens_per_second": 15657.363
    },
    {
      "epoch": 3.8247701460248784,
      "grad_norm": 3.5864977836608887,
      "learning_rate": 1.24845826112268e-05,
      "loss": 4.7852,
      "num_input_tokens_seen": 723546202,
      "step": 21216,
      "train_runtime": 46204.856,
      "train_tokens_per_second": 15659.527
    },
    {
      "epoch": 3.8305390301063635,
      "grad_norm": 3.2372655868530273,
      "learning_rate": 1.24601923289431e-05,
      "loss": 4.7727,
      "num_input_tokens_seen": 724642906,
      "step": 21248,
      "train_runtime": 46268.6705,
      "train_tokens_per_second": 15661.632
    },
    {
      "epoch": 3.8363079141878496,
      "grad_norm": 3.4235150814056396,
      "learning_rate": 1.2435786458461618e-05,
      "loss": 4.6994,
      "num_input_tokens_seen": 725682586,
      "step": 21280,
      "train_runtime": 46329.7216,
      "train_tokens_per_second": 15663.435
    },
    {
      "epoch": 3.8420767982693347,
      "grad_norm": 3.4388885498046875,
      "learning_rate": 1.2411365154422113e-05,
      "loss": 4.7651,
      "num_input_tokens_seen": 726743002,
      "step": 21312,
      "train_runtime": 46391.6317,
      "train_tokens_per_second": 15665.39
    },
    {
      "epoch": 3.8478456823508203,
      "grad_norm": 3.6956417560577393,
      "learning_rate": 1.2386928571562119e-05,
      "loss": 4.7267,
      "num_input_tokens_seen": 727834202,
      "step": 21344,
      "train_runtime": 46455.0816,
      "train_tokens_per_second": 15667.483
    },
    {
      "epoch": 3.853614566432306,
      "grad_norm": 3.5856399536132812,
      "learning_rate": 1.2362476864715998e-05,
      "loss": 4.7211,
      "num_input_tokens_seen": 728932730,
      "step": 21376,
      "train_runtime": 46518.9439,
      "train_tokens_per_second": 15669.589
    },
    {
      "epoch": 3.8593834505137914,
      "grad_norm": 3.240321159362793,
      "learning_rate": 1.2338010188813927e-05,
      "loss": 4.7494,
      "num_input_tokens_seen": 730044986,
      "step": 21408,
      "train_runtime": 46583.5642,
      "train_tokens_per_second": 15671.729
    },
    {
      "epoch": 3.865152334595277,
      "grad_norm": 3.4376065731048584,
      "learning_rate": 1.231352869888093e-05,
      "loss": 4.8075,
      "num_input_tokens_seen": 731104602,
      "step": 21440,
      "train_runtime": 46645.6493,
      "train_tokens_per_second": 15673.586
    },
    {
      "epoch": 3.870921218676762,
      "grad_norm": 3.1283822059631348,
      "learning_rate": 1.2289032550035902e-05,
      "loss": 4.7481,
      "num_input_tokens_seen": 732200794,
      "step": 21472,
      "train_runtime": 46709.3855,
      "train_tokens_per_second": 15675.667
    },
    {
      "epoch": 3.8766901027582477,
      "grad_norm": 3.121706247329712,
      "learning_rate": 1.2264521897490617e-05,
      "loss": 4.6926,
      "num_input_tokens_seen": 733319994,
      "step": 21504,
      "train_runtime": 46774.3999,
      "train_tokens_per_second": 15677.807
    },
    {
      "epoch": 3.8824589868397332,
      "grad_norm": 3.498020648956299,
      "learning_rate": 1.2239996896548746e-05,
      "loss": 4.799,
      "num_input_tokens_seen": 734418266,
      "step": 21536,
      "train_runtime": 46838.3087,
      "train_tokens_per_second": 15679.863
    },
    {
      "epoch": 3.888227870921219,
      "grad_norm": 3.615661144256592,
      "learning_rate": 1.2215457702604867e-05,
      "loss": 4.7689,
      "num_input_tokens_seen": 735527162,
      "step": 21568,
      "train_runtime": 46902.7092,
      "train_tokens_per_second": 15681.976
    },
    {
      "epoch": 3.8939967550027044,
      "grad_norm": 3.3173937797546387,
      "learning_rate": 1.2190904471143503e-05,
      "loss": 4.7854,
      "num_input_tokens_seen": 736657818,
      "step": 21600,
      "train_runtime": 46968.2044,
      "train_tokens_per_second": 15684.181
    },
    {
      "epoch": 3.8997656390841895,
      "grad_norm": 3.3909049034118652,
      "learning_rate": 1.2166337357738108e-05,
      "loss": 4.7924,
      "num_input_tokens_seen": 737757562,
      "step": 21632,
      "train_runtime": 47032.2943,
      "train_tokens_per_second": 15686.191
    },
    {
      "epoch": 3.905534523165675,
      "grad_norm": 3.996185541152954,
      "learning_rate": 1.2141756518050096e-05,
      "loss": 4.7096,
      "num_input_tokens_seen": 738840762,
      "step": 21664,
      "train_runtime": 47095.5668,
      "train_tokens_per_second": 15688.117
    },
    {
      "epoch": 3.9113034072471606,
      "grad_norm": 3.469714879989624,
      "learning_rate": 1.2117162107827863e-05,
      "loss": 4.7227,
      "num_input_tokens_seen": 739889786,
      "step": 21696,
      "train_runtime": 47157.3213,
      "train_tokens_per_second": 15689.818
    },
    {
      "epoch": 3.917072291328646,
      "grad_norm": 3.5775279998779297,
      "learning_rate": 1.2092554282905775e-05,
      "loss": 4.7358,
      "num_input_tokens_seen": 740998970,
      "step": 21728,
      "train_runtime": 47221.8325,
      "train_tokens_per_second": 15691.872
    },
    {
      "epoch": 3.9228411754101318,
      "grad_norm": 3.4302499294281006,
      "learning_rate": 1.2067933199203205e-05,
      "loss": 4.771,
      "num_input_tokens_seen": 742087002,
      "step": 21760,
      "train_runtime": 47285.373,
      "train_tokens_per_second": 15693.796
    },
    {
      "epoch": 3.928610059491617,
      "grad_norm": 3.530515193939209,
      "learning_rate": 1.2043299012723537e-05,
      "loss": 4.7428,
      "num_input_tokens_seen": 743213210,
      "step": 21792,
      "train_runtime": 47350.813,
      "train_tokens_per_second": 15695.891
    },
    {
      "epoch": 3.9343789435731025,
      "grad_norm": 3.494300603866577,
      "learning_rate": 1.2018651879553173e-05,
      "loss": 4.7242,
      "num_input_tokens_seen": 744287258,
      "step": 21824,
      "train_runtime": 47413.6167,
      "train_tokens_per_second": 15697.753
    },
    {
      "epoch": 3.940147827654588,
      "grad_norm": 3.8710272312164307,
      "learning_rate": 1.1993991955860546e-05,
      "loss": 4.8322,
      "num_input_tokens_seen": 745406394,
      "step": 21856,
      "train_runtime": 47478.4988,
      "train_tokens_per_second": 15699.873
    },
    {
      "epoch": 3.9459167117360736,
      "grad_norm": 3.433681011199951,
      "learning_rate": 1.1969319397895141e-05,
      "loss": 4.7332,
      "num_input_tokens_seen": 746491770,
      "step": 21888,
      "train_runtime": 47541.8519,
      "train_tokens_per_second": 15701.781
    },
    {
      "epoch": 3.951685595817559,
      "grad_norm": 3.674109697341919,
      "learning_rate": 1.1944634361986483e-05,
      "loss": 4.7156,
      "num_input_tokens_seen": 747589498,
      "step": 21920,
      "train_runtime": 47605.8457,
      "train_tokens_per_second": 15703.733
    },
    {
      "epoch": 3.9574544798990443,
      "grad_norm": 3.6008870601654053,
      "learning_rate": 1.1919937004543163e-05,
      "loss": 4.7771,
      "num_input_tokens_seen": 748671994,
      "step": 21952,
      "train_runtime": 47669.0098,
      "train_tokens_per_second": 15705.633
    },
    {
      "epoch": 3.96322336398053,
      "grad_norm": 3.5166571140289307,
      "learning_rate": 1.1895227482051854e-05,
      "loss": 4.7069,
      "num_input_tokens_seen": 749760282,
      "step": 21984,
      "train_runtime": 47732.4302,
      "train_tokens_per_second": 15707.566
    },
    {
      "epoch": 3.9689922480620154,
      "grad_norm": 3.0758495330810547,
      "learning_rate": 1.1870505951076294e-05,
      "loss": 4.7567,
      "num_input_tokens_seen": 750888282,
      "step": 22016,
      "train_runtime": 47797.8088,
      "train_tokens_per_second": 15709.68
    },
    {
      "epoch": 3.974761132143501,
      "grad_norm": 3.746664047241211,
      "learning_rate": 1.184577256825632e-05,
      "loss": 4.7391,
      "num_input_tokens_seen": 751930394,
      "step": 22048,
      "train_runtime": 47859.0333,
      "train_tokens_per_second": 15711.358
    },
    {
      "epoch": 3.9805300162249866,
      "grad_norm": 3.664435386657715,
      "learning_rate": 1.1821027490306858e-05,
      "loss": 4.736,
      "num_input_tokens_seen": 753038394,
      "step": 22080,
      "train_runtime": 47923.3478,
      "train_tokens_per_second": 15713.393
    },
    {
      "epoch": 3.9862989003064717,
      "grad_norm": 3.9003732204437256,
      "learning_rate": 1.1796270874016936e-05,
      "loss": 4.7438,
      "num_input_tokens_seen": 754127418,
      "step": 22112,
      "train_runtime": 47986.888,
      "train_tokens_per_second": 15715.281
    },
    {
      "epoch": 3.9920677843879577,
      "grad_norm": 3.4392848014831543,
      "learning_rate": 1.1771502876248695e-05,
      "loss": 4.7226,
      "num_input_tokens_seen": 755206458,
      "step": 22144,
      "train_runtime": 48049.9251,
      "train_tokens_per_second": 15717.12
    },
    {
      "epoch": 3.997836668469443,
      "grad_norm": 3.591405153274536,
      "learning_rate": 1.1746723653936391e-05,
      "loss": 4.8017,
      "num_input_tokens_seen": 756306010,
      "step": 22176,
      "train_runtime": 48113.7671,
      "train_tokens_per_second": 15719.119
    },
    {
      "epoch": 4.0,
      "eval_accuracy": 0.018867817190023964,
      "eval_class_accuracy": 0.47297771912895015,
      "eval_exact_token_accuracy": 0.0,
      "eval_f1": 0.03698783180559956,
      "eval_loss": 2.687624216079712,
      "eval_mean_iou": 0.15427231425830787,
      "eval_mean_iou_25": 0.15427231425830787,
      "eval_precision": 0.038624100657098155,
      "eval_prediction_target_similarity": 6.787691799073818e-05,
      "eval_recall": 0.0354877864771285,
      "eval_recall_25": 0.08638469794867823,
      "eval_runtime": 1273.6469,
      "eval_samples_per_second": 3.888,
      "eval_sequence_accuracy": 0.0,
      "eval_steps_per_second": 0.122,
      "eval_top3_token_accuracy": 0.05489888787269592,
      "eval_top5_token_accuracy": 0.09822749346494675,
      "num_input_tokens_seen": 756708466,
      "step": 22188
    },
    {
      "epoch": 4.003605552550929,
      "grad_norm": 3.322425127029419,
      "learning_rate": 1.1721933364085393e-05,
      "loss": 4.7716,
      "num_input_tokens_seen": 757394322,
      "step": 22208,
      "train_runtime": 49452.8306,
      "train_tokens_per_second": 15315.49
    },
    {
      "epoch": 4.009374436632414,
      "grad_norm": 3.7084691524505615,
      "learning_rate": 1.169713216377121e-05,
      "loss": 4.7534,
      "num_input_tokens_seen": 758521714,
      "step": 22240,
      "train_runtime": 49518.3414,
      "train_tokens_per_second": 15317.995
    },
    {
      "epoch": 4.015143320713899,
      "grad_norm": 3.4881510734558105,
      "learning_rate": 1.1672320210138466e-05,
      "loss": 4.7632,
      "num_input_tokens_seen": 759641714,
      "step": 22272,
      "train_runtime": 49583.6276,
      "train_tokens_per_second": 15320.414
    },
    {
      "epoch": 4.020912204795385,
      "grad_norm": 3.269408941268921,
      "learning_rate": 1.1647497660399929e-05,
      "loss": 4.6812,
      "num_input_tokens_seen": 760766482,
      "step": 22304,
      "train_runtime": 49649.0631,
      "train_tokens_per_second": 15322.877
    },
    {
      "epoch": 4.02668108887687,
      "grad_norm": 3.5884978771209717,
      "learning_rate": 1.16226646718355e-05,
      "loss": 4.7633,
      "num_input_tokens_seen": 761839058,
      "step": 22336,
      "train_runtime": 49712.1019,
      "train_tokens_per_second": 15325.022
    },
    {
      "epoch": 4.032449972958356,
      "grad_norm": 3.692598819732666,
      "learning_rate": 1.1597821401791234e-05,
      "loss": 4.7328,
      "num_input_tokens_seen": 762918674,
      "step": 22368,
      "train_runtime": 49775.5645,
      "train_tokens_per_second": 15327.173
    },
    {
      "epoch": 4.038218857039841,
      "grad_norm": 3.784428596496582,
      "learning_rate": 1.1572968007678318e-05,
      "loss": 4.7329,
      "num_input_tokens_seen": 763986098,
      "step": 22400,
      "train_runtime": 49838.2991,
      "train_tokens_per_second": 15329.297
    },
    {
      "epoch": 4.0439877411213265,
      "grad_norm": 3.5011539459228516,
      "learning_rate": 1.1548104646972091e-05,
      "loss": 4.7153,
      "num_input_tokens_seen": 765106098,
      "step": 22432,
      "train_runtime": 49903.4252,
      "train_tokens_per_second": 15331.735
    },
    {
      "epoch": 4.0497566252028125,
      "grad_norm": 3.3577470779418945,
      "learning_rate": 1.1523231477211044e-05,
      "loss": 4.6591,
      "num_input_tokens_seen": 766187218,
      "step": 22464,
      "train_runtime": 49966.6693,
      "train_tokens_per_second": 15333.966
    },
    {
      "epoch": 4.055525509284298,
      "grad_norm": 3.421644687652588,
      "learning_rate": 1.1498348655995817e-05,
      "loss": 4.7207,
      "num_input_tokens_seen": 767270802,
      "step": 22496,
      "train_runtime": 50030.2593,
      "train_tokens_per_second": 15336.135
    },
    {
      "epoch": 4.061294393365784,
      "grad_norm": 3.836932897567749,
      "learning_rate": 1.1473456340988208e-05,
      "loss": 4.7226,
      "num_input_tokens_seen": 768395570,
      "step": 22528,
      "train_runtime": 50095.8074,
      "train_tokens_per_second": 15338.521
    },
    {
      "epoch": 4.067063277447269,
      "grad_norm": 3.611565113067627,
      "learning_rate": 1.1448554689910163e-05,
      "loss": 4.6543,
      "num_input_tokens_seen": 769482866,
      "step": 22560,
      "train_runtime": 50159.3911,
      "train_tokens_per_second": 15340.754
    },
    {
      "epoch": 4.072832161528754,
      "grad_norm": 3.335110902786255,
      "learning_rate": 1.142364386054279e-05,
      "loss": 4.73,
      "num_input_tokens_seen": 770598098,
      "step": 22592,
      "train_runtime": 50224.2057,
      "train_tokens_per_second": 15343.161
    },
    {
      "epoch": 4.07860104561024,
      "grad_norm": 3.224360466003418,
      "learning_rate": 1.1398724010725343e-05,
      "loss": 4.7384,
      "num_input_tokens_seen": 771683410,
      "step": 22624,
      "train_runtime": 50287.7476,
      "train_tokens_per_second": 15345.356
    },
    {
      "epoch": 4.084369929691725,
      "grad_norm": 3.3530168533325195,
      "learning_rate": 1.1373795298354243e-05,
      "loss": 4.6861,
      "num_input_tokens_seen": 772764530,
      "step": 22656,
      "train_runtime": 50350.9175,
      "train_tokens_per_second": 15347.576
    },
    {
      "epoch": 4.090138813773211,
      "grad_norm": 3.315657377243042,
      "learning_rate": 1.1348857881382054e-05,
      "loss": 4.7307,
      "num_input_tokens_seen": 773852402,
      "step": 22688,
      "train_runtime": 50414.5995,
      "train_tokens_per_second": 15349.768
    },
    {
      "epoch": 4.095907697854696,
      "grad_norm": 3.7837462425231934,
      "learning_rate": 1.1323911917816504e-05,
      "loss": 4.7257,
      "num_input_tokens_seen": 774933362,
      "step": 22720,
      "train_runtime": 50478.0063,
      "train_tokens_per_second": 15351.901
    },
    {
      "epoch": 4.101676581936181,
      "grad_norm": 3.6813290119171143,
      "learning_rate": 1.1298957565719459e-05,
      "loss": 4.7712,
      "num_input_tokens_seen": 776047250,
      "step": 22752,
      "train_runtime": 50543.0082,
      "train_tokens_per_second": 15354.196
    },
    {
      "epoch": 4.107445466017667,
      "grad_norm": 3.6600236892700195,
      "learning_rate": 1.1273994983205956e-05,
      "loss": 4.7352,
      "num_input_tokens_seen": 777141042,
      "step": 22784,
      "train_runtime": 50607.1945,
      "train_tokens_per_second": 15356.335
    },
    {
      "epoch": 4.1132143500991525,
      "grad_norm": 3.624169111251831,
      "learning_rate": 1.1249024328443167e-05,
      "loss": 4.7659,
      "num_input_tokens_seen": 778218706,
      "step": 22816,
      "train_runtime": 50670.4956,
      "train_tokens_per_second": 15358.419
    },
    {
      "epoch": 4.1189832341806385,
      "grad_norm": 3.5588037967681885,
      "learning_rate": 1.1224045759649416e-05,
      "loss": 4.69,
      "num_input_tokens_seen": 779302898,
      "step": 22848,
      "train_runtime": 50734.1082,
      "train_tokens_per_second": 15360.532
    },
    {
      "epoch": 4.124752118262124,
      "grad_norm": 3.3021788597106934,
      "learning_rate": 1.119905943509317e-05,
      "loss": 4.7142,
      "num_input_tokens_seen": 780396658,
      "step": 22880,
      "train_runtime": 50798.2195,
      "train_tokens_per_second": 15362.677
    },
    {
      "epoch": 4.130521002343609,
      "grad_norm": 3.527541160583496,
      "learning_rate": 1.1174065513092039e-05,
      "loss": 4.6482,
      "num_input_tokens_seen": 781466226,
      "step": 22912,
      "train_runtime": 50860.8215,
      "train_tokens_per_second": 15364.798
    },
    {
      "epoch": 4.136289886425095,
      "grad_norm": 3.7377336025238037,
      "learning_rate": 1.114906415201177e-05,
      "loss": 4.7133,
      "num_input_tokens_seen": 782530194,
      "step": 22944,
      "train_runtime": 50923.2626,
      "train_tokens_per_second": 15366.851
    },
    {
      "epoch": 4.14205877050658,
      "grad_norm": 3.7292075157165527,
      "learning_rate": 1.112405551026525e-05,
      "loss": 4.769,
      "num_input_tokens_seen": 783618770,
      "step": 22976,
      "train_runtime": 50987.0855,
      "train_tokens_per_second": 15368.966
    },
    {
      "epoch": 4.147827654588066,
      "grad_norm": 3.5705726146698,
      "learning_rate": 1.109903974631149e-05,
      "loss": 4.7405,
      "num_input_tokens_seen": 784665906,
      "step": 23008,
      "train_runtime": 51048.8547,
      "train_tokens_per_second": 15370.882
    },
    {
      "epoch": 4.153596538669551,
      "grad_norm": 3.7855429649353027,
      "learning_rate": 1.1074017018654638e-05,
      "loss": 4.7358,
      "num_input_tokens_seen": 785774674,
      "step": 23040,
      "train_runtime": 51113.5799,
      "train_tokens_per_second": 15373.11
    },
    {
      "epoch": 4.159365422751037,
      "grad_norm": 3.2799642086029053,
      "learning_rate": 1.1048987485842957e-05,
      "loss": 4.7343,
      "num_input_tokens_seen": 786878930,
      "step": 23072,
      "train_runtime": 51178.1702,
      "train_tokens_per_second": 15375.285
    },
    {
      "epoch": 4.165134306832522,
      "grad_norm": 3.4690024852752686,
      "learning_rate": 1.102395130646783e-05,
      "loss": 4.6615,
      "num_input_tokens_seen": 787911058,
      "step": 23104,
      "train_runtime": 51239.3418,
      "train_tokens_per_second": 15377.072
    },
    {
      "epoch": 4.170903190914007,
      "grad_norm": 3.5920135974884033,
      "learning_rate": 1.0998908639162757e-05,
      "loss": 4.7234,
      "num_input_tokens_seen": 788988146,
      "step": 23136,
      "train_runtime": 51302.4964,
      "train_tokens_per_second": 15379.137
    },
    {
      "epoch": 4.176672074995493,
      "grad_norm": 4.101573467254639,
      "learning_rate": 1.097385964260235e-05,
      "loss": 4.7661,
      "num_input_tokens_seen": 790086386,
      "step": 23168,
      "train_runtime": 51366.4148,
      "train_tokens_per_second": 15381.381
    },
    {
      "epoch": 4.182440959076978,
      "grad_norm": 3.954244613647461,
      "learning_rate": 1.0948804475501312e-05,
      "loss": 4.7574,
      "num_input_tokens_seen": 791191058,
      "step": 23200,
      "train_runtime": 51430.7331,
      "train_tokens_per_second": 15383.624
    },
    {
      "epoch": 4.1882098431584645,
      "grad_norm": 3.510561466217041,
      "learning_rate": 1.092374329661345e-05,
      "loss": 4.6726,
      "num_input_tokens_seen": 792246258,
      "step": 23232,
      "train_runtime": 51492.6381,
      "train_tokens_per_second": 15385.622
    },
    {
      "epoch": 4.19397872723995,
      "grad_norm": 3.424189329147339,
      "learning_rate": 1.089867626473067e-05,
      "loss": 4.7374,
      "num_input_tokens_seen": 793369330,
      "step": 23264,
      "train_runtime": 51557.7927,
      "train_tokens_per_second": 15387.962
    },
    {
      "epoch": 4.199747611321435,
      "grad_norm": 3.774461269378662,
      "learning_rate": 1.0873603538681953e-05,
      "loss": 4.6784,
      "num_input_tokens_seen": 794399058,
      "step": 23296,
      "train_runtime": 51618.5801,
      "train_tokens_per_second": 15389.789
    },
    {
      "epoch": 4.205516495402921,
      "grad_norm": 3.7306530475616455,
      "learning_rate": 1.0848525277332363e-05,
      "loss": 4.7226,
      "num_input_tokens_seen": 795478994,
      "step": 23328,
      "train_runtime": 51681.9773,
      "train_tokens_per_second": 15391.806
    },
    {
      "epoch": 4.211285379484406,
      "grad_norm": 3.9046568870544434,
      "learning_rate": 1.082344163958204e-05,
      "loss": 4.7351,
      "num_input_tokens_seen": 796583186,
      "step": 23360,
      "train_runtime": 51746.5316,
      "train_tokens_per_second": 15393.944
    },
    {
      "epoch": 4.217054263565892,
      "grad_norm": 3.482933759689331,
      "learning_rate": 1.0798352784365184e-05,
      "loss": 4.6795,
      "num_input_tokens_seen": 797642418,
      "step": 23392,
      "train_runtime": 51809.0643,
      "train_tokens_per_second": 15395.808
    },
    {
      "epoch": 4.222823147647377,
      "grad_norm": 3.449554920196533,
      "learning_rate": 1.0773258870649057e-05,
      "loss": 4.7442,
      "num_input_tokens_seen": 798724946,
      "step": 23424,
      "train_runtime": 51872.6388,
      "train_tokens_per_second": 15397.808
    },
    {
      "epoch": 4.228592031728862,
      "grad_norm": 4.303156852722168,
      "learning_rate": 1.0748160057432973e-05,
      "loss": 4.7159,
      "num_input_tokens_seen": 799824786,
      "step": 23456,
      "train_runtime": 51936.9573,
      "train_tokens_per_second": 15399.916
    },
    {
      "epoch": 4.234360915810348,
      "grad_norm": 3.8373944759368896,
      "learning_rate": 1.0723056503747289e-05,
      "loss": 4.6786,
      "num_input_tokens_seen": 800938930,
      "step": 23488,
      "train_runtime": 52002.1012,
      "train_tokens_per_second": 15402.049
    },
    {
      "epoch": 4.240129799891833,
      "grad_norm": 3.4864633083343506,
      "learning_rate": 1.0697948368652396e-05,
      "loss": 4.7841,
      "num_input_tokens_seen": 802027282,
      "step": 23520,
      "train_runtime": 52065.7647,
      "train_tokens_per_second": 15404.12
    },
    {
      "epoch": 4.245898683973319,
      "grad_norm": 3.4210124015808105,
      "learning_rate": 1.0672835811237719e-05,
      "loss": 4.7077,
      "num_input_tokens_seen": 803126450,
      "step": 23552,
      "train_runtime": 52129.734,
      "train_tokens_per_second": 15406.302
    },
    {
      "epoch": 4.251667568054804,
      "grad_norm": 3.5947563648223877,
      "learning_rate": 1.06477189906207e-05,
      "loss": 4.769,
      "num_input_tokens_seen": 804186610,
      "step": 23584,
      "train_runtime": 52191.8999,
      "train_tokens_per_second": 15408.265
    },
    {
      "epoch": 4.2574364521362895,
      "grad_norm": 3.976348400115967,
      "learning_rate": 1.0622598065945793e-05,
      "loss": 4.7185,
      "num_input_tokens_seen": 805277906,
      "step": 23616,
      "train_runtime": 52255.57,
      "train_tokens_per_second": 15410.375
    },
    {
      "epoch": 4.2632053362177755,
      "grad_norm": 3.7820491790771484,
      "learning_rate": 1.0597473196383458e-05,
      "loss": 4.7236,
      "num_input_tokens_seen": 806389010,
      "step": 23648,
      "train_runtime": 52320.1533,
      "train_tokens_per_second": 15412.589
    },
    {
      "epoch": 4.268974220299261,
      "grad_norm": 3.75384521484375,
      "learning_rate": 1.0572344541129147e-05,
      "loss": 4.6898,
      "num_input_tokens_seen": 807454226,
      "step": 23680,
      "train_runtime": 52382.6366,
      "train_tokens_per_second": 15414.54
    },
    {
      "epoch": 4.274743104380747,
      "grad_norm": 3.841655969619751,
      "learning_rate": 1.0547212259402306e-05,
      "loss": 4.7216,
      "num_input_tokens_seen": 808523826,
      "step": 23712,
      "train_runtime": 52445.3548,
      "train_tokens_per_second": 15416.5
    },
    {
      "epoch": 4.280511988462232,
      "grad_norm": 4.107283115386963,
      "learning_rate": 1.0522076510445349e-05,
      "loss": 4.726,
      "num_input_tokens_seen": 809658098,
      "step": 23744,
      "train_runtime": 52511.374,
      "train_tokens_per_second": 15418.719
    },
    {
      "epoch": 4.286280872543717,
      "grad_norm": 3.233168125152588,
      "learning_rate": 1.0496937453522669e-05,
      "loss": 4.6632,
      "num_input_tokens_seen": 810732018,
      "step": 23776,
      "train_runtime": 52574.511,
      "train_tokens_per_second": 15420.629
    },
    {
      "epoch": 4.292049756625203,
      "grad_norm": 4.236541271209717,
      "learning_rate": 1.0471795247919611e-05,
      "loss": 4.6939,
      "num_input_tokens_seen": 811821394,
      "step": 23808,
      "train_runtime": 52638.3801,
      "train_tokens_per_second": 15422.614
    },
    {
      "epoch": 4.297818640706688,
      "grad_norm": 3.481049060821533,
      "learning_rate": 1.0446650052941475e-05,
      "loss": 4.769,
      "num_input_tokens_seen": 812919666,
      "step": 23840,
      "train_runtime": 52702.3469,
      "train_tokens_per_second": 15424.734
    },
    {
      "epoch": 4.303587524788174,
      "grad_norm": 3.748762607574463,
      "learning_rate": 1.0421502027912501e-05,
      "loss": 4.7269,
      "num_input_tokens_seen": 814019154,
      "step": 23872,
      "train_runtime": 52766.3308,
      "train_tokens_per_second": 15426.867
    },
    {
      "epoch": 4.309356408869659,
      "grad_norm": 3.3550074100494385,
      "learning_rate": 1.0396351332174857e-05,
      "loss": 4.7278,
      "num_input_tokens_seen": 815093778,
      "step": 23904,
      "train_runtime": 52829.135,
      "train_tokens_per_second": 15428.869
    },
    {
      "epoch": 4.315125292951144,
      "grad_norm": 3.4375460147857666,
      "learning_rate": 1.0371198125087638e-05,
      "loss": 4.6558,
      "num_input_tokens_seen": 816138226,
      "step": 23936,
      "train_runtime": 52890.5316,
      "train_tokens_per_second": 15430.706
    },
    {
      "epoch": 4.32089417703263,
      "grad_norm": 3.9269869327545166,
      "learning_rate": 1.0346042566025856e-05,
      "loss": 4.7604,
      "num_input_tokens_seen": 817233074,
      "step": 23968,
      "train_runtime": 52954.2763,
      "train_tokens_per_second": 15432.806
    },
    {
      "epoch": 4.3266630611141155,
      "grad_norm": 4.104332447052002,
      "learning_rate": 1.032088481437941e-05,
      "loss": 4.6889,
      "num_input_tokens_seen": 818302994,
      "step": 24000,
      "train_runtime": 53016.9319,
      "train_tokens_per_second": 15434.748
    },
    {
      "epoch": 4.3324319451956015,
      "grad_norm": 3.759237766265869,
      "learning_rate": 1.0295725029552106e-05,
      "loss": 4.7646,
      "num_input_tokens_seen": 819429362,
      "step": 24032,
      "train_runtime": 53082.4083,
      "train_tokens_per_second": 15436.929
    },
    {
      "epoch": 4.338200829277087,
      "grad_norm": 3.9689974784851074,
      "learning_rate": 1.027056337096063e-05,
      "loss": 4.679,
      "num_input_tokens_seen": 820505266,
      "step": 24064,
      "train_runtime": 53145.555,
      "train_tokens_per_second": 15438.831
    },
    {
      "epoch": 4.343969713358572,
      "grad_norm": 3.6698718070983887,
      "learning_rate": 1.0245399998033537e-05,
      "loss": 4.76,
      "num_input_tokens_seen": 821621682,
      "step": 24096,
      "train_runtime": 53210.828,
      "train_tokens_per_second": 15440.874
    },
    {
      "epoch": 4.349738597440058,
      "grad_norm": 3.2157909870147705,
      "learning_rate": 1.0220235070210236e-05,
      "loss": 4.6542,
      "num_input_tokens_seen": 822704850,
      "step": 24128,
      "train_runtime": 53274.3498,
      "train_tokens_per_second": 15442.795
    },
    {
      "epoch": 4.355507481521543,
      "grad_norm": 3.6067965030670166,
      "learning_rate": 1.0195068746940008e-05,
      "loss": 4.7033,
      "num_input_tokens_seen": 823793298,
      "step": 24160,
      "train_runtime": 53338.1656,
      "train_tokens_per_second": 15444.725
    },
    {
      "epoch": 4.361276365603029,
      "grad_norm": 3.3264412879943848,
      "learning_rate": 1.0169901187680966e-05,
      "loss": 4.7422,
      "num_input_tokens_seen": 824896434,
      "step": 24192,
      "train_runtime": 53402.6833,
      "train_tokens_per_second": 15446.723
    },
    {
      "epoch": 4.367045249684514,
      "grad_norm": 3.9756557941436768,
      "learning_rate": 1.0144732551899044e-05,
      "loss": 4.7217,
      "num_input_tokens_seen": 825987922,
      "step": 24224,
      "train_runtime": 53466.5381,
      "train_tokens_per_second": 15448.689
    },
    {
      "epoch": 4.372814133766,
      "grad_norm": 4.202337741851807,
      "learning_rate": 1.0119562999067018e-05,
      "loss": 4.6944,
      "num_input_tokens_seen": 827066802,
      "step": 24256,
      "train_runtime": 53529.806,
      "train_tokens_per_second": 15450.585
    },
    {
      "epoch": 4.378583017847485,
      "grad_norm": 4.183559417724609,
      "learning_rate": 1.0094392688663457e-05,
      "loss": 4.7084,
      "num_input_tokens_seen": 828170130,
      "step": 24288,
      "train_runtime": 53594.4041,
      "train_tokens_per_second": 15452.549
    },
    {
      "epoch": 4.38435190192897,
      "grad_norm": 3.353914499282837,
      "learning_rate": 1.0069221780171733e-05,
      "loss": 4.7213,
      "num_input_tokens_seen": 829240594,
      "step": 24320,
      "train_runtime": 53657.3546,
      "train_tokens_per_second": 15454.37
    },
    {
      "epoch": 4.390120786010456,
      "grad_norm": 3.8924636840820312,
      "learning_rate": 1.004405043307902e-05,
      "loss": 4.711,
      "num_input_tokens_seen": 830321682,
      "step": 24352,
      "train_runtime": 53720.7955,
      "train_tokens_per_second": 15456.243
    },
    {
      "epoch": 4.395889670091941,
      "grad_norm": 3.2245523929595947,
      "learning_rate": 1.0018878806875256e-05,
      "loss": 4.7244,
      "num_input_tokens_seen": 831412146,
      "step": 24384,
      "train_runtime": 53784.6948,
      "train_tokens_per_second": 15458.155
    },
    {
      "epoch": 4.401658554173427,
      "grad_norm": 3.6359615325927734,
      "learning_rate": 9.993707061052154e-06,
      "loss": 4.7259,
      "num_input_tokens_seen": 832439442,
      "step": 24416,
      "train_runtime": 53845.6494,
      "train_tokens_per_second": 15459.734
    },
    {
      "epoch": 4.407427438254913,
      "grad_norm": 3.558703660964966,
      "learning_rate": 9.968535355102185e-06,
      "loss": 4.7243,
      "num_input_tokens_seen": 833514034,
      "step": 24448,
      "train_runtime": 53908.8139,
      "train_tokens_per_second": 15461.554
    },
    {
      "epoch": 4.413196322336398,
      "grad_norm": 3.5116724967956543,
      "learning_rate": 9.943363848517572e-06,
      "loss": 4.7257,
      "num_input_tokens_seen": 834638130,
      "step": 24480,
      "train_runtime": 53974.0657,
      "train_tokens_per_second": 15463.688
    },
    {
      "epoch": 4.418965206417884,
      "grad_norm": 3.2311995029449463,
      "learning_rate": 9.918192700789259e-06,
      "loss": 4.793,
      "num_input_tokens_seen": 835786738,
      "step": 24512,
      "train_runtime": 54040.4518,
      "train_tokens_per_second": 15465.947
    },
    {
      "epoch": 4.424734090499369,
      "grad_norm": 3.5584352016448975,
      "learning_rate": 9.893022071405933e-06,
      "loss": 4.7212,
      "num_input_tokens_seen": 836900818,
      "step": 24544,
      "train_runtime": 54105.1238,
      "train_tokens_per_second": 15468.051
    },
    {
      "epoch": 4.430502974580855,
      "grad_norm": 3.4493916034698486,
      "learning_rate": 9.867852119852997e-06,
      "loss": 4.6993,
      "num_input_tokens_seen": 838031762,
      "step": 24576,
      "train_runtime": 54170.6649,
      "train_tokens_per_second": 15470.214
    },
    {
      "epoch": 4.43627185866234,
      "grad_norm": 3.502546787261963,
      "learning_rate": 9.842683005611536e-06,
      "loss": 4.6416,
      "num_input_tokens_seen": 839099250,
      "step": 24608,
      "train_runtime": 54232.918,
      "train_tokens_per_second": 15472.139
    },
    {
      "epoch": 4.442040742743825,
      "grad_norm": 3.7298684120178223,
      "learning_rate": 9.817514888157361e-06,
      "loss": 4.7395,
      "num_input_tokens_seen": 840169234,
      "step": 24640,
      "train_runtime": 54295.6172,
      "train_tokens_per_second": 15473.979
    },
    {
      "epoch": 4.447809626825311,
      "grad_norm": 3.6185672283172607,
      "learning_rate": 9.792347926959952e-06,
      "loss": 4.6969,
      "num_input_tokens_seen": 841256402,
      "step": 24672,
      "train_runtime": 54359.3796,
      "train_tokens_per_second": 15475.828
    },
    {
      "epoch": 4.453578510906796,
      "grad_norm": 3.488919734954834,
      "learning_rate": 9.767182281481455e-06,
      "loss": 4.689,
      "num_input_tokens_seen": 842344242,
      "step": 24704,
      "train_runtime": 54423.186,
      "train_tokens_per_second": 15477.672
    },
    {
      "epoch": 4.459347394988282,
      "grad_norm": 3.7902278900146484,
      "learning_rate": 9.742018111175687e-06,
      "loss": 4.7401,
      "num_input_tokens_seen": 843451314,
      "step": 24736,
      "train_runtime": 54487.9351,
      "train_tokens_per_second": 15479.598
    },
    {
      "epoch": 4.465116279069767,
      "grad_norm": 3.976384401321411,
      "learning_rate": 9.716855575487132e-06,
      "loss": 4.7298,
      "num_input_tokens_seen": 844570610,
      "step": 24768,
      "train_runtime": 54553.2198,
      "train_tokens_per_second": 15481.591
    },
    {
      "epoch": 4.470885163151253,
      "grad_norm": 3.8750901222229004,
      "learning_rate": 9.69169483384989e-06,
      "loss": 4.7743,
      "num_input_tokens_seen": 845703026,
      "step": 24800,
      "train_runtime": 54619.0687,
      "train_tokens_per_second": 15483.659
    },
    {
      "epoch": 4.4766540472327385,
      "grad_norm": 3.4742050170898438,
      "learning_rate": 9.666536045686709e-06,
      "loss": 4.7062,
      "num_input_tokens_seen": 846781266,
      "step": 24832,
      "train_runtime": 54681.9878,
      "train_tokens_per_second": 15485.561
    },
    {
      "epoch": 4.482422931314224,
      "grad_norm": 3.7113752365112305,
      "learning_rate": 9.641379370407968e-06,
      "loss": 4.6802,
      "num_input_tokens_seen": 847842930,
      "step": 24864,
      "train_runtime": 54744.2228,
      "train_tokens_per_second": 15487.35
    },
    {
      "epoch": 4.48819181539571,
      "grad_norm": 3.760610818862915,
      "learning_rate": 9.616224967410636e-06,
      "loss": 4.7343,
      "num_input_tokens_seen": 848875666,
      "step": 24896,
      "train_runtime": 54805.3708,
      "train_tokens_per_second": 15488.914
    },
    {
      "epoch": 4.493960699477195,
      "grad_norm": 3.5955939292907715,
      "learning_rate": 9.591072996077302e-06,
      "loss": 4.6819,
      "num_input_tokens_seen": 849972402,
      "step": 24928,
      "train_runtime": 54869.206,
      "train_tokens_per_second": 15490.882
    },
    {
      "epoch": 4.499729583558681,
      "grad_norm": 3.6473441123962402,
      "learning_rate": 9.56592361577514e-06,
      "loss": 4.6943,
      "num_input_tokens_seen": 851028882,
      "step": 24960,
      "train_runtime": 54931.4574,
      "train_tokens_per_second": 15492.56
    },
    {
      "epoch": 4.505498467640166,
      "grad_norm": 3.6033058166503906,
      "learning_rate": 9.540776985854908e-06,
      "loss": 4.7153,
      "num_input_tokens_seen": 852135602,
      "step": 24992,
      "train_runtime": 54995.7041,
      "train_tokens_per_second": 15494.585
    },
    {
      "epoch": 4.511267351721651,
      "grad_norm": 3.5584702491760254,
      "learning_rate": 9.515633265649943e-06,
      "loss": 4.6952,
      "num_input_tokens_seen": 853218354,
      "step": 25024,
      "train_runtime": 55058.5571,
      "train_tokens_per_second": 15496.562
    },
    {
      "epoch": 4.517036235803137,
      "grad_norm": 3.405545234680176,
      "learning_rate": 9.490492614475134e-06,
      "loss": 4.682,
      "num_input_tokens_seen": 854276914,
      "step": 25056,
      "train_runtime": 55120.531,
      "train_tokens_per_second": 15498.343
    },
    {
      "epoch": 4.522805119884622,
      "grad_norm": 3.4271984100341797,
      "learning_rate": 9.465355191625932e-06,
      "loss": 4.7144,
      "num_input_tokens_seen": 855340818,
      "step": 25088,
      "train_runtime": 55182.8838,
      "train_tokens_per_second": 15500.111
    },
    {
      "epoch": 4.528574003966108,
      "grad_norm": 3.8405227661132812,
      "learning_rate": 9.440221156377334e-06,
      "loss": 4.7083,
      "num_input_tokens_seen": 856395538,
      "step": 25120,
      "train_runtime": 55244.7778,
      "train_tokens_per_second": 15501.837
    },
    {
      "epoch": 4.534342888047593,
      "grad_norm": 3.7330009937286377,
      "learning_rate": 9.415090667982867e-06,
      "loss": 4.7014,
      "num_input_tokens_seen": 857528306,
      "step": 25152,
      "train_runtime": 55310.3476,
      "train_tokens_per_second": 15503.94
    },
    {
      "epoch": 4.5401117721290785,
      "grad_norm": 4.124905586242676,
      "learning_rate": 9.38996388567359e-06,
      "loss": 4.7649,
      "num_input_tokens_seen": 858603154,
      "step": 25184,
      "train_runtime": 55373.2121,
      "train_tokens_per_second": 15505.749
    },
    {
      "epoch": 4.5458806562105645,
      "grad_norm": 3.592542886734009,
      "learning_rate": 9.364840968657077e-06,
      "loss": 4.7705,
      "num_input_tokens_seen": 859714930,
      "step": 25216,
      "train_runtime": 55437.8868,
      "train_tokens_per_second": 15507.715
    },
    {
      "epoch": 4.55164954029205,
      "grad_norm": 3.7679734230041504,
      "learning_rate": 9.339722076116407e-06,
      "loss": 4.7143,
      "num_input_tokens_seen": 860799506,
      "step": 25248,
      "train_runtime": 55501.3516,
      "train_tokens_per_second": 15509.523
    },
    {
      "epoch": 4.557418424373536,
      "grad_norm": 4.18757438659668,
      "learning_rate": 9.314607367209165e-06,
      "loss": 4.7776,
      "num_input_tokens_seen": 861920850,
      "step": 25280,
      "train_runtime": 55566.4208,
      "train_tokens_per_second": 15511.542
    },
    {
      "epoch": 4.563187308455021,
      "grad_norm": 3.7412524223327637,
      "learning_rate": 9.28949700106643e-06,
      "loss": 4.6619,
      "num_input_tokens_seen": 863052658,
      "step": 25312,
      "train_runtime": 55631.9947,
      "train_tokens_per_second": 15513.603
    },
    {
      "epoch": 4.568956192536506,
      "grad_norm": 3.5490562915802,
      "learning_rate": 9.264391136791753e-06,
      "loss": 4.719,
      "num_input_tokens_seen": 864174610,
      "step": 25344,
      "train_runtime": 55697.0362,
      "train_tokens_per_second": 15515.63
    },
    {
      "epoch": 4.574725076617992,
      "grad_norm": 3.737346887588501,
      "learning_rate": 9.239289933460176e-06,
      "loss": 4.7942,
      "num_input_tokens_seen": 865290994,
      "step": 25376,
      "train_runtime": 55761.8595,
      "train_tokens_per_second": 15517.614
    },
    {
      "epoch": 4.580493960699477,
      "grad_norm": 3.3676440715789795,
      "learning_rate": 9.214193550117189e-06,
      "loss": 4.783,
      "num_input_tokens_seen": 866385586,
      "step": 25408,
      "train_runtime": 55825.773,
      "train_tokens_per_second": 15519.455
    },
    {
      "epoch": 4.586262844780963,
      "grad_norm": 3.5460472106933594,
      "learning_rate": 9.189102145777764e-06,
      "loss": 4.732,
      "num_input_tokens_seen": 867448402,
      "step": 25440,
      "train_runtime": 55888.1633,
      "train_tokens_per_second": 15521.147
    },
    {
      "epoch": 4.592031728862448,
      "grad_norm": 3.066054344177246,
      "learning_rate": 9.164015879425312e-06,
      "loss": 4.626,
      "num_input_tokens_seen": 868551666,
      "step": 25472,
      "train_runtime": 55952.7576,
      "train_tokens_per_second": 15522.947
    },
    {
      "epoch": 4.597800612943933,
      "grad_norm": 3.8644886016845703,
      "learning_rate": 9.138934910010683e-06,
      "loss": 4.6824,
      "num_input_tokens_seen": 869630450,
      "step": 25504,
      "train_runtime": 56015.9617,
      "train_tokens_per_second": 15524.69
    },
    {
      "epoch": 4.603569497025419,
      "grad_norm": 3.366549015045166,
      "learning_rate": 9.113859396451181e-06,
      "loss": 4.6889,
      "num_input_tokens_seen": 870761362,
      "step": 25536,
      "train_runtime": 56081.4715,
      "train_tokens_per_second": 15526.721
    },
    {
      "epoch": 4.609338381106904,
      "grad_norm": 3.741568088531494,
      "learning_rate": 9.088789497629534e-06,
      "loss": 4.7529,
      "num_input_tokens_seen": 871898290,
      "step": 25568,
      "train_runtime": 56147.2476,
      "train_tokens_per_second": 15528.781
    },
    {
      "epoch": 4.61510726518839,
      "grad_norm": 3.356623411178589,
      "learning_rate": 9.063725372392881e-06,
      "loss": 4.6567,
      "num_input_tokens_seen": 873026130,
      "step": 25600,
      "train_runtime": 56212.7257,
      "train_tokens_per_second": 15530.756
    },
    {
      "epoch": 4.6208761492698756,
      "grad_norm": 3.7548086643218994,
      "learning_rate": 9.038667179551801e-06,
      "loss": 4.6651,
      "num_input_tokens_seen": 874074482,
      "step": 25632,
      "train_runtime": 56274.4297,
      "train_tokens_per_second": 15532.356
    },
    {
      "epoch": 4.626645033351361,
      "grad_norm": 3.605208396911621,
      "learning_rate": 9.013615077879274e-06,
      "loss": 4.7615,
      "num_input_tokens_seen": 875161298,
      "step": 25664,
      "train_runtime": 56338.1737,
      "train_tokens_per_second": 15534.073
    },
    {
      "epoch": 4.632413917432847,
      "grad_norm": 3.708808422088623,
      "learning_rate": 8.988569226109676e-06,
      "loss": 4.6948,
      "num_input_tokens_seen": 876251474,
      "step": 25696,
      "train_runtime": 56402.0745,
      "train_tokens_per_second": 15535.802
    },
    {
      "epoch": 4.638182801514332,
      "grad_norm": 3.7357535362243652,
      "learning_rate": 8.963529782937794e-06,
      "loss": 4.7177,
      "num_input_tokens_seen": 877356882,
      "step": 25728,
      "train_runtime": 56466.4199,
      "train_tokens_per_second": 15537.675
    },
    {
      "epoch": 4.643951685595818,
      "grad_norm": 4.007845401763916,
      "learning_rate": 8.938496907017815e-06,
      "loss": 4.7441,
      "num_input_tokens_seen": 878451122,
      "step": 25760,
      "train_runtime": 56530.4616,
      "train_tokens_per_second": 15539.429
    },
    {
      "epoch": 4.649720569677303,
      "grad_norm": 3.5874717235565186,
      "learning_rate": 8.913470756962296e-06,
      "loss": 4.7249,
      "num_input_tokens_seen": 879534450,
      "step": 25792,
      "train_runtime": 56593.8011,
      "train_tokens_per_second": 15541.18
    },
    {
      "epoch": 4.655489453758788,
      "grad_norm": 3.6376476287841797,
      "learning_rate": 8.888451491341193e-06,
      "loss": 4.7221,
      "num_input_tokens_seen": 880624786,
      "step": 25824,
      "train_runtime": 56657.3686,
      "train_tokens_per_second": 15542.988
    },
    {
      "epoch": 4.661258337840274,
      "grad_norm": 4.031990051269531,
      "learning_rate": 8.863439268680832e-06,
      "loss": 4.7008,
      "num_input_tokens_seen": 881707954,
      "step": 25856,
      "train_runtime": 56720.664,
      "train_tokens_per_second": 15544.74
    },
    {
      "epoch": 4.667027221921759,
      "grad_norm": 3.606391191482544,
      "learning_rate": 8.83843424746292e-06,
      "loss": 4.7553,
      "num_input_tokens_seen": 882789970,
      "step": 25888,
      "train_runtime": 56783.8779,
      "train_tokens_per_second": 15546.49
    },
    {
      "epoch": 4.672796106003245,
      "grad_norm": 3.519212007522583,
      "learning_rate": 8.813436586123533e-06,
      "loss": 4.7058,
      "num_input_tokens_seen": 883895602,
      "step": 25920,
      "train_runtime": 56848.4925,
      "train_tokens_per_second": 15548.268
    },
    {
      "epoch": 4.67856499008473,
      "grad_norm": 3.2261312007904053,
      "learning_rate": 8.78844644305211e-06,
      "loss": 4.7765,
      "num_input_tokens_seen": 885018418,
      "step": 25952,
      "train_runtime": 56913.7725,
      "train_tokens_per_second": 15550.163
    },
    {
      "epoch": 4.6843338741662155,
      "grad_norm": 3.707117795944214,
      "learning_rate": 8.763463976590455e-06,
      "loss": 4.7378,
      "num_input_tokens_seen": 886099346,
      "step": 25984,
      "train_runtime": 56976.8249,
      "train_tokens_per_second": 15551.926
    },
    {
      "epoch": 4.6901027582477015,
      "grad_norm": 3.70758056640625,
      "learning_rate": 8.73848934503174e-06,
      "loss": 4.7208,
      "num_input_tokens_seen": 887173074,
      "step": 26016,
      "train_runtime": 57039.6821,
      "train_tokens_per_second": 15553.612
    },
    {
      "epoch": 4.695871642329187,
      "grad_norm": 3.4769842624664307,
      "learning_rate": 8.713522706619475e-06,
      "loss": 4.7314,
      "num_input_tokens_seen": 888300562,
      "step": 26048,
      "train_runtime": 57104.8679,
      "train_tokens_per_second": 15555.601
    },
    {
      "epoch": 4.701640526410673,
      "grad_norm": 3.2652599811553955,
      "learning_rate": 8.68856421954654e-06,
      "loss": 4.7229,
      "num_input_tokens_seen": 889410898,
      "step": 26080,
      "train_runtime": 57169.3353,
      "train_tokens_per_second": 15557.482
    },
    {
      "epoch": 4.707409410492158,
      "grad_norm": 3.2731878757476807,
      "learning_rate": 8.663614041954165e-06,
      "loss": 4.7281,
      "num_input_tokens_seen": 890508370,
      "step": 26112,
      "train_runtime": 57233.2358,
      "train_tokens_per_second": 15559.287
    },
    {
      "epoch": 4.713178294573644,
      "grad_norm": 4.246127128601074,
      "learning_rate": 8.63867233193092e-06,
      "loss": 4.7639,
      "num_input_tokens_seen": 891616626,
      "step": 26144,
      "train_runtime": 57297.5385,
      "train_tokens_per_second": 15561.168
    },
    {
      "epoch": 4.718947178655129,
      "grad_norm": 3.8438169956207275,
      "learning_rate": 8.613739247511732e-06,
      "loss": 4.7679,
      "num_input_tokens_seen": 892755570,
      "step": 26176,
      "train_runtime": 57363.3346,
      "train_tokens_per_second": 15563.174
    },
    {
      "epoch": 4.724716062736614,
      "grad_norm": 4.690268039703369,
      "learning_rate": 8.588814946676876e-06,
      "loss": 4.6729,
      "num_input_tokens_seen": 893842386,
      "step": 26208,
      "train_runtime": 57426.8331,
      "train_tokens_per_second": 15564.891
    },
    {
      "epoch": 4.7304849468181,
      "grad_norm": 3.903825521469116,
      "learning_rate": 8.563899587350962e-06,
      "loss": 4.7375,
      "num_input_tokens_seen": 894945330,
      "step": 26240,
      "train_runtime": 57490.9807,
      "train_tokens_per_second": 15566.708
    },
    {
      "epoch": 4.736253830899585,
      "grad_norm": 3.6207096576690674,
      "learning_rate": 8.53899332740196e-06,
      "loss": 4.6771,
      "num_input_tokens_seen": 896021362,
      "step": 26272,
      "train_runtime": 57553.9163,
      "train_tokens_per_second": 15568.382
    },
    {
      "epoch": 4.742022714981071,
      "grad_norm": 3.8605644702911377,
      "learning_rate": 8.514096324640166e-06,
      "loss": 4.7929,
      "num_input_tokens_seen": 897153650,
      "step": 26304,
      "train_runtime": 57619.5435,
      "train_tokens_per_second": 15570.301
    },
    {
      "epoch": 4.747791599062556,
      "grad_norm": 4.0212578773498535,
      "learning_rate": 8.48920873681724e-06,
      "loss": 4.6736,
      "num_input_tokens_seen": 898220658,
      "step": 26336,
      "train_runtime": 57681.9653,
      "train_tokens_per_second": 15571.95
    },
    {
      "epoch": 4.7535604831440414,
      "grad_norm": 3.7804677486419678,
      "learning_rate": 8.464330721625181e-06,
      "loss": 4.6751,
      "num_input_tokens_seen": 899323826,
      "step": 26368,
      "train_runtime": 57746.1508,
      "train_tokens_per_second": 15573.745
    },
    {
      "epoch": 4.7593293672255275,
      "grad_norm": 3.7662129402160645,
      "learning_rate": 8.43946243669532e-06,
      "loss": 4.7206,
      "num_input_tokens_seen": 900416498,
      "step": 26400,
      "train_runtime": 57809.9397,
      "train_tokens_per_second": 15575.462
    },
    {
      "epoch": 4.765098251307013,
      "grad_norm": 3.6008641719818115,
      "learning_rate": 8.41460403959736e-06,
      "loss": 4.6615,
      "num_input_tokens_seen": 901505586,
      "step": 26432,
      "train_runtime": 57873.4253,
      "train_tokens_per_second": 15577.194
    },
    {
      "epoch": 4.770867135388499,
      "grad_norm": 4.010145664215088,
      "learning_rate": 8.38975568783834e-06,
      "loss": 4.7648,
      "num_input_tokens_seen": 902619282,
      "step": 26464,
      "train_runtime": 57938.065,
      "train_tokens_per_second": 15579.037
    },
    {
      "epoch": 4.776636019469984,
      "grad_norm": 3.7274580001831055,
      "learning_rate": 8.36491753886164e-06,
      "loss": 4.7593,
      "num_input_tokens_seen": 903692530,
      "step": 26496,
      "train_runtime": 58000.8925,
      "train_tokens_per_second": 15580.666
    },
    {
      "epoch": 4.78240490355147,
      "grad_norm": 3.5668036937713623,
      "learning_rate": 8.340089750046013e-06,
      "loss": 4.7255,
      "num_input_tokens_seen": 904741170,
      "step": 26528,
      "train_runtime": 58062.4877,
      "train_tokens_per_second": 15582.198
    },
    {
      "epoch": 4.788173787632955,
      "grad_norm": 3.8083906173706055,
      "learning_rate": 8.31527247870456e-06,
      "loss": 4.7585,
      "num_input_tokens_seen": 905844498,
      "step": 26560,
      "train_runtime": 58126.6249,
      "train_tokens_per_second": 15583.986
    },
    {
      "epoch": 4.79394267171444,
      "grad_norm": 3.546593189239502,
      "learning_rate": 8.290465882083732e-06,
      "loss": 4.746,
      "num_input_tokens_seen": 906962450,
      "step": 26592,
      "train_runtime": 58191.567,
      "train_tokens_per_second": 15585.806
    },
    {
      "epoch": 4.799711555795926,
      "grad_norm": 3.6536977291107178,
      "learning_rate": 8.265670117362358e-06,
      "loss": 4.7774,
      "num_input_tokens_seen": 908057458,
      "step": 26624,
      "train_runtime": 58255.7066,
      "train_tokens_per_second": 15587.442
    },
    {
      "epoch": 4.805480439877411,
      "grad_norm": 3.2744195461273193,
      "learning_rate": 8.240885341650633e-06,
      "loss": 4.6946,
      "num_input_tokens_seen": 909134834,
      "step": 26656,
      "train_runtime": 58318.9191,
      "train_tokens_per_second": 15589.021
    },
    {
      "epoch": 4.811249323958897,
      "grad_norm": 3.3519506454467773,
      "learning_rate": 8.21611171198911e-06,
      "loss": 4.8006,
      "num_input_tokens_seen": 910238034,
      "step": 26688,
      "train_runtime": 58383.5033,
      "train_tokens_per_second": 15590.672
    },
    {
      "epoch": 4.817018208040382,
      "grad_norm": 3.6464717388153076,
      "learning_rate": 8.191349385347734e-06,
      "loss": 4.7045,
      "num_input_tokens_seen": 911277618,
      "step": 26720,
      "train_runtime": 58444.9853,
      "train_tokens_per_second": 15592.058
    },
    {
      "epoch": 4.822787092121867,
      "grad_norm": 4.400794506072998,
      "learning_rate": 8.166598518624818e-06,
      "loss": 4.6613,
      "num_input_tokens_seen": 912377970,
      "step": 26752,
      "train_runtime": 58508.9869,
      "train_tokens_per_second": 15593.809
    },
    {
      "epoch": 4.828555976203353,
      "grad_norm": 3.6051342487335205,
      "learning_rate": 8.141859268646078e-06,
      "loss": 4.7194,
      "num_input_tokens_seen": 913447794,
      "step": 26784,
      "train_runtime": 58571.5675,
      "train_tokens_per_second": 15595.413
    },
    {
      "epoch": 4.8343248602848385,
      "grad_norm": 3.9535977840423584,
      "learning_rate": 8.117131792163617e-06,
      "loss": 4.697,
      "num_input_tokens_seen": 914518898,
      "step": 26816,
      "train_runtime": 58634.2364,
      "train_tokens_per_second": 15597.012
    },
    {
      "epoch": 4.840093744366325,
      "grad_norm": 3.5686895847320557,
      "learning_rate": 8.092416245854934e-06,
      "loss": 4.6561,
      "num_input_tokens_seen": 915620274,
      "step": 26848,
      "train_runtime": 58698.3404,
      "train_tokens_per_second": 15598.742
    },
    {
      "epoch": 4.84586262844781,
      "grad_norm": 4.061814785003662,
      "learning_rate": 8.067712786321946e-06,
      "loss": 4.7148,
      "num_input_tokens_seen": 916677202,
      "step": 26880,
      "train_runtime": 58760.6645,
      "train_tokens_per_second": 15600.184
    },
    {
      "epoch": 4.851631512529295,
      "grad_norm": 3.5776476860046387,
      "learning_rate": 8.043021570089982e-06,
      "loss": 4.7172,
      "num_input_tokens_seen": 917801682,
      "step": 26912,
      "train_runtime": 58826.1088,
      "train_tokens_per_second": 15601.944
    },
    {
      "epoch": 4.857400396610781,
      "grad_norm": 3.7137670516967773,
      "learning_rate": 8.018342753606793e-06,
      "loss": 4.7142,
      "num_input_tokens_seen": 918865074,
      "step": 26944,
      "train_runtime": 58888.5677,
      "train_tokens_per_second": 15603.454
    },
    {
      "epoch": 4.863169280692266,
      "grad_norm": 3.690323829650879,
      "learning_rate": 7.993676493241565e-06,
      "loss": 4.7189,
      "num_input_tokens_seen": 919981106,
      "step": 26976,
      "train_runtime": 58953.7146,
      "train_tokens_per_second": 15605.142
    },
    {
      "epoch": 4.868938164773752,
      "grad_norm": 3.852079391479492,
      "learning_rate": 7.969022945283929e-06,
      "loss": 4.6809,
      "num_input_tokens_seen": 921080562,
      "step": 27008,
      "train_runtime": 59018.2958,
      "train_tokens_per_second": 15606.695
    },
    {
      "epoch": 4.874707048855237,
      "grad_norm": 3.5305864810943604,
      "learning_rate": 7.94438226594296e-06,
      "loss": 4.755,
      "num_input_tokens_seen": 922184402,
      "step": 27040,
      "train_runtime": 59082.89,
      "train_tokens_per_second": 15608.316
    },
    {
      "epoch": 4.880475932936722,
      "grad_norm": 3.9362668991088867,
      "learning_rate": 7.919754611346207e-06,
      "loss": 4.6954,
      "num_input_tokens_seen": 923269010,
      "step": 27072,
      "train_runtime": 59146.7028,
      "train_tokens_per_second": 15609.814
    },
    {
      "epoch": 4.886244817018208,
      "grad_norm": 3.400358200073242,
      "learning_rate": 7.895140137538684e-06,
      "loss": 4.7259,
      "num_input_tokens_seen": 924348146,
      "step": 27104,
      "train_runtime": 59210.3147,
      "train_tokens_per_second": 15611.269
    },
    {
      "epoch": 4.892013701099693,
      "grad_norm": 3.816216230392456,
      "learning_rate": 7.87053900048189e-06,
      "loss": 4.7132,
      "num_input_tokens_seen": 925451858,
      "step": 27136,
      "train_runtime": 59275.1009,
      "train_tokens_per_second": 15612.826
    },
    {
      "epoch": 4.897782585181179,
      "grad_norm": 3.7122859954833984,
      "learning_rate": 7.845951356052819e-06,
      "loss": 4.7316,
      "num_input_tokens_seen": 926559602,
      "step": 27168,
      "train_runtime": 59340.0939,
      "train_tokens_per_second": 15614.394
    },
    {
      "epoch": 4.9035514692626645,
      "grad_norm": 3.4983792304992676,
      "learning_rate": 7.821377360042982e-06,
      "loss": 4.7186,
      "num_input_tokens_seen": 927601298,
      "step": 27200,
      "train_runtime": 59401.9632,
      "train_tokens_per_second": 15615.667
    },
    {
      "epoch": 4.90932035334415,
      "grad_norm": 3.8072187900543213,
      "learning_rate": 7.796817168157398e-06,
      "loss": 4.7247,
      "num_input_tokens_seen": 928678738,
      "step": 27232,
      "train_runtime": 59465.4795,
      "train_tokens_per_second": 15617.107
    },
    {
      "epoch": 4.915089237425636,
      "grad_norm": 3.5677573680877686,
      "learning_rate": 7.772270936013633e-06,
      "loss": 4.684,
      "num_input_tokens_seen": 929800338,
      "step": 27264,
      "train_runtime": 59531.0901,
      "train_tokens_per_second": 15618.735
    },
    {
      "epoch": 4.920858121507121,
      "grad_norm": 3.5860633850097656,
      "learning_rate": 7.74773881914079e-06,
      "loss": 4.7151,
      "num_input_tokens_seen": 930836210,
      "step": 27296,
      "train_runtime": 59592.6114,
      "train_tokens_per_second": 15619.994
    },
    {
      "epoch": 4.926627005588607,
      "grad_norm": 3.4628777503967285,
      "learning_rate": 7.723220972978546e-06,
      "loss": 4.6122,
      "num_input_tokens_seen": 931879154,
      "step": 27328,
      "train_runtime": 59654.4951,
      "train_tokens_per_second": 15621.273
    },
    {
      "epoch": 4.932395889670092,
      "grad_norm": 3.4738283157348633,
      "learning_rate": 7.698717552876156e-06,
      "loss": 4.7177,
      "num_input_tokens_seen": 932970450,
      "step": 27360,
      "train_runtime": 59718.6313,
      "train_tokens_per_second": 15622.77
    },
    {
      "epoch": 4.938164773751577,
      "grad_norm": 3.561321973800659,
      "learning_rate": 7.674228714091452e-06,
      "loss": 4.7026,
      "num_input_tokens_seen": 934066610,
      "step": 27392,
      "train_runtime": 59782.9318,
      "train_tokens_per_second": 15624.302
    },
    {
      "epoch": 4.943933657833063,
      "grad_norm": 3.660968542098999,
      "learning_rate": 7.649754611789899e-06,
      "loss": 4.7549,
      "num_input_tokens_seen": 935184370,
      "step": 27424,
      "train_runtime": 59847.8976,
      "train_tokens_per_second": 15626.019
    },
    {
      "epoch": 4.949702541914548,
      "grad_norm": 3.859855890274048,
      "learning_rate": 7.625295401043581e-06,
      "loss": 4.7431,
      "num_input_tokens_seen": 936318866,
      "step": 27456,
      "train_runtime": 59913.528,
      "train_tokens_per_second": 15627.837
    },
    {
      "epoch": 4.955471425996034,
      "grad_norm": 3.5302205085754395,
      "learning_rate": 7.600851236830214e-06,
      "loss": 4.66,
      "num_input_tokens_seen": 937464850,
      "step": 27488,
      "train_runtime": 59979.7924,
      "train_tokens_per_second": 15629.678
    },
    {
      "epoch": 4.961240310077519,
      "grad_norm": 3.3141708374023438,
      "learning_rate": 7.576422274032198e-06,
      "loss": 4.6652,
      "num_input_tokens_seen": 938532082,
      "step": 27520,
      "train_runtime": 60042.5827,
      "train_tokens_per_second": 15631.108
    },
    {
      "epoch": 4.967009194159004,
      "grad_norm": 3.556471109390259,
      "learning_rate": 7.552008667435604e-06,
      "loss": 4.702,
      "num_input_tokens_seen": 939649042,
      "step": 27552,
      "train_runtime": 60107.8291,
      "train_tokens_per_second": 15632.723
    },
    {
      "epoch": 4.9727780782404905,
      "grad_norm": 3.8089423179626465,
      "learning_rate": 7.527610571729191e-06,
      "loss": 4.7217,
      "num_input_tokens_seen": 940731058,
      "step": 27584,
      "train_runtime": 60171.4821,
      "train_tokens_per_second": 15634.168
    },
    {
      "epoch": 4.978546962321976,
      "grad_norm": 3.842869520187378,
      "learning_rate": 7.503228141503463e-06,
      "loss": 4.6705,
      "num_input_tokens_seen": 941848562,
      "step": 27616,
      "train_runtime": 60236.6314,
      "train_tokens_per_second": 15635.811
    },
    {
      "epoch": 4.984315846403462,
      "grad_norm": 3.6253292560577393,
      "learning_rate": 7.47886153124965e-06,
      "loss": 4.7169,
      "num_input_tokens_seen": 942906994,
      "step": 27648,
      "train_runtime": 60298.6239,
      "train_tokens_per_second": 15637.289
    },
    {
      "epoch": 4.990084730484947,
      "grad_norm": 5.146467208862305,
      "learning_rate": 7.454510895358742e-06,
      "loss": 4.7602,
      "num_input_tokens_seen": 944020498,
      "step": 27680,
      "train_runtime": 60363.5637,
      "train_tokens_per_second": 15638.913
    },
    {
      "epoch": 4.995853614566432,
      "grad_norm": 4.035292148590088,
      "learning_rate": 7.43017638812052e-06,
      "loss": 4.7286,
      "num_input_tokens_seen": 945109970,
      "step": 27712,
      "train_runtime": 60427.5883,
      "train_tokens_per_second": 15640.372
    },
    {
      "epoch": 5.0,
      "eval_accuracy": 0.019249635057343572,
      "eval_class_accuracy": 0.4737767637779475,
      "eval_exact_token_accuracy": 0.0,
      "eval_f1": 0.03772951031741137,
      "eval_loss": 2.6808197498321533,
      "eval_mean_iou": 0.1565496708583554,
      "eval_mean_iou_25": 0.1565496708583554,
      "eval_precision": 0.03939740498506356,
      "eval_prediction_target_similarity": 6.085344502837499e-05,
      "eval_recall": 0.0362003786518331,
      "eval_recall_25": 0.08822695902958272,
      "eval_runtime": 1273.9513,
      "eval_samples_per_second": 3.887,
      "eval_sequence_accuracy": 0.0,
      "eval_steps_per_second": 0.122,
      "eval_top3_token_accuracy": 0.044882189482450485,
      "eval_top5_token_accuracy": 0.0883849635720253,
      "num_input_tokens_seen": 945907470,
      "step": 27735
    }
  ],
  "logging_steps": 32,
  "max_steps": 44376,
  "num_input_tokens_seen": 945907470,
  "num_train_epochs": 8,
  "save_steps": 5547,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 2.6757411852713776e+17,
  "train_batch_size": 32,
  "trial_name": null,
  "trial_params": null
}
