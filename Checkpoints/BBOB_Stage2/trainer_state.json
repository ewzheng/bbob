{
  "best_global_step": 13284,
  "best_metric": 2.3506674766540527,
  "best_model_checkpoint": "Output/vision_2025-07-22_05-03-18/checkpoint-13284",
  "epoch": 4.0,
  "eval_steps": 2214,
  "global_step": 17712,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.007227146970809102,
      "grad_norm": 59.8697624206543,
      "learning_rate": 8.747178329571107e-07,
      "loss": 16.965,
      "num_input_tokens_seen": 105824,
      "step": 32,
      "train_runtime": 9.3935,
      "train_tokens_per_second": 11265.657
    },
    {
      "epoch": 0.014454293941618204,
      "grad_norm": 27.78458023071289,
      "learning_rate": 1.7776523702031604e-06,
      "loss": 14.6407,
      "num_input_tokens_seen": 212224,
      "step": 64,
      "train_runtime": 17.4425,
      "train_tokens_per_second": 12167.038
    },
    {
      "epoch": 0.021681440912427304,
      "grad_norm": 23.863109588623047,
      "learning_rate": 2.6805869074492102e-06,
      "loss": 14.1237,
      "num_input_tokens_seen": 322848,
      "step": 96,
      "train_runtime": 25.4789,
      "train_tokens_per_second": 12671.184
    },
    {
      "epoch": 0.028908587883236407,
      "grad_norm": 22.040307998657227,
      "learning_rate": 3.58352144469526e-06,
      "loss": 13.4853,
      "num_input_tokens_seen": 433760,
      "step": 128,
      "train_runtime": 33.5138,
      "train_tokens_per_second": 12942.724
    },
    {
      "epoch": 0.03613573485404551,
      "grad_norm": 22.24885368347168,
      "learning_rate": 4.486455981941309e-06,
      "loss": 13.1743,
      "num_input_tokens_seen": 542112,
      "step": 160,
      "train_runtime": 41.5329,
      "train_tokens_per_second": 13052.584
    },
    {
      "epoch": 0.04336288182485461,
      "grad_norm": 20.440229415893555,
      "learning_rate": 5.389390519187359e-06,
      "loss": 13.179,
      "num_input_tokens_seen": 652832,
      "step": 192,
      "train_runtime": 49.5688,
      "train_tokens_per_second": 13170.215
    },
    {
      "epoch": 0.05059002879566371,
      "grad_norm": 19.053058624267578,
      "learning_rate": 6.292325056433408e-06,
      "loss": 12.8342,
      "num_input_tokens_seen": 762752,
      "step": 224,
      "train_runtime": 57.5699,
      "train_tokens_per_second": 13249.139
    },
    {
      "epoch": 0.057817175766472814,
      "grad_norm": 17.999576568603516,
      "learning_rate": 7.195259593679458e-06,
      "loss": 12.8301,
      "num_input_tokens_seen": 870784,
      "step": 256,
      "train_runtime": 65.5402,
      "train_tokens_per_second": 13286.253
    },
    {
      "epoch": 0.06504432273728192,
      "grad_norm": 20.699670791625977,
      "learning_rate": 8.098194130925508e-06,
      "loss": 12.3899,
      "num_input_tokens_seen": 979328,
      "step": 288,
      "train_runtime": 73.5127,
      "train_tokens_per_second": 13321.89
    },
    {
      "epoch": 0.07227146970809102,
      "grad_norm": 20.374753952026367,
      "learning_rate": 9.001128668171557e-06,
      "loss": 12.3445,
      "num_input_tokens_seen": 1085504,
      "step": 320,
      "train_runtime": 81.4724,
      "train_tokens_per_second": 13323.579
    },
    {
      "epoch": 0.07949861667890012,
      "grad_norm": 18.450851440429688,
      "learning_rate": 9.904063205417607e-06,
      "loss": 12.2319,
      "num_input_tokens_seen": 1194400,
      "step": 352,
      "train_runtime": 89.4429,
      "train_tokens_per_second": 13353.768
    },
    {
      "epoch": 0.08672576364970921,
      "grad_norm": 18.804311752319336,
      "learning_rate": 1.0806997742663657e-05,
      "loss": 12.1553,
      "num_input_tokens_seen": 1303008,
      "step": 384,
      "train_runtime": 97.4261,
      "train_tokens_per_second": 13374.325
    },
    {
      "epoch": 0.09395291062051832,
      "grad_norm": 17.66101837158203,
      "learning_rate": 1.1709932279909709e-05,
      "loss": 11.9142,
      "num_input_tokens_seen": 1411872,
      "step": 416,
      "train_runtime": 105.4056,
      "train_tokens_per_second": 13394.654
    },
    {
      "epoch": 0.10118005759132742,
      "grad_norm": 17.95196533203125,
      "learning_rate": 1.2612866817155759e-05,
      "loss": 11.9074,
      "num_input_tokens_seen": 1524960,
      "step": 448,
      "train_runtime": 113.4696,
      "train_tokens_per_second": 13439.369
    },
    {
      "epoch": 0.10840720456213652,
      "grad_norm": 19.076805114746094,
      "learning_rate": 1.3515801354401805e-05,
      "loss": 11.7248,
      "num_input_tokens_seen": 1635008,
      "step": 480,
      "train_runtime": 121.4497,
      "train_tokens_per_second": 13462.425
    },
    {
      "epoch": 0.11563435153294563,
      "grad_norm": 16.79903793334961,
      "learning_rate": 1.4418735891647855e-05,
      "loss": 11.6887,
      "num_input_tokens_seen": 1746272,
      "step": 512,
      "train_runtime": 129.4338,
      "train_tokens_per_second": 13491.619
    },
    {
      "epoch": 0.12286149850375473,
      "grad_norm": 16.81481170654297,
      "learning_rate": 1.5321670428893907e-05,
      "loss": 11.4981,
      "num_input_tokens_seen": 1855552,
      "step": 544,
      "train_runtime": 137.4063,
      "train_tokens_per_second": 13504.128
    },
    {
      "epoch": 0.13008864547456384,
      "grad_norm": 16.132360458374023,
      "learning_rate": 1.6224604966139957e-05,
      "loss": 11.5729,
      "num_input_tokens_seen": 1965280,
      "step": 576,
      "train_runtime": 145.39,
      "train_tokens_per_second": 13517.298
    },
    {
      "epoch": 0.13731579244537293,
      "grad_norm": 16.645183563232422,
      "learning_rate": 1.7127539503386007e-05,
      "loss": 11.4485,
      "num_input_tokens_seen": 2075168,
      "step": 608,
      "train_runtime": 153.416,
      "train_tokens_per_second": 13526.409
    },
    {
      "epoch": 0.14454293941618204,
      "grad_norm": 16.20722007751465,
      "learning_rate": 1.8030474040632057e-05,
      "loss": 11.4539,
      "num_input_tokens_seen": 2185152,
      "step": 640,
      "train_runtime": 161.417,
      "train_tokens_per_second": 13537.31
    },
    {
      "epoch": 0.15177008638699113,
      "grad_norm": 17.709686279296875,
      "learning_rate": 1.8933408577878104e-05,
      "loss": 11.4085,
      "num_input_tokens_seen": 2294016,
      "step": 672,
      "train_runtime": 169.411,
      "train_tokens_per_second": 13541.13
    },
    {
      "epoch": 0.15899723335780025,
      "grad_norm": 16.941131591796875,
      "learning_rate": 1.9836343115124154e-05,
      "loss": 11.4446,
      "num_input_tokens_seen": 2402048,
      "step": 704,
      "train_runtime": 177.4,
      "train_tokens_per_second": 13540.295
    },
    {
      "epoch": 0.16622438032860934,
      "grad_norm": 16.148454666137695,
      "learning_rate": 2.0739277652370204e-05,
      "loss": 11.1276,
      "num_input_tokens_seen": 2509312,
      "step": 736,
      "train_runtime": 185.3694,
      "train_tokens_per_second": 13536.819
    },
    {
      "epoch": 0.17345152729941843,
      "grad_norm": 16.967317581176758,
      "learning_rate": 2.1642212189616254e-05,
      "loss": 11.298,
      "num_input_tokens_seen": 2620992,
      "step": 768,
      "train_runtime": 193.3654,
      "train_tokens_per_second": 13554.605
    },
    {
      "epoch": 0.18067867427022755,
      "grad_norm": 15.280611991882324,
      "learning_rate": 2.2545146726862304e-05,
      "loss": 11.0585,
      "num_input_tokens_seen": 2730400,
      "step": 800,
      "train_runtime": 201.3531,
      "train_tokens_per_second": 13560.258
    },
    {
      "epoch": 0.18790582124103664,
      "grad_norm": 15.218660354614258,
      "learning_rate": 2.3448081264108354e-05,
      "loss": 11.1029,
      "num_input_tokens_seen": 2838336,
      "step": 832,
      "train_runtime": 209.374,
      "train_tokens_per_second": 13556.299
    },
    {
      "epoch": 0.19513296821184575,
      "grad_norm": 15.90068531036377,
      "learning_rate": 2.4351015801354404e-05,
      "loss": 10.9171,
      "num_input_tokens_seen": 2947136,
      "step": 864,
      "train_runtime": 217.3668,
      "train_tokens_per_second": 13558.356
    },
    {
      "epoch": 0.20236011518265484,
      "grad_norm": 15.729728698730469,
      "learning_rate": 2.525395033860045e-05,
      "loss": 10.8746,
      "num_input_tokens_seen": 3053248,
      "step": 896,
      "train_runtime": 225.3279,
      "train_tokens_per_second": 13550.245
    },
    {
      "epoch": 0.20958726215346396,
      "grad_norm": 15.646979331970215,
      "learning_rate": 2.6156884875846504e-05,
      "loss": 10.8686,
      "num_input_tokens_seen": 3162496,
      "step": 928,
      "train_runtime": 233.3118,
      "train_tokens_per_second": 13554.805
    },
    {
      "epoch": 0.21681440912427305,
      "grad_norm": 14.64767074584961,
      "learning_rate": 2.705981941309255e-05,
      "loss": 10.8963,
      "num_input_tokens_seen": 3271136,
      "step": 960,
      "train_runtime": 241.3023,
      "train_tokens_per_second": 13556.174
    },
    {
      "epoch": 0.22404155609508214,
      "grad_norm": 15.000544548034668,
      "learning_rate": 2.79627539503386e-05,
      "loss": 10.8222,
      "num_input_tokens_seen": 3380800,
      "step": 992,
      "train_runtime": 249.2789,
      "train_tokens_per_second": 13562.32
    },
    {
      "epoch": 0.23126870306589126,
      "grad_norm": 15.643575668334961,
      "learning_rate": 2.886568848758465e-05,
      "loss": 10.8137,
      "num_input_tokens_seen": 3489312,
      "step": 1024,
      "train_runtime": 257.253,
      "train_tokens_per_second": 13563.738
    },
    {
      "epoch": 0.23849585003670035,
      "grad_norm": 15.664891242980957,
      "learning_rate": 2.97686230248307e-05,
      "loss": 10.8589,
      "num_input_tokens_seen": 3599456,
      "step": 1056,
      "train_runtime": 265.2538,
      "train_tokens_per_second": 13569.856
    },
    {
      "epoch": 0.24572299700750946,
      "grad_norm": 15.207317352294922,
      "learning_rate": 3.0671557562076754e-05,
      "loss": 10.8476,
      "num_input_tokens_seen": 3711168,
      "step": 1088,
      "train_runtime": 273.2596,
      "train_tokens_per_second": 13581.107
    },
    {
      "epoch": 0.25295014397831855,
      "grad_norm": 13.771032333374023,
      "learning_rate": 3.15744920993228e-05,
      "loss": 10.7337,
      "num_input_tokens_seen": 3816832,
      "step": 1120,
      "train_runtime": 281.2226,
      "train_tokens_per_second": 13572.279
    },
    {
      "epoch": 0.26017729094912767,
      "grad_norm": 14.129536628723145,
      "learning_rate": 3.2477426636568854e-05,
      "loss": 10.6353,
      "num_input_tokens_seen": 3923904,
      "step": 1152,
      "train_runtime": 289.1962,
      "train_tokens_per_second": 13568.31
    },
    {
      "epoch": 0.2674044379199368,
      "grad_norm": 14.964567184448242,
      "learning_rate": 3.33803611738149e-05,
      "loss": 10.6548,
      "num_input_tokens_seen": 4033440,
      "step": 1184,
      "train_runtime": 297.1746,
      "train_tokens_per_second": 13572.629
    },
    {
      "epoch": 0.27463158489074585,
      "grad_norm": 15.197214126586914,
      "learning_rate": 3.428329571106095e-05,
      "loss": 10.6839,
      "num_input_tokens_seen": 4141216,
      "step": 1216,
      "train_runtime": 305.1565,
      "train_tokens_per_second": 13570.792
    },
    {
      "epoch": 0.28185873186155497,
      "grad_norm": 14.07080078125,
      "learning_rate": 3.5186230248307e-05,
      "loss": 10.5923,
      "num_input_tokens_seen": 4250016,
      "step": 1248,
      "train_runtime": 313.1322,
      "train_tokens_per_second": 13572.595
    },
    {
      "epoch": 0.2890858788323641,
      "grad_norm": 14.71224308013916,
      "learning_rate": 3.608916478555305e-05,
      "loss": 10.6496,
      "num_input_tokens_seen": 4358272,
      "step": 1280,
      "train_runtime": 321.1097,
      "train_tokens_per_second": 13572.534
    },
    {
      "epoch": 0.29631302580317315,
      "grad_norm": 13.741596221923828,
      "learning_rate": 3.69920993227991e-05,
      "loss": 10.5767,
      "num_input_tokens_seen": 4465248,
      "step": 1312,
      "train_runtime": 329.0889,
      "train_tokens_per_second": 13568.517
    },
    {
      "epoch": 0.30354017277398226,
      "grad_norm": 14.669835090637207,
      "learning_rate": 3.789503386004515e-05,
      "loss": 10.6141,
      "num_input_tokens_seen": 4572448,
      "step": 1344,
      "train_runtime": 337.0681,
      "train_tokens_per_second": 13565.355
    },
    {
      "epoch": 0.3107673197447914,
      "grad_norm": 13.834663391113281,
      "learning_rate": 3.87979683972912e-05,
      "loss": 10.4931,
      "num_input_tokens_seen": 4682432,
      "step": 1376,
      "train_runtime": 345.0559,
      "train_tokens_per_second": 13570.067
    },
    {
      "epoch": 0.3179944667156005,
      "grad_norm": 14.24205493927002,
      "learning_rate": 3.970090293453725e-05,
      "loss": 10.6671,
      "num_input_tokens_seen": 4788992,
      "step": 1408,
      "train_runtime": 353.0248,
      "train_tokens_per_second": 13565.595
    },
    {
      "epoch": 0.32522161368640956,
      "grad_norm": 13.280802726745605,
      "learning_rate": 4.06038374717833e-05,
      "loss": 10.5987,
      "num_input_tokens_seen": 4896576,
      "step": 1440,
      "train_runtime": 361.0218,
      "train_tokens_per_second": 13563.105
    },
    {
      "epoch": 0.3324487606572187,
      "grad_norm": 13.319524765014648,
      "learning_rate": 4.150677200902935e-05,
      "loss": 10.4904,
      "num_input_tokens_seen": 5004000,
      "step": 1472,
      "train_runtime": 368.9655,
      "train_tokens_per_second": 13562.243
    },
    {
      "epoch": 0.3396759076280278,
      "grad_norm": 13.056076049804688,
      "learning_rate": 4.24097065462754e-05,
      "loss": 10.5151,
      "num_input_tokens_seen": 5115424,
      "step": 1504,
      "train_runtime": 376.9496,
      "train_tokens_per_second": 13570.576
    },
    {
      "epoch": 0.34690305459883686,
      "grad_norm": 13.138190269470215,
      "learning_rate": 4.331264108352145e-05,
      "loss": 10.4529,
      "num_input_tokens_seen": 5223648,
      "step": 1536,
      "train_runtime": 384.9082,
      "train_tokens_per_second": 13571.152
    },
    {
      "epoch": 0.354130201569646,
      "grad_norm": 13.51944351196289,
      "learning_rate": 4.42155756207675e-05,
      "loss": 10.394,
      "num_input_tokens_seen": 5329984,
      "step": 1568,
      "train_runtime": 392.8634,
      "train_tokens_per_second": 13567.015
    },
    {
      "epoch": 0.3613573485404551,
      "grad_norm": 13.023834228515625,
      "learning_rate": 4.511851015801354e-05,
      "loss": 10.3839,
      "num_input_tokens_seen": 5438912,
      "step": 1600,
      "train_runtime": 400.8158,
      "train_tokens_per_second": 13569.603
    },
    {
      "epoch": 0.3685844955112642,
      "grad_norm": 13.269426345825195,
      "learning_rate": 4.60214446952596e-05,
      "loss": 10.3309,
      "num_input_tokens_seen": 5547744,
      "step": 1632,
      "train_runtime": 408.7718,
      "train_tokens_per_second": 13571.739
    },
    {
      "epoch": 0.37581164248207327,
      "grad_norm": 14.431382179260254,
      "learning_rate": 4.692437923250564e-05,
      "loss": 10.4014,
      "num_input_tokens_seen": 5655104,
      "step": 1664,
      "train_runtime": 416.7271,
      "train_tokens_per_second": 13570.283
    },
    {
      "epoch": 0.3830387894528824,
      "grad_norm": 12.982739448547363,
      "learning_rate": 4.78273137697517e-05,
      "loss": 10.3439,
      "num_input_tokens_seen": 5759968,
      "step": 1696,
      "train_runtime": 424.6788,
      "train_tokens_per_second": 13563.115
    },
    {
      "epoch": 0.3902659364236915,
      "grad_norm": 13.308042526245117,
      "learning_rate": 4.873024830699774e-05,
      "loss": 10.2379,
      "num_input_tokens_seen": 5867712,
      "step": 1728,
      "train_runtime": 432.6893,
      "train_tokens_per_second": 13561.028
    },
    {
      "epoch": 0.39749308339450057,
      "grad_norm": 12.890275001525879,
      "learning_rate": 4.96331828442438e-05,
      "loss": 10.2711,
      "num_input_tokens_seen": 5979104,
      "step": 1760,
      "train_runtime": 440.6905,
      "train_tokens_per_second": 13567.582
    },
    {
      "epoch": 0.4047202303653097,
      "grad_norm": 12.340851783752441,
      "learning_rate": 4.999982471699151e-05,
      "loss": 10.3846,
      "num_input_tokens_seen": 6088704,
      "step": 1792,
      "train_runtime": 448.7208,
      "train_tokens_per_second": 13569.024
    },
    {
      "epoch": 0.4119473773361188,
      "grad_norm": 12.810661315917969,
      "learning_rate": 4.999873709750874e-05,
      "loss": 10.3007,
      "num_input_tokens_seen": 6197024,
      "step": 1824,
      "train_runtime": 456.6891,
      "train_tokens_per_second": 13569.459
    },
    {
      "epoch": 0.4191745243069279,
      "grad_norm": 12.769746780395508,
      "learning_rate": 4.99966551270654e-05,
      "loss": 10.4047,
      "num_input_tokens_seen": 6304032,
      "step": 1856,
      "train_runtime": 464.645,
      "train_tokens_per_second": 13567.417
    },
    {
      "epoch": 0.426401671277737,
      "grad_norm": 13.461515426635742,
      "learning_rate": 4.9993578888474036e-05,
      "loss": 10.3951,
      "num_input_tokens_seen": 6411840,
      "step": 1888,
      "train_runtime": 472.6146,
      "train_tokens_per_second": 13566.741
    },
    {
      "epoch": 0.4336288182485461,
      "grad_norm": 13.342884063720703,
      "learning_rate": 4.9989508504095275e-05,
      "loss": 10.3406,
      "num_input_tokens_seen": 6519424,
      "step": 1920,
      "train_runtime": 480.6787,
      "train_tokens_per_second": 13562.957
    },
    {
      "epoch": 0.4408559652193552,
      "grad_norm": 12.600870132446289,
      "learning_rate": 4.9984444135832916e-05,
      "loss": 10.2248,
      "num_input_tokens_seen": 6625632,
      "step": 1952,
      "train_runtime": 488.647,
      "train_tokens_per_second": 13559.137
    },
    {
      "epoch": 0.4480831121901643,
      "grad_norm": 11.945220947265625,
      "learning_rate": 4.9978385985127515e-05,
      "loss": 10.123,
      "num_input_tokens_seen": 6731456,
      "step": 1984,
      "train_runtime": 496.6239,
      "train_tokens_per_second": 13554.434
    },
    {
      "epoch": 0.4553102591609734,
      "grad_norm": 12.51501750946045,
      "learning_rate": 4.997133429294836e-05,
      "loss": 10.087,
      "num_input_tokens_seen": 6837376,
      "step": 2016,
      "train_runtime": 504.6504,
      "train_tokens_per_second": 13548.739
    },
    {
      "epoch": 0.4625374061317825,
      "grad_norm": 12.051273345947266,
      "learning_rate": 4.996328933978389e-05,
      "loss": 10.3561,
      "num_input_tokens_seen": 6945472,
      "step": 2048,
      "train_runtime": 512.7653,
      "train_tokens_per_second": 13545.129
    },
    {
      "epoch": 0.46976455310259163,
      "grad_norm": 13.069073677062988,
      "learning_rate": 4.995425144563056e-05,
      "loss": 10.0505,
      "num_input_tokens_seen": 7053696,
      "step": 2080,
      "train_runtime": 520.7558,
      "train_tokens_per_second": 13545.112
    },
    {
      "epoch": 0.4769917000734007,
      "grad_norm": 12.198127746582031,
      "learning_rate": 4.9944220969980074e-05,
      "loss": 10.3118,
      "num_input_tokens_seen": 7163296,
      "step": 2112,
      "train_runtime": 528.7487,
      "train_tokens_per_second": 13547.638
    },
    {
      "epoch": 0.4842188470442098,
      "grad_norm": 12.36948299407959,
      "learning_rate": 4.9933198311805106e-05,
      "loss": 10.1006,
      "num_input_tokens_seen": 7271840,
      "step": 2144,
      "train_runtime": 536.816,
      "train_tokens_per_second": 13546.243
    },
    {
      "epoch": 0.4914459940150189,
      "grad_norm": 12.736051559448242,
      "learning_rate": 4.992118390954344e-05,
      "loss": 10.2015,
      "num_input_tokens_seen": 7381888,
      "step": 2176,
      "train_runtime": 544.8163,
      "train_tokens_per_second": 13549.315
    },
    {
      "epoch": 0.498673140985828,
      "grad_norm": 12.186558723449707,
      "learning_rate": 4.990817824108051e-05,
      "loss": 10.2996,
      "num_input_tokens_seen": 7488992,
      "step": 2208,
      "train_runtime": 552.7955,
      "train_tokens_per_second": 13547.49
    },
    {
      "epoch": 0.5000282310428548,
      "eval_exact_token_accuracy": 0.009285968728363514,
      "eval_loss": 2.556161403656006,
      "eval_prediction_target_similarity": 0.021322657402046658,
      "eval_runtime": 38.2533,
      "eval_samples_per_second": 653.8,
      "eval_sequence_accuracy": 0.0,
      "eval_steps_per_second": 20.443,
      "eval_top3_token_accuracy": 0.03734498471021652,
      "eval_top5_token_accuracy": 0.06000255420804024,
      "num_input_tokens_seen": 7509216,
      "step": 2214
    },
    {
      "epoch": 0.5059002879566371,
      "grad_norm": 11.626673698425293,
      "learning_rate": 4.989418182373041e-05,
      "loss": 10.2068,
      "num_input_tokens_seen": 7597280,
      "step": 2240,
      "train_runtime": 599.9296,
      "train_tokens_per_second": 12663.618
    },
    {
      "epoch": 0.5131274349274462,
      "grad_norm": 12.460028648376465,
      "learning_rate": 4.987919521421529e-05,
      "loss": 10.1157,
      "num_input_tokens_seen": 7705696,
      "step": 2272,
      "train_runtime": 607.9047,
      "train_tokens_per_second": 12675.829
    },
    {
      "epoch": 0.5203545818982553,
      "grad_norm": 11.419310569763184,
      "learning_rate": 4.986321900864327e-05,
      "loss": 10.0287,
      "num_input_tokens_seen": 7815200,
      "step": 2304,
      "train_runtime": 615.8803,
      "train_tokens_per_second": 12689.479
    },
    {
      "epoch": 0.5275817288690644,
      "grad_norm": 13.07050609588623,
      "learning_rate": 4.984625384248465e-05,
      "loss": 10.1599,
      "num_input_tokens_seen": 7925888,
      "step": 2336,
      "train_runtime": 623.8636,
      "train_tokens_per_second": 12704.522
    },
    {
      "epoch": 0.5348088758398736,
      "grad_norm": 12.229494094848633,
      "learning_rate": 4.982830039054669e-05,
      "loss": 10.0795,
      "num_input_tokens_seen": 8037152,
      "step": 2368,
      "train_runtime": 631.839,
      "train_tokens_per_second": 12720.253
    },
    {
      "epoch": 0.5420360228106826,
      "grad_norm": 12.331194877624512,
      "learning_rate": 4.9809359366946754e-05,
      "loss": 10.1858,
      "num_input_tokens_seen": 8146656,
      "step": 2400,
      "train_runtime": 639.8061,
      "train_tokens_per_second": 12733.008
    },
    {
      "epoch": 0.5492631697814917,
      "grad_norm": 11.371849060058594,
      "learning_rate": 4.978943152508391e-05,
      "loss": 10.1133,
      "num_input_tokens_seen": 8259200,
      "step": 2432,
      "train_runtime": 647.7862,
      "train_tokens_per_second": 12749.886
    },
    {
      "epoch": 0.5564903167523009,
      "grad_norm": 12.187331199645996,
      "learning_rate": 4.9768517657608926e-05,
      "loss": 10.1013,
      "num_input_tokens_seen": 8369056,
      "step": 2464,
      "train_runtime": 655.7606,
      "train_tokens_per_second": 12762.365
    },
    {
      "epoch": 0.5637174637231099,
      "grad_norm": 10.919928550720215,
      "learning_rate": 4.974661859639281e-05,
      "loss": 10.0907,
      "num_input_tokens_seen": 8476480,
      "step": 2496,
      "train_runtime": 663.714,
      "train_tokens_per_second": 12771.283
    },
    {
      "epoch": 0.570944610693919,
      "grad_norm": 11.471762657165527,
      "learning_rate": 4.9723735212493674e-05,
      "loss": 10.1578,
      "num_input_tokens_seen": 8585632,
      "step": 2528,
      "train_runtime": 671.682,
      "train_tokens_per_second": 12782.287
    },
    {
      "epoch": 0.5781717576647282,
      "grad_norm": 12.021941184997559,
      "learning_rate": 4.9699868416122067e-05,
      "loss": 10.0394,
      "num_input_tokens_seen": 8693440,
      "step": 2560,
      "train_runtime": 679.6307,
      "train_tokens_per_second": 12791.417
    },
    {
      "epoch": 0.5853989046355372,
      "grad_norm": 11.250447273254395,
      "learning_rate": 4.967501915660483e-05,
      "loss": 10.0757,
      "num_input_tokens_seen": 8803552,
      "step": 2592,
      "train_runtime": 687.6021,
      "train_tokens_per_second": 12803.264
    },
    {
      "epoch": 0.5926260516063463,
      "grad_norm": 11.439910888671875,
      "learning_rate": 4.96491884223473e-05,
      "loss": 10.1111,
      "num_input_tokens_seen": 8910912,
      "step": 2624,
      "train_runtime": 695.5907,
      "train_tokens_per_second": 12810.568
    },
    {
      "epoch": 0.5998531985771555,
      "grad_norm": 11.502098083496094,
      "learning_rate": 4.9622377240793996e-05,
      "loss": 9.9074,
      "num_input_tokens_seen": 9020576,
      "step": 2656,
      "train_runtime": 703.5996,
      "train_tokens_per_second": 12820.61
    },
    {
      "epoch": 0.6070803455479645,
      "grad_norm": 11.307975769042969,
      "learning_rate": 4.959458667838776e-05,
      "loss": 9.8618,
      "num_input_tokens_seen": 9129440,
      "step": 2688,
      "train_runtime": 711.5781,
      "train_tokens_per_second": 12829.85
    },
    {
      "epoch": 0.6143074925187736,
      "grad_norm": 11.914899826049805,
      "learning_rate": 4.956581784052732e-05,
      "loss": 10.1175,
      "num_input_tokens_seen": 9242656,
      "step": 2720,
      "train_runtime": 719.5826,
      "train_tokens_per_second": 12844.468
    },
    {
      "epoch": 0.6215346394895828,
      "grad_norm": 11.127532005310059,
      "learning_rate": 4.953607187152335e-05,
      "loss": 10.0798,
      "num_input_tokens_seen": 9351360,
      "step": 2752,
      "train_runtime": 727.5511,
      "train_tokens_per_second": 12853.201
    },
    {
      "epoch": 0.6287617864603918,
      "grad_norm": 11.908459663391113,
      "learning_rate": 4.950534995455293e-05,
      "loss": 9.9064,
      "num_input_tokens_seen": 9462080,
      "step": 2784,
      "train_runtime": 735.532,
      "train_tokens_per_second": 12864.267
    },
    {
      "epoch": 0.635988933431201,
      "grad_norm": 11.718379974365234,
      "learning_rate": 4.9473653311612496e-05,
      "loss": 9.875,
      "num_input_tokens_seen": 9568256,
      "step": 2816,
      "train_runtime": 743.5378,
      "train_tokens_per_second": 12868.554
    },
    {
      "epoch": 0.6432160804020101,
      "grad_norm": 11.494741439819336,
      "learning_rate": 4.944098320346924e-05,
      "loss": 9.9342,
      "num_input_tokens_seen": 9675104,
      "step": 2848,
      "train_runtime": 751.5332,
      "train_tokens_per_second": 12873.821
    },
    {
      "epoch": 0.6504432273728191,
      "grad_norm": 11.059250831604004,
      "learning_rate": 4.940734092961092e-05,
      "loss": 9.8817,
      "num_input_tokens_seen": 9781952,
      "step": 2880,
      "train_runtime": 759.488,
      "train_tokens_per_second": 12879.667
    },
    {
      "epoch": 0.6576703743436283,
      "grad_norm": 11.843018531799316,
      "learning_rate": 4.937272782819427e-05,
      "loss": 9.9112,
      "num_input_tokens_seen": 9885216,
      "step": 2912,
      "train_runtime": 767.498,
      "train_tokens_per_second": 12879.793
    },
    {
      "epoch": 0.6648975213144374,
      "grad_norm": 11.523880004882812,
      "learning_rate": 4.933714527599163e-05,
      "loss": 9.907,
      "num_input_tokens_seen": 9995392,
      "step": 2944,
      "train_runtime": 775.4864,
      "train_tokens_per_second": 12889.19
    },
    {
      "epoch": 0.6721246682852464,
      "grad_norm": 10.663911819458008,
      "learning_rate": 4.93005946883363e-05,
      "loss": 9.8955,
      "num_input_tokens_seen": 10105312,
      "step": 2976,
      "train_runtime": 783.5145,
      "train_tokens_per_second": 12897.415
    },
    {
      "epoch": 0.6793518152560556,
      "grad_norm": 11.946403503417969,
      "learning_rate": 4.9263077519066205e-05,
      "loss": 9.9914,
      "num_input_tokens_seen": 10211104,
      "step": 3008,
      "train_runtime": 791.5197,
      "train_tokens_per_second": 12900.631
    },
    {
      "epoch": 0.6865789622268647,
      "grad_norm": 11.356331825256348,
      "learning_rate": 4.922459526046606e-05,
      "loss": 9.9825,
      "num_input_tokens_seen": 10317504,
      "step": 3040,
      "train_runtime": 799.484,
      "train_tokens_per_second": 12905.203
    },
    {
      "epoch": 0.6938061091976737,
      "grad_norm": 11.492193222045898,
      "learning_rate": 4.918514944320803e-05,
      "loss": 9.8637,
      "num_input_tokens_seen": 10424384,
      "step": 3072,
      "train_runtime": 807.4545,
      "train_tokens_per_second": 12910.181
    },
    {
      "epoch": 0.7010332561684829,
      "grad_norm": 11.272955894470215,
      "learning_rate": 4.9144741636290814e-05,
      "loss": 9.9217,
      "num_input_tokens_seen": 10531648,
      "step": 3104,
      "train_runtime": 815.4243,
      "train_tokens_per_second": 12915.543
    },
    {
      "epoch": 0.708260403139292,
      "grad_norm": 11.536249160766602,
      "learning_rate": 4.9103373446977275e-05,
      "loss": 9.9743,
      "num_input_tokens_seen": 10636416,
      "step": 3136,
      "train_runtime": 823.3858,
      "train_tokens_per_second": 12917.901
    },
    {
      "epoch": 0.7154875501101011,
      "grad_norm": 11.05697250366211,
      "learning_rate": 4.906104652073049e-05,
      "loss": 9.9725,
      "num_input_tokens_seen": 10747904,
      "step": 3168,
      "train_runtime": 831.4077,
      "train_tokens_per_second": 12927.357
    },
    {
      "epoch": 0.7227146970809102,
      "grad_norm": 11.582247734069824,
      "learning_rate": 4.901776254114828e-05,
      "loss": 10.0,
      "num_input_tokens_seen": 10856704,
      "step": 3200,
      "train_runtime": 839.4312,
      "train_tokens_per_second": 12933.405
    },
    {
      "epoch": 0.7299418440517192,
      "grad_norm": 11.568798065185547,
      "learning_rate": 4.897352322989631e-05,
      "loss": 9.894,
      "num_input_tokens_seen": 10962656,
      "step": 3232,
      "train_runtime": 847.3877,
      "train_tokens_per_second": 12937.002
    },
    {
      "epoch": 0.7371689910225284,
      "grad_norm": 11.175493240356445,
      "learning_rate": 4.8928330346639525e-05,
      "loss": 10.017,
      "num_input_tokens_seen": 11069216,
      "step": 3264,
      "train_runtime": 855.3425,
      "train_tokens_per_second": 12941.268
    },
    {
      "epoch": 0.7443961379933375,
      "grad_norm": 12.092423439025879,
      "learning_rate": 4.8882185688972205e-05,
      "loss": 9.98,
      "num_input_tokens_seen": 11178400,
      "step": 3296,
      "train_runtime": 863.3073,
      "train_tokens_per_second": 12948.345
    },
    {
      "epoch": 0.7516232849641465,
      "grad_norm": 11.334515571594238,
      "learning_rate": 4.8835091092346464e-05,
      "loss": 9.7461,
      "num_input_tokens_seen": 11285312,
      "step": 3328,
      "train_runtime": 871.2555,
      "train_tokens_per_second": 12952.93
    },
    {
      "epoch": 0.7588504319349557,
      "grad_norm": 11.489630699157715,
      "learning_rate": 4.8787048429999213e-05,
      "loss": 9.887,
      "num_input_tokens_seen": 11393024,
      "step": 3360,
      "train_runtime": 879.2123,
      "train_tokens_per_second": 12958.218
    },
    {
      "epoch": 0.7660775789057648,
      "grad_norm": 11.156010627746582,
      "learning_rate": 4.8738059612877704e-05,
      "loss": 9.9471,
      "num_input_tokens_seen": 11498112,
      "step": 3392,
      "train_runtime": 887.1727,
      "train_tokens_per_second": 12960.399
    },
    {
      "epoch": 0.7733047258765738,
      "grad_norm": 11.3107271194458,
      "learning_rate": 4.868812658956344e-05,
      "loss": 9.8465,
      "num_input_tokens_seen": 11605984,
      "step": 3424,
      "train_runtime": 895.1591,
      "train_tokens_per_second": 12965.275
    },
    {
      "epoch": 0.780531872847383,
      "grad_norm": 10.939892768859863,
      "learning_rate": 4.863725134619477e-05,
      "loss": 9.9715,
      "num_input_tokens_seen": 11714432,
      "step": 3456,
      "train_runtime": 903.1447,
      "train_tokens_per_second": 12970.715
    },
    {
      "epoch": 0.7877590198181921,
      "grad_norm": 11.120620727539062,
      "learning_rate": 4.858543590638778e-05,
      "loss": 9.8787,
      "num_input_tokens_seen": 11821984,
      "step": 3488,
      "train_runtime": 911.1144,
      "train_tokens_per_second": 12975.302
    },
    {
      "epoch": 0.7949861667890011,
      "grad_norm": 10.778027534484863,
      "learning_rate": 4.8532682331155894e-05,
      "loss": 9.7008,
      "num_input_tokens_seen": 11929024,
      "step": 3520,
      "train_runtime": 919.0751,
      "train_tokens_per_second": 12979.379
    },
    {
      "epoch": 0.8022133137598103,
      "grad_norm": 10.636876106262207,
      "learning_rate": 4.847899271882783e-05,
      "loss": 9.8467,
      "num_input_tokens_seen": 12039520,
      "step": 3552,
      "train_runtime": 927.0617,
      "train_tokens_per_second": 12986.751
    },
    {
      "epoch": 0.8094404607306194,
      "grad_norm": 10.44221019744873,
      "learning_rate": 4.842436920496417e-05,
      "loss": 9.7854,
      "num_input_tokens_seen": 12146624,
      "step": 3584,
      "train_runtime": 935.0248,
      "train_tokens_per_second": 12990.697
    },
    {
      "epoch": 0.8166676077014285,
      "grad_norm": 10.654596328735352,
      "learning_rate": 4.8368813962272416e-05,
      "loss": 9.8154,
      "num_input_tokens_seen": 12252672,
      "step": 3616,
      "train_runtime": 942.9891,
      "train_tokens_per_second": 12993.439
    },
    {
      "epoch": 0.8238947546722376,
      "grad_norm": 11.109253883361816,
      "learning_rate": 4.8312329200520564e-05,
      "loss": 9.8774,
      "num_input_tokens_seen": 12358848,
      "step": 3648,
      "train_runtime": 950.9577,
      "train_tokens_per_second": 12996.213
    },
    {
      "epoch": 0.8311219016430467,
      "grad_norm": 10.435067176818848,
      "learning_rate": 4.8254917166449175e-05,
      "loss": 9.6625,
      "num_input_tokens_seen": 12464672,
      "step": 3680,
      "train_runtime": 958.9219,
      "train_tokens_per_second": 12998.631
    },
    {
      "epoch": 0.8383490486138558,
      "grad_norm": 10.909099578857422,
      "learning_rate": 4.819658014368207e-05,
      "loss": 9.7763,
      "num_input_tokens_seen": 12573696,
      "step": 3712,
      "train_runtime": 966.9079,
      "train_tokens_per_second": 13004.027
    },
    {
      "epoch": 0.8455761955846649,
      "grad_norm": 11.008915901184082,
      "learning_rate": 4.8137320452635446e-05,
      "loss": 9.8442,
      "num_input_tokens_seen": 12680352,
      "step": 3744,
      "train_runtime": 974.882,
      "train_tokens_per_second": 13007.063
    },
    {
      "epoch": 0.852803342555474,
      "grad_norm": 11.137667655944824,
      "learning_rate": 4.807714045042561e-05,
      "loss": 9.7755,
      "num_input_tokens_seen": 12789120,
      "step": 3776,
      "train_runtime": 982.8635,
      "train_tokens_per_second": 13012.102
    },
    {
      "epoch": 0.8600304895262831,
      "grad_norm": 10.43778133392334,
      "learning_rate": 4.8016042530775196e-05,
      "loss": 9.8673,
      "num_input_tokens_seen": 12896160,
      "step": 3808,
      "train_runtime": 990.8243,
      "train_tokens_per_second": 13015.587
    },
    {
      "epoch": 0.8672576364970922,
      "grad_norm": 11.31557846069336,
      "learning_rate": 4.7954029123917974e-05,
      "loss": 9.8688,
      "num_input_tokens_seen": 13003136,
      "step": 3840,
      "train_runtime": 998.7533,
      "train_tokens_per_second": 13019.367
    },
    {
      "epoch": 0.8744847834679013,
      "grad_norm": 11.191354751586914,
      "learning_rate": 4.789110269650219e-05,
      "loss": 9.8458,
      "num_input_tokens_seen": 13111808,
      "step": 3872,
      "train_runtime": 1006.6829,
      "train_tokens_per_second": 13024.765
    },
    {
      "epoch": 0.8817119304387104,
      "grad_norm": 11.254477500915527,
      "learning_rate": 4.7827265751492414e-05,
      "loss": 9.8635,
      "num_input_tokens_seen": 13219968,
      "step": 3904,
      "train_runtime": 1014.6149,
      "train_tokens_per_second": 13029.543
    },
    {
      "epoch": 0.8889390774095195,
      "grad_norm": 11.264737129211426,
      "learning_rate": 4.7762520828070024e-05,
      "loss": 9.698,
      "num_input_tokens_seen": 13325472,
      "step": 3936,
      "train_runtime": 1022.5214,
      "train_tokens_per_second": 13031.973
    },
    {
      "epoch": 0.8961662243803286,
      "grad_norm": 11.88194465637207,
      "learning_rate": 4.7696870501532184e-05,
      "loss": 9.7356,
      "num_input_tokens_seen": 13434336,
      "step": 3968,
      "train_runtime": 1030.4482,
      "train_tokens_per_second": 13037.372
    },
    {
      "epoch": 0.9033933713511377,
      "grad_norm": 11.274479866027832,
      "learning_rate": 4.7630317383189415e-05,
      "loss": 9.9573,
      "num_input_tokens_seen": 13544960,
      "step": 4000,
      "train_runtime": 1038.3824,
      "train_tokens_per_second": 13044.289
    },
    {
      "epoch": 0.9106205183219468,
      "grad_norm": 10.610489845275879,
      "learning_rate": 4.756286412026173e-05,
      "loss": 9.7438,
      "num_input_tokens_seen": 13653280,
      "step": 4032,
      "train_runtime": 1046.2986,
      "train_tokens_per_second": 13049.123
    },
    {
      "epoch": 0.917847665292756,
      "grad_norm": 11.345120429992676,
      "learning_rate": 4.7494513395773346e-05,
      "loss": 9.785,
      "num_input_tokens_seen": 13762208,
      "step": 4064,
      "train_runtime": 1054.2309,
      "train_tokens_per_second": 13054.263
    },
    {
      "epoch": 0.925074812263565,
      "grad_norm": 10.629451751708984,
      "learning_rate": 4.742526792844593e-05,
      "loss": 9.6964,
      "num_input_tokens_seen": 13869792,
      "step": 4096,
      "train_runtime": 1062.1569,
      "train_tokens_per_second": 13058.139
    },
    {
      "epoch": 0.9323019592343741,
      "grad_norm": 10.5888671875,
      "learning_rate": 4.735513047259051e-05,
      "loss": 9.8374,
      "num_input_tokens_seen": 13979904,
      "step": 4128,
      "train_runtime": 1070.1261,
      "train_tokens_per_second": 13063.791
    },
    {
      "epoch": 0.9395291062051833,
      "grad_norm": 10.708563804626465,
      "learning_rate": 4.728410381799787e-05,
      "loss": 9.7135,
      "num_input_tokens_seen": 14089664,
      "step": 4160,
      "train_runtime": 1078.0758,
      "train_tokens_per_second": 13069.27
    },
    {
      "epoch": 0.9467562531759923,
      "grad_norm": 11.519598007202148,
      "learning_rate": 4.721219078982762e-05,
      "loss": 9.8202,
      "num_input_tokens_seen": 14198624,
      "step": 4192,
      "train_runtime": 1086.0138,
      "train_tokens_per_second": 13074.074
    },
    {
      "epoch": 0.9539834001468014,
      "grad_norm": 10.692373275756836,
      "learning_rate": 4.7139394248495806e-05,
      "loss": 9.6856,
      "num_input_tokens_seen": 14308320,
      "step": 4224,
      "train_runtime": 1093.9484,
      "train_tokens_per_second": 13079.52
    },
    {
      "epoch": 0.9612105471176106,
      "grad_norm": 11.22472858428955,
      "learning_rate": 4.7065717089561116e-05,
      "loss": 9.7215,
      "num_input_tokens_seen": 14417024,
      "step": 4256,
      "train_runtime": 1101.8791,
      "train_tokens_per_second": 13084.034
    },
    {
      "epoch": 0.9684376940884196,
      "grad_norm": 10.84533405303955,
      "learning_rate": 4.699116224360977e-05,
      "loss": 9.6394,
      "num_input_tokens_seen": 14523200,
      "step": 4288,
      "train_runtime": 1109.7848,
      "train_tokens_per_second": 13086.501
    },
    {
      "epoch": 0.9756648410592287,
      "grad_norm": 10.93552017211914,
      "learning_rate": 4.691573267613885e-05,
      "loss": 9.6206,
      "num_input_tokens_seen": 14632608,
      "step": 4320,
      "train_runtime": 1117.7248,
      "train_tokens_per_second": 13091.423
    },
    {
      "epoch": 0.9828919880300379,
      "grad_norm": 10.873786926269531,
      "learning_rate": 4.683943138743845e-05,
      "loss": 9.7096,
      "num_input_tokens_seen": 14740288,
      "step": 4352,
      "train_runtime": 1125.6379,
      "train_tokens_per_second": 13095.053
    },
    {
      "epoch": 0.9901191350008469,
      "grad_norm": 11.144505500793457,
      "learning_rate": 4.676226141247227e-05,
      "loss": 9.7417,
      "num_input_tokens_seen": 14850336,
      "step": 4384,
      "train_runtime": 1133.5617,
      "train_tokens_per_second": 13100.598
    },
    {
      "epoch": 0.997346281971656,
      "grad_norm": 10.52340316772461,
      "learning_rate": 4.6684225820756926e-05,
      "loss": 9.6327,
      "num_input_tokens_seen": 14958624,
      "step": 4416,
      "train_runtime": 1141.5003,
      "train_tokens_per_second": 13104.354
    },
    {
      "epoch": 1.0,
      "eval_exact_token_accuracy": 0.0031871802639216185,
      "eval_loss": 2.446176290512085,
      "eval_prediction_target_similarity": 0.010505578787629812,
      "eval_runtime": 38.0991,
      "eval_samples_per_second": 656.445,
      "eval_sequence_accuracy": 0.0,
      "eval_steps_per_second": 20.525,
      "eval_top3_token_accuracy": 0.015845749527215958,
      "eval_top5_token_accuracy": 0.0328776091337204,
      "num_input_tokens_seen": 14999685,
      "step": 4428
    },
    {
      "epoch": 1.0045169668567557,
      "grad_norm": 31.97062873840332,
      "learning_rate": 4.660532771623983e-05,
      "loss": 9.3498,
      "num_input_tokens_seen": 15073701,
      "step": 4448,
      "train_runtime": 1189.4846,
      "train_tokens_per_second": 12672.465
    },
    {
      "epoch": 1.0117441138275647,
      "grad_norm": 11.630021095275879,
      "learning_rate": 4.652557023717577e-05,
      "loss": 9.3364,
      "num_input_tokens_seen": 15182629,
      "step": 4480,
      "train_runtime": 1197.5107,
      "train_tokens_per_second": 12678.492
    },
    {
      "epoch": 1.0189712607983739,
      "grad_norm": 10.971976280212402,
      "learning_rate": 4.644495655600203e-05,
      "loss": 9.0958,
      "num_input_tokens_seen": 15287941,
      "step": 4512,
      "train_runtime": 1205.5346,
      "train_tokens_per_second": 12681.462
    },
    {
      "epoch": 1.026198407769183,
      "grad_norm": 10.950613021850586,
      "learning_rate": 4.636348987921224e-05,
      "loss": 9.241,
      "num_input_tokens_seen": 15393701,
      "step": 4544,
      "train_runtime": 1213.5582,
      "train_tokens_per_second": 12684.765
    },
    {
      "epoch": 1.0334255547399922,
      "grad_norm": 11.335638999938965,
      "learning_rate": 4.628117344722883e-05,
      "loss": 9.3354,
      "num_input_tokens_seen": 15504901,
      "step": 4576,
      "train_runtime": 1221.6277,
      "train_tokens_per_second": 12692.002
    },
    {
      "epoch": 1.0406527017108012,
      "grad_norm": 10.984670639038086,
      "learning_rate": 4.619801053427413e-05,
      "loss": 9.2667,
      "num_input_tokens_seen": 15614693,
      "step": 4608,
      "train_runtime": 1229.606,
      "train_tokens_per_second": 12698.94
    },
    {
      "epoch": 1.0478798486816103,
      "grad_norm": 11.432820320129395,
      "learning_rate": 4.6114004448240136e-05,
      "loss": 9.3889,
      "num_input_tokens_seen": 15721637,
      "step": 4640,
      "train_runtime": 1237.5789,
      "train_tokens_per_second": 12703.544
    },
    {
      "epoch": 1.0551069956524195,
      "grad_norm": 11.077058792114258,
      "learning_rate": 4.602915853055694e-05,
      "loss": 9.3772,
      "num_input_tokens_seen": 15829765,
      "step": 4672,
      "train_runtime": 1245.5478,
      "train_tokens_per_second": 12709.078
    },
    {
      "epoch": 1.0623341426232285,
      "grad_norm": 10.911458969116211,
      "learning_rate": 4.594347615605981e-05,
      "loss": 9.4252,
      "num_input_tokens_seen": 15941061,
      "step": 4704,
      "train_runtime": 1253.542,
      "train_tokens_per_second": 12716.815
    },
    {
      "epoch": 1.0695612895940376,
      "grad_norm": 12.071025848388672,
      "learning_rate": 4.585696073285497e-05,
      "loss": 9.3606,
      "num_input_tokens_seen": 16050949,
      "step": 4736,
      "train_runtime": 1261.5746,
      "train_tokens_per_second": 12722.949
    },
    {
      "epoch": 1.0767884365648468,
      "grad_norm": 11.500454902648926,
      "learning_rate": 4.576961570218403e-05,
      "loss": 9.2056,
      "num_input_tokens_seen": 16157669,
      "step": 4768,
      "train_runtime": 1269.5425,
      "train_tokens_per_second": 12727.159
    },
    {
      "epoch": 1.0840155835356557,
      "grad_norm": 11.268909454345703,
      "learning_rate": 4.5681444538287086e-05,
      "loss": 9.3154,
      "num_input_tokens_seen": 16267301,
      "step": 4800,
      "train_runtime": 1277.5315,
      "train_tokens_per_second": 12733.386
    },
    {
      "epoch": 1.091242730506465,
      "grad_norm": 11.402393341064453,
      "learning_rate": 4.5592450748264587e-05,
      "loss": 9.2239,
      "num_input_tokens_seen": 16373157,
      "step": 4832,
      "train_runtime": 1285.4951,
      "train_tokens_per_second": 12736.849
    },
    {
      "epoch": 1.098469877477274,
      "grad_norm": 11.296697616577148,
      "learning_rate": 4.550263787193775e-05,
      "loss": 9.2212,
      "num_input_tokens_seen": 16481381,
      "step": 4864,
      "train_runtime": 1293.4813,
      "train_tokens_per_second": 12741.878
    },
    {
      "epoch": 1.105697024448083,
      "grad_norm": 11.587206840515137,
      "learning_rate": 4.541200948170785e-05,
      "loss": 9.2724,
      "num_input_tokens_seen": 16590469,
      "step": 4896,
      "train_runtime": 1301.4724,
      "train_tokens_per_second": 12747.461
    },
    {
      "epoch": 1.1129241714188922,
      "grad_norm": 12.328251838684082,
      "learning_rate": 4.532056918241405e-05,
      "loss": 9.1849,
      "num_input_tokens_seen": 16696165,
      "step": 4928,
      "train_runtime": 1309.4461,
      "train_tokens_per_second": 12750.556
    },
    {
      "epoch": 1.1201513183897014,
      "grad_norm": 11.482054710388184,
      "learning_rate": 4.522832061119008e-05,
      "loss": 9.3053,
      "num_input_tokens_seen": 16806725,
      "step": 4960,
      "train_runtime": 1317.44,
      "train_tokens_per_second": 12757.108
    },
    {
      "epoch": 1.1273784653605103,
      "grad_norm": 12.343602180480957,
      "learning_rate": 4.51352674373195e-05,
      "loss": 9.2298,
      "num_input_tokens_seen": 16913733,
      "step": 4992,
      "train_runtime": 1325.4318,
      "train_tokens_per_second": 12760.923
    },
    {
      "epoch": 1.1346056123313195,
      "grad_norm": 11.866896629333496,
      "learning_rate": 4.50414133620898e-05,
      "loss": 9.4158,
      "num_input_tokens_seen": 17028869,
      "step": 5024,
      "train_runtime": 1333.4772,
      "train_tokens_per_second": 12770.274
    },
    {
      "epoch": 1.1418327593021287,
      "grad_norm": 11.837981224060059,
      "learning_rate": 4.4946762118645156e-05,
      "loss": 9.2959,
      "num_input_tokens_seen": 17139685,
      "step": 5056,
      "train_runtime": 1341.4838,
      "train_tokens_per_second": 12776.662
    },
    {
      "epoch": 1.1490599062729376,
      "grad_norm": 11.418581008911133,
      "learning_rate": 4.4851317471837964e-05,
      "loss": 9.3516,
      "num_input_tokens_seen": 17245541,
      "step": 5088,
      "train_runtime": 1349.4637,
      "train_tokens_per_second": 12779.551
    },
    {
      "epoch": 1.1562870532437468,
      "grad_norm": 11.284612655639648,
      "learning_rate": 4.475508321807902e-05,
      "loss": 9.3089,
      "num_input_tokens_seen": 17355333,
      "step": 5120,
      "train_runtime": 1357.4658,
      "train_tokens_per_second": 12785.098
    },
    {
      "epoch": 1.163514200214556,
      "grad_norm": 11.8165283203125,
      "learning_rate": 4.465806318518663e-05,
      "loss": 9.2412,
      "num_input_tokens_seen": 17464613,
      "step": 5152,
      "train_runtime": 1365.4599,
      "train_tokens_per_second": 12790.279
    },
    {
      "epoch": 1.170741347185365,
      "grad_norm": 11.401427268981934,
      "learning_rate": 4.4560261232234236e-05,
      "loss": 9.2479,
      "num_input_tokens_seen": 17573573,
      "step": 5184,
      "train_runtime": 1373.4663,
      "train_tokens_per_second": 12795.052
    },
    {
      "epoch": 1.177968494156174,
      "grad_norm": 11.91321849822998,
      "learning_rate": 4.4461681249397e-05,
      "loss": 9.2353,
      "num_input_tokens_seen": 17677477,
      "step": 5216,
      "train_runtime": 1381.4323,
      "train_tokens_per_second": 12796.485
    },
    {
      "epoch": 1.1851956411269833,
      "grad_norm": 12.42469310760498,
      "learning_rate": 4.436232715779702e-05,
      "loss": 9.3849,
      "num_input_tokens_seen": 17787653,
      "step": 5248,
      "train_runtime": 1389.4424,
      "train_tokens_per_second": 12802.008
    },
    {
      "epoch": 1.1924227880977925,
      "grad_norm": 11.451302528381348,
      "learning_rate": 4.426220290934739e-05,
      "loss": 9.2951,
      "num_input_tokens_seen": 17894629,
      "step": 5280,
      "train_runtime": 1397.4223,
      "train_tokens_per_second": 12805.455
    },
    {
      "epoch": 1.1996499350686014,
      "grad_norm": 12.316774368286133,
      "learning_rate": 4.4161312486594997e-05,
      "loss": 9.2041,
      "num_input_tokens_seen": 18000005,
      "step": 5312,
      "train_runtime": 1405.3907,
      "train_tokens_per_second": 12807.83
    },
    {
      "epoch": 1.2068770820394106,
      "grad_norm": 11.855438232421875,
      "learning_rate": 4.405965990256212e-05,
      "loss": 9.26,
      "num_input_tokens_seen": 18109733,
      "step": 5344,
      "train_runtime": 1413.3938,
      "train_tokens_per_second": 12812.942
    },
    {
      "epoch": 1.2141042290102195,
      "grad_norm": 11.927081108093262,
      "learning_rate": 4.395724920058678e-05,
      "loss": 9.2838,
      "num_input_tokens_seen": 18218597,
      "step": 5376,
      "train_runtime": 1421.3977,
      "train_tokens_per_second": 12817.381
    },
    {
      "epoch": 1.2213313759810287,
      "grad_norm": 11.284934043884277,
      "learning_rate": 4.385408445416197e-05,
      "loss": 9.2574,
      "num_input_tokens_seen": 18329125,
      "step": 5408,
      "train_runtime": 1429.4047,
      "train_tokens_per_second": 12822.908
    },
    {
      "epoch": 1.2285585229518379,
      "grad_norm": 11.537347793579102,
      "learning_rate": 4.3750169766773555e-05,
      "loss": 9.2775,
      "num_input_tokens_seen": 18439493,
      "step": 5440,
      "train_runtime": 1437.4177,
      "train_tokens_per_second": 12828.208
    },
    {
      "epoch": 1.235785669922647,
      "grad_norm": 11.380985260009766,
      "learning_rate": 4.364550927173711e-05,
      "loss": 9.3235,
      "num_input_tokens_seen": 18543845,
      "step": 5472,
      "train_runtime": 1445.3849,
      "train_tokens_per_second": 12829.693
    },
    {
      "epoch": 1.243012816893456,
      "grad_norm": 12.211498260498047,
      "learning_rate": 4.354010713203348e-05,
      "loss": 9.4111,
      "num_input_tokens_seen": 18653637,
      "step": 5504,
      "train_runtime": 1453.3986,
      "train_tokens_per_second": 12834.495
    },
    {
      "epoch": 1.2502399638642652,
      "grad_norm": 11.489727973937988,
      "learning_rate": 4.3433967540143215e-05,
      "loss": 9.2543,
      "num_input_tokens_seen": 18764421,
      "step": 5536,
      "train_runtime": 1461.4001,
      "train_tokens_per_second": 12840.03
    },
    {
      "epoch": 1.2574671108350741,
      "grad_norm": 11.45051383972168,
      "learning_rate": 4.332709471787978e-05,
      "loss": 9.3527,
      "num_input_tokens_seen": 18872389,
      "step": 5568,
      "train_runtime": 1469.3918,
      "train_tokens_per_second": 12843.674
    },
    {
      "epoch": 1.2646942578058833,
      "grad_norm": 11.345503807067871,
      "learning_rate": 4.321949291622166e-05,
      "loss": 9.2605,
      "num_input_tokens_seen": 18979109,
      "step": 5600,
      "train_runtime": 1477.3902,
      "train_tokens_per_second": 12846.376
    },
    {
      "epoch": 1.2719214047766925,
      "grad_norm": 11.15483283996582,
      "learning_rate": 4.3111166415143263e-05,
      "loss": 9.2953,
      "num_input_tokens_seen": 19086085,
      "step": 5632,
      "train_runtime": 1485.4355,
      "train_tokens_per_second": 12848.814
    },
    {
      "epoch": 1.2791485517475016,
      "grad_norm": 14.253543853759766,
      "learning_rate": 4.300211952344466e-05,
      "loss": 9.3043,
      "num_input_tokens_seen": 19194213,
      "step": 5664,
      "train_runtime": 1493.4304,
      "train_tokens_per_second": 12852.432
    },
    {
      "epoch": 1.2863756987183106,
      "grad_norm": 11.792632102966309,
      "learning_rate": 4.289235657858022e-05,
      "loss": 9.244,
      "num_input_tokens_seen": 19300133,
      "step": 5696,
      "train_runtime": 1501.4008,
      "train_tokens_per_second": 12854.75
    },
    {
      "epoch": 1.2936028456891198,
      "grad_norm": 11.302199363708496,
      "learning_rate": 4.2781881946486093e-05,
      "loss": 9.3053,
      "num_input_tokens_seen": 19412933,
      "step": 5728,
      "train_runtime": 1509.4126,
      "train_tokens_per_second": 12861.251
    },
    {
      "epoch": 1.300829992659929,
      "grad_norm": 12.287952423095703,
      "learning_rate": 4.267070002140651e-05,
      "loss": 9.416,
      "num_input_tokens_seen": 19523717,
      "step": 5760,
      "train_runtime": 1517.4145,
      "train_tokens_per_second": 12866.436
    },
    {
      "epoch": 1.3080571396307379,
      "grad_norm": 11.274589538574219,
      "learning_rate": 4.255881522571904e-05,
      "loss": 9.3062,
      "num_input_tokens_seen": 19632101,
      "step": 5792,
      "train_runtime": 1525.4099,
      "train_tokens_per_second": 12870.05
    },
    {
      "epoch": 1.315284286601547,
      "grad_norm": 11.937210083007812,
      "learning_rate": 4.2446232009758654e-05,
      "loss": 9.2079,
      "num_input_tokens_seen": 19738821,
      "step": 5824,
      "train_runtime": 1533.3896,
      "train_tokens_per_second": 12872.672
    },
    {
      "epoch": 1.3225114335723562,
      "grad_norm": 11.818521499633789,
      "learning_rate": 4.233295485164073e-05,
      "loss": 9.3598,
      "num_input_tokens_seen": 19845733,
      "step": 5856,
      "train_runtime": 1541.4109,
      "train_tokens_per_second": 12875.044
    },
    {
      "epoch": 1.3297385805431652,
      "grad_norm": 11.455673217773438,
      "learning_rate": 4.2218988257082906e-05,
      "loss": 9.237,
      "num_input_tokens_seen": 19954373,
      "step": 5888,
      "train_runtime": 1549.3976,
      "train_tokens_per_second": 12878.794
    },
    {
      "epoch": 1.3369657275139744,
      "grad_norm": 11.911782264709473,
      "learning_rate": 4.21043367592259e-05,
      "loss": 9.1894,
      "num_input_tokens_seen": 20063653,
      "step": 5920,
      "train_runtime": 1557.38,
      "train_tokens_per_second": 12882.953
    },
    {
      "epoch": 1.3441928744847835,
      "grad_norm": 11.692244529724121,
      "learning_rate": 4.1989004918453164e-05,
      "loss": 9.1928,
      "num_input_tokens_seen": 20172165,
      "step": 5952,
      "train_runtime": 1565.3594,
      "train_tokens_per_second": 12886.603
    },
    {
      "epoch": 1.3514200214555925,
      "grad_norm": 11.670120239257812,
      "learning_rate": 4.187299732220952e-05,
      "loss": 9.3702,
      "num_input_tokens_seen": 20279429,
      "step": 5984,
      "train_runtime": 1573.3803,
      "train_tokens_per_second": 12889.083
    },
    {
      "epoch": 1.3586471684264017,
      "grad_norm": 11.418225288391113,
      "learning_rate": 4.1756318584818626e-05,
      "loss": 9.1814,
      "num_input_tokens_seen": 20384773,
      "step": 6016,
      "train_runtime": 1581.3603,
      "train_tokens_per_second": 12890.656
    },
    {
      "epoch": 1.3658743153972108,
      "grad_norm": 11.680295944213867,
      "learning_rate": 4.163897334729954e-05,
      "loss": 9.3123,
      "num_input_tokens_seen": 20489637,
      "step": 6048,
      "train_runtime": 1589.3272,
      "train_tokens_per_second": 12892.019
    },
    {
      "epoch": 1.37310146236802,
      "grad_norm": 11.393439292907715,
      "learning_rate": 4.1520966277182e-05,
      "loss": 9.1858,
      "num_input_tokens_seen": 20597893,
      "step": 6080,
      "train_runtime": 1597.3184,
      "train_tokens_per_second": 12895.296
    },
    {
      "epoch": 1.380328609338829,
      "grad_norm": 11.701091766357422,
      "learning_rate": 4.14023020683209e-05,
      "loss": 9.2168,
      "num_input_tokens_seen": 20706181,
      "step": 6112,
      "train_runtime": 1605.3067,
      "train_tokens_per_second": 12898.583
    },
    {
      "epoch": 1.3875557563096381,
      "grad_norm": 11.6464262008667,
      "learning_rate": 4.128298544070946e-05,
      "loss": 9.2414,
      "num_input_tokens_seen": 20817253,
      "step": 6144,
      "train_runtime": 1613.3153,
      "train_tokens_per_second": 12903.4
    },
    {
      "epoch": 1.394782903280447,
      "grad_norm": 11.56137466430664,
      "learning_rate": 4.116302114029155e-05,
      "loss": 9.3008,
      "num_input_tokens_seen": 20926533,
      "step": 6176,
      "train_runtime": 1621.3092,
      "train_tokens_per_second": 12907.182
    },
    {
      "epoch": 1.4020100502512562,
      "grad_norm": 12.157585144042969,
      "learning_rate": 4.1042413938772925e-05,
      "loss": 9.2443,
      "num_input_tokens_seen": 21032773,
      "step": 6208,
      "train_runtime": 1629.2939,
      "train_tokens_per_second": 12909.134
    },
    {
      "epoch": 1.4092371972220654,
      "grad_norm": 11.586462020874023,
      "learning_rate": 4.0921168633431394e-05,
      "loss": 9.309,
      "num_input_tokens_seen": 21141061,
      "step": 6240,
      "train_runtime": 1637.2795,
      "train_tokens_per_second": 12912.31
    },
    {
      "epoch": 1.4164643441928746,
      "grad_norm": 11.528801918029785,
      "learning_rate": 4.0799290046926e-05,
      "loss": 9.292,
      "num_input_tokens_seen": 21248645,
      "step": 6272,
      "train_runtime": 1645.2961,
      "train_tokens_per_second": 12914.785
    },
    {
      "epoch": 1.4236914911636835,
      "grad_norm": 11.736306190490723,
      "learning_rate": 4.067678302710523e-05,
      "loss": 9.3551,
      "num_input_tokens_seen": 21359173,
      "step": 6304,
      "train_runtime": 1653.2837,
      "train_tokens_per_second": 12919.242
    },
    {
      "epoch": 1.4309186381344927,
      "grad_norm": 11.707889556884766,
      "learning_rate": 4.055365244681414e-05,
      "loss": 9.2426,
      "num_input_tokens_seen": 21467109,
      "step": 6336,
      "train_runtime": 1661.2773,
      "train_tokens_per_second": 12922.05
    },
    {
      "epoch": 1.4381457851053017,
      "grad_norm": 12.67696475982666,
      "learning_rate": 4.0429903203700594e-05,
      "loss": 9.3044,
      "num_input_tokens_seen": 21574821,
      "step": 6368,
      "train_runtime": 1669.2767,
      "train_tokens_per_second": 12924.652
    },
    {
      "epoch": 1.4453729320761108,
      "grad_norm": 12.385729789733887,
      "learning_rate": 4.030554022002038e-05,
      "loss": 9.2617,
      "num_input_tokens_seen": 21683845,
      "step": 6400,
      "train_runtime": 1677.2832,
      "train_tokens_per_second": 12927.957
    },
    {
      "epoch": 1.45260007904692,
      "grad_norm": 12.113226890563965,
      "learning_rate": 4.018056844244148e-05,
      "loss": 9.1946,
      "num_input_tokens_seen": 21792069,
      "step": 6432,
      "train_runtime": 1685.283,
      "train_tokens_per_second": 12930.807
    },
    {
      "epoch": 1.4598272260177292,
      "grad_norm": 11.734798431396484,
      "learning_rate": 4.005499284184728e-05,
      "loss": 9.1576,
      "num_input_tokens_seen": 21900357,
      "step": 6464,
      "train_runtime": 1693.3615,
      "train_tokens_per_second": 12933.067
    },
    {
      "epoch": 1.4670543729885381,
      "grad_norm": 11.682732582092285,
      "learning_rate": 3.992881841313888e-05,
      "loss": 9.299,
      "num_input_tokens_seen": 22011461,
      "step": 6496,
      "train_runtime": 1701.3643,
      "train_tokens_per_second": 12937.535
    },
    {
      "epoch": 1.4742815199593473,
      "grad_norm": 11.302396774291992,
      "learning_rate": 3.9802050175036377e-05,
      "loss": 9.2819,
      "num_input_tokens_seen": 22119333,
      "step": 6528,
      "train_runtime": 1709.3497,
      "train_tokens_per_second": 12940.203
    },
    {
      "epoch": 1.4815086669301565,
      "grad_norm": 11.840173721313477,
      "learning_rate": 3.967469316987925e-05,
      "loss": 9.3161,
      "num_input_tokens_seen": 22226149,
      "step": 6560,
      "train_runtime": 1717.3213,
      "train_tokens_per_second": 12942.336
    },
    {
      "epoch": 1.4887358139009654,
      "grad_norm": 11.766807556152344,
      "learning_rate": 3.954675246342583e-05,
      "loss": 9.2363,
      "num_input_tokens_seen": 22336517,
      "step": 6592,
      "train_runtime": 1725.3214,
      "train_tokens_per_second": 12946.293
    },
    {
      "epoch": 1.4959629608717746,
      "grad_norm": 11.698545455932617,
      "learning_rate": 3.9418233144651765e-05,
      "loss": 9.1898,
      "num_input_tokens_seen": 22444101,
      "step": 6624,
      "train_runtime": 1733.311,
      "train_tokens_per_second": 12948.687
    },
    {
      "epoch": 1.5000282310428548,
      "eval_exact_token_accuracy": 0.006007886491715908,
      "eval_loss": 2.4028189182281494,
      "eval_prediction_target_similarity": 0.013209870455371224,
      "eval_runtime": 38.1051,
      "eval_samples_per_second": 656.343,
      "eval_sequence_accuracy": 0.0,
      "eval_steps_per_second": 20.522,
      "eval_top3_token_accuracy": 0.023348046466708183,
      "eval_top5_token_accuracy": 0.0404769591987133,
      "num_input_tokens_seen": 22502565,
      "step": 6642
    },
    {
      "epoch": 1.5031901078425838,
      "grad_norm": 11.54082202911377,
      "learning_rate": 3.9289140325547626e-05,
      "loss": 9.186,
      "num_input_tokens_seen": 22552005,
      "step": 6656,
      "train_runtime": 1780.2733,
      "train_tokens_per_second": 12667.721
    },
    {
      "epoch": 1.510417254813393,
      "grad_norm": 11.467602729797363,
      "learning_rate": 3.9159479140915524e-05,
      "loss": 9.2323,
      "num_input_tokens_seen": 22657861,
      "step": 6688,
      "train_runtime": 1788.2547,
      "train_tokens_per_second": 12670.377
    },
    {
      "epoch": 1.517644401784202,
      "grad_norm": 11.527681350708008,
      "learning_rate": 3.9029254748164954e-05,
      "loss": 9.2212,
      "num_input_tokens_seen": 22765989,
      "step": 6720,
      "train_runtime": 1796.2505,
      "train_tokens_per_second": 12674.173
    },
    {
      "epoch": 1.5248715487550109,
      "grad_norm": 11.931780815124512,
      "learning_rate": 3.8898472327107574e-05,
      "loss": 9.1019,
      "num_input_tokens_seen": 22871077,
      "step": 6752,
      "train_runtime": 1804.2449,
      "train_tokens_per_second": 12676.26
    },
    {
      "epoch": 1.53209869572582,
      "grad_norm": 11.269338607788086,
      "learning_rate": 3.8767137079751206e-05,
      "loss": 9.3755,
      "num_input_tokens_seen": 22978693,
      "step": 6784,
      "train_runtime": 1812.293,
      "train_tokens_per_second": 12679.348
    },
    {
      "epoch": 1.5393258426966292,
      "grad_norm": 12.183550834655762,
      "learning_rate": 3.863525423009293e-05,
      "loss": 9.2379,
      "num_input_tokens_seen": 23088581,
      "step": 6816,
      "train_runtime": 1820.3056,
      "train_tokens_per_second": 12683.903
    },
    {
      "epoch": 1.5465529896674384,
      "grad_norm": 11.402620315551758,
      "learning_rate": 3.850282902391126e-05,
      "loss": 9.1631,
      "num_input_tokens_seen": 23198821,
      "step": 6848,
      "train_runtime": 1828.3165,
      "train_tokens_per_second": 12688.624
    },
    {
      "epoch": 1.5537801366382475,
      "grad_norm": 11.865264892578125,
      "learning_rate": 3.836986672855752e-05,
      "loss": 9.146,
      "num_input_tokens_seen": 23306661,
      "step": 6880,
      "train_runtime": 1836.3063,
      "train_tokens_per_second": 12692.142
    },
    {
      "epoch": 1.5610072836090565,
      "grad_norm": 11.929115295410156,
      "learning_rate": 3.823637263274633e-05,
      "loss": 9.1874,
      "num_input_tokens_seen": 23414501,
      "step": 6912,
      "train_runtime": 1844.2939,
      "train_tokens_per_second": 12695.645
    },
    {
      "epoch": 1.5682344305798657,
      "grad_norm": 11.65829086303711,
      "learning_rate": 3.8102352046345216e-05,
      "loss": 9.2399,
      "num_input_tokens_seen": 23521989,
      "step": 6944,
      "train_runtime": 1852.2738,
      "train_tokens_per_second": 12698.981
    },
    {
      "epoch": 1.5754615775506746,
      "grad_norm": 11.978333473205566,
      "learning_rate": 3.796781030016342e-05,
      "loss": 9.3005,
      "num_input_tokens_seen": 23630501,
      "step": 6976,
      "train_runtime": 1860.2687,
      "train_tokens_per_second": 12702.736
    },
    {
      "epoch": 1.5826887245214838,
      "grad_norm": 11.302565574645996,
      "learning_rate": 3.783275274573987e-05,
      "loss": 9.2558,
      "num_input_tokens_seen": 23739813,
      "step": 7008,
      "train_runtime": 1868.2701,
      "train_tokens_per_second": 12706.842
    },
    {
      "epoch": 1.589915871492293,
      "grad_norm": 12.077000617980957,
      "learning_rate": 3.769718475513029e-05,
      "loss": 9.1451,
      "num_input_tokens_seen": 23850629,
      "step": 7040,
      "train_runtime": 1876.2803,
      "train_tokens_per_second": 12711.655
    },
    {
      "epoch": 1.5971430184631021,
      "grad_norm": 12.00036334991455,
      "learning_rate": 3.756111172069356e-05,
      "loss": 9.2541,
      "num_input_tokens_seen": 23957829,
      "step": 7072,
      "train_runtime": 1884.2615,
      "train_tokens_per_second": 12714.705
    },
    {
      "epoch": 1.604370165433911,
      "grad_norm": 11.470970153808594,
      "learning_rate": 3.7424539054877185e-05,
      "loss": 9.2688,
      "num_input_tokens_seen": 24065413,
      "step": 7104,
      "train_runtime": 1892.2484,
      "train_tokens_per_second": 12717.893
    },
    {
      "epoch": 1.6115973124047203,
      "grad_norm": 12.690217971801758,
      "learning_rate": 3.7287472190002035e-05,
      "loss": 9.1534,
      "num_input_tokens_seen": 24175237,
      "step": 7136,
      "train_runtime": 1900.2506,
      "train_tokens_per_second": 12722.131
    },
    {
      "epoch": 1.6188244593755292,
      "grad_norm": 10.790614128112793,
      "learning_rate": 3.71499165780463e-05,
      "loss": 9.0867,
      "num_input_tokens_seen": 24282245,
      "step": 7168,
      "train_runtime": 1908.2366,
      "train_tokens_per_second": 12724.966
    },
    {
      "epoch": 1.6260516063463384,
      "grad_norm": 11.414484024047852,
      "learning_rate": 3.7011877690428543e-05,
      "loss": 9.277,
      "num_input_tokens_seen": 24393637,
      "step": 7200,
      "train_runtime": 1916.2436,
      "train_tokens_per_second": 12729.925
    },
    {
      "epoch": 1.6332787533171476,
      "grad_norm": 11.676867485046387,
      "learning_rate": 3.687336101779017e-05,
      "loss": 9.2468,
      "num_input_tokens_seen": 24501477,
      "step": 7232,
      "train_runtime": 1924.2351,
      "train_tokens_per_second": 12733.099
    },
    {
      "epoch": 1.6405059002879567,
      "grad_norm": 12.31120491027832,
      "learning_rate": 3.673437206977696e-05,
      "loss": 9.1329,
      "num_input_tokens_seen": 24609509,
      "step": 7264,
      "train_runtime": 1932.2254,
      "train_tokens_per_second": 12736.355
    },
    {
      "epoch": 1.647733047258766,
      "grad_norm": 12.248489379882812,
      "learning_rate": 3.6594916374819934e-05,
      "loss": 9.1734,
      "num_input_tokens_seen": 24714789,
      "step": 7296,
      "train_runtime": 1940.1859,
      "train_tokens_per_second": 12738.361
    },
    {
      "epoch": 1.6549601942295749,
      "grad_norm": 11.699071884155273,
      "learning_rate": 3.6454999479915484e-05,
      "loss": 9.2789,
      "num_input_tokens_seen": 24820325,
      "step": 7328,
      "train_runtime": 1948.1377,
      "train_tokens_per_second": 12740.539
    },
    {
      "epoch": 1.6621873412003838,
      "grad_norm": 11.819514274597168,
      "learning_rate": 3.63146269504047e-05,
      "loss": 9.208,
      "num_input_tokens_seen": 24927781,
      "step": 7360,
      "train_runtime": 1956.119,
      "train_tokens_per_second": 12743.489
    },
    {
      "epoch": 1.669414488171193,
      "grad_norm": 11.800122261047363,
      "learning_rate": 3.617380436975203e-05,
      "loss": 9.2477,
      "num_input_tokens_seen": 25034757,
      "step": 7392,
      "train_runtime": 1964.0897,
      "train_tokens_per_second": 12746.239
    },
    {
      "epoch": 1.6766416351420022,
      "grad_norm": 11.605195999145508,
      "learning_rate": 3.6032537339323166e-05,
      "loss": 9.2273,
      "num_input_tokens_seen": 25145061,
      "step": 7424,
      "train_runtime": 1972.0935,
      "train_tokens_per_second": 12750.441
    },
    {
      "epoch": 1.6838687821128113,
      "grad_norm": 11.291119575500488,
      "learning_rate": 3.589083147816223e-05,
      "loss": 9.1897,
      "num_input_tokens_seen": 25250437,
      "step": 7456,
      "train_runtime": 1980.0603,
      "train_tokens_per_second": 12752.358
    },
    {
      "epoch": 1.6910959290836205,
      "grad_norm": 12.13893985748291,
      "learning_rate": 3.574869242276836e-05,
      "loss": 9.1414,
      "num_input_tokens_seen": 25360101,
      "step": 7488,
      "train_runtime": 1988.0543,
      "train_tokens_per_second": 12756.242
    },
    {
      "epoch": 1.6983230760544294,
      "grad_norm": 11.732057571411133,
      "learning_rate": 3.560612582687138e-05,
      "loss": 9.137,
      "num_input_tokens_seen": 25467813,
      "step": 7520,
      "train_runtime": 1996.0458,
      "train_tokens_per_second": 12759.133
    },
    {
      "epoch": 1.7055502230252384,
      "grad_norm": 11.350836753845215,
      "learning_rate": 3.546313736120704e-05,
      "loss": 9.2261,
      "num_input_tokens_seen": 25580197,
      "step": 7552,
      "train_runtime": 2004.0496,
      "train_tokens_per_second": 12764.253
    },
    {
      "epoch": 1.7127773699960476,
      "grad_norm": 12.01934814453125,
      "learning_rate": 3.5319732713291365e-05,
      "loss": 9.169,
      "num_input_tokens_seen": 25685925,
      "step": 7584,
      "train_runtime": 2012.0182,
      "train_tokens_per_second": 12766.249
    },
    {
      "epoch": 1.7200045169668567,
      "grad_norm": 10.770981788635254,
      "learning_rate": 3.517591758719448e-05,
      "loss": 9.0668,
      "num_input_tokens_seen": 25794405,
      "step": 7616,
      "train_runtime": 2020.0144,
      "train_tokens_per_second": 12769.416
    },
    {
      "epoch": 1.727231663937666,
      "grad_norm": 11.598608016967773,
      "learning_rate": 3.5031697703313706e-05,
      "loss": 9.2078,
      "num_input_tokens_seen": 25903365,
      "step": 7648,
      "train_runtime": 2028.0025,
      "train_tokens_per_second": 12772.847
    },
    {
      "epoch": 1.734458810908475,
      "grad_norm": 11.840791702270508,
      "learning_rate": 3.488707879814604e-05,
      "loss": 9.1208,
      "num_input_tokens_seen": 26013893,
      "step": 7680,
      "train_runtime": 2035.9949,
      "train_tokens_per_second": 12776.993
    },
    {
      "epoch": 1.741685957879284,
      "grad_norm": 11.734840393066406,
      "learning_rate": 3.4742066624059955e-05,
      "loss": 9.3165,
      "num_input_tokens_seen": 26123589,
      "step": 7712,
      "train_runtime": 2043.9943,
      "train_tokens_per_second": 12780.657
    },
    {
      "epoch": 1.7489131048500932,
      "grad_norm": 11.84100341796875,
      "learning_rate": 3.459666694906661e-05,
      "loss": 9.2058,
      "num_input_tokens_seen": 26230661,
      "step": 7744,
      "train_runtime": 2052.0317,
      "train_tokens_per_second": 12782.776
    },
    {
      "epoch": 1.7561402518209022,
      "grad_norm": 11.372296333312988,
      "learning_rate": 3.445088555659042e-05,
      "loss": 9.1531,
      "num_input_tokens_seen": 26339045,
      "step": 7776,
      "train_runtime": 2060.011,
      "train_tokens_per_second": 12785.876
    },
    {
      "epoch": 1.7633673987917113,
      "grad_norm": 11.241960525512695,
      "learning_rate": 3.4304728245239024e-05,
      "loss": 9.1784,
      "num_input_tokens_seen": 26445541,
      "step": 7808,
      "train_runtime": 2067.9821,
      "train_tokens_per_second": 12788.09
    },
    {
      "epoch": 1.7705945457625205,
      "grad_norm": 12.294357299804688,
      "learning_rate": 3.4158200828572604e-05,
      "loss": 9.2691,
      "num_input_tokens_seen": 26554725,
      "step": 7840,
      "train_runtime": 2075.9639,
      "train_tokens_per_second": 12791.516
    },
    {
      "epoch": 1.7778216927333297,
      "grad_norm": 11.434219360351562,
      "learning_rate": 3.40113091348727e-05,
      "loss": 9.146,
      "num_input_tokens_seen": 26662469,
      "step": 7872,
      "train_runtime": 2083.9397,
      "train_tokens_per_second": 12794.261
    },
    {
      "epoch": 1.7850488397041386,
      "grad_norm": 11.6057767868042,
      "learning_rate": 3.386405900691031e-05,
      "loss": 9.0698,
      "num_input_tokens_seen": 26769061,
      "step": 7904,
      "train_runtime": 2091.9129,
      "train_tokens_per_second": 12796.451
    },
    {
      "epoch": 1.7922759866749478,
      "grad_norm": 12.140463829040527,
      "learning_rate": 3.3716456301713584e-05,
      "loss": 9.1421,
      "num_input_tokens_seen": 26876613,
      "step": 7936,
      "train_runtime": 2099.8914,
      "train_tokens_per_second": 12799.049
    },
    {
      "epoch": 1.7995031336457568,
      "grad_norm": 12.483891487121582,
      "learning_rate": 3.356850689033475e-05,
      "loss": 9.0672,
      "num_input_tokens_seen": 26986117,
      "step": 7968,
      "train_runtime": 2107.8769,
      "train_tokens_per_second": 12802.511
    },
    {
      "epoch": 1.806730280616566,
      "grad_norm": 12.097796440124512,
      "learning_rate": 3.3420216657616674e-05,
      "loss": 9.2086,
      "num_input_tokens_seen": 27096037,
      "step": 8000,
      "train_runtime": 2115.8737,
      "train_tokens_per_second": 12806.075
    },
    {
      "epoch": 1.813957427587375,
      "grad_norm": 11.923651695251465,
      "learning_rate": 3.3271591501958735e-05,
      "loss": 9.3454,
      "num_input_tokens_seen": 27208869,
      "step": 8032,
      "train_runtime": 2123.8846,
      "train_tokens_per_second": 12810.898
    },
    {
      "epoch": 1.8211845745581843,
      "grad_norm": 11.663363456726074,
      "learning_rate": 3.31226373350822e-05,
      "loss": 9.1649,
      "num_input_tokens_seen": 27316933,
      "step": 8064,
      "train_runtime": 2131.8728,
      "train_tokens_per_second": 12813.585
    },
    {
      "epoch": 1.8284117215289932,
      "grad_norm": 11.398730278015137,
      "learning_rate": 3.297336008179515e-05,
      "loss": 9.0687,
      "num_input_tokens_seen": 27424581,
      "step": 8096,
      "train_runtime": 2139.8543,
      "train_tokens_per_second": 12816.097
    },
    {
      "epoch": 1.8356388684998024,
      "grad_norm": 11.945693969726562,
      "learning_rate": 3.282376567975672e-05,
      "loss": 9.1665,
      "num_input_tokens_seen": 27530757,
      "step": 8128,
      "train_runtime": 2147.8218,
      "train_tokens_per_second": 12817.99
    },
    {
      "epoch": 1.8428660154706114,
      "grad_norm": 11.525486946105957,
      "learning_rate": 3.2673860079241006e-05,
      "loss": 9.301,
      "num_input_tokens_seen": 27640517,
      "step": 8160,
      "train_runtime": 2155.8151,
      "train_tokens_per_second": 12821.377
    },
    {
      "epoch": 1.8500931624414205,
      "grad_norm": 11.457229614257812,
      "learning_rate": 3.252364924290032e-05,
      "loss": 9.1605,
      "num_input_tokens_seen": 27746053,
      "step": 8192,
      "train_runtime": 2163.7898,
      "train_tokens_per_second": 12822.896
    },
    {
      "epoch": 1.8573203094122297,
      "grad_norm": 12.059234619140625,
      "learning_rate": 3.237313914552808e-05,
      "loss": 9.0732,
      "num_input_tokens_seen": 27854821,
      "step": 8224,
      "train_runtime": 2171.7861,
      "train_tokens_per_second": 12825.767
    },
    {
      "epoch": 1.8645474563830389,
      "grad_norm": 11.73699951171875,
      "learning_rate": 3.222233577382108e-05,
      "loss": 9.0752,
      "num_input_tokens_seen": 27963557,
      "step": 8256,
      "train_runtime": 2179.7677,
      "train_tokens_per_second": 12828.687
    },
    {
      "epoch": 1.871774603353848,
      "grad_norm": 11.466715812683105,
      "learning_rate": 3.2071245126141456e-05,
      "loss": 9.1722,
      "num_input_tokens_seen": 28071301,
      "step": 8288,
      "train_runtime": 2187.7549,
      "train_tokens_per_second": 12831.1
    },
    {
      "epoch": 1.879001750324657,
      "grad_norm": 11.801844596862793,
      "learning_rate": 3.191987321227801e-05,
      "loss": 9.127,
      "num_input_tokens_seen": 28178693,
      "step": 8320,
      "train_runtime": 2195.7265,
      "train_tokens_per_second": 12833.425
    },
    {
      "epoch": 1.886228897295466,
      "grad_norm": 11.67275619506836,
      "learning_rate": 3.176822605320723e-05,
      "loss": 9.114,
      "num_input_tokens_seen": 28287013,
      "step": 8352,
      "train_runtime": 2203.7152,
      "train_tokens_per_second": 12836.057
    },
    {
      "epoch": 1.8934560442662751,
      "grad_norm": 12.060771942138672,
      "learning_rate": 3.161630968085374e-05,
      "loss": 9.1625,
      "num_input_tokens_seen": 28396549,
      "step": 8384,
      "train_runtime": 2211.71,
      "train_tokens_per_second": 12839.183
    },
    {
      "epoch": 1.9006831912370843,
      "grad_norm": 11.749178886413574,
      "learning_rate": 3.146413013785044e-05,
      "loss": 9.0637,
      "num_input_tokens_seen": 28502213,
      "step": 8416,
      "train_runtime": 2219.684,
      "train_tokens_per_second": 12840.662
    },
    {
      "epoch": 1.9079103382078935,
      "grad_norm": 12.020139694213867,
      "learning_rate": 3.131169347729809e-05,
      "loss": 9.1964,
      "num_input_tokens_seen": 28615461,
      "step": 8448,
      "train_runtime": 2227.7015,
      "train_tokens_per_second": 12845.285
    },
    {
      "epoch": 1.9151374851787026,
      "grad_norm": 11.57796573638916,
      "learning_rate": 3.1159005762524577e-05,
      "loss": 9.0872,
      "num_input_tokens_seen": 28721573,
      "step": 8480,
      "train_runtime": 2235.6729,
      "train_tokens_per_second": 12846.948
    },
    {
      "epoch": 1.9223646321495116,
      "grad_norm": 11.067450523376465,
      "learning_rate": 3.100607306684374e-05,
      "loss": 9.1697,
      "num_input_tokens_seen": 28830629,
      "step": 8512,
      "train_runtime": 2243.6655,
      "train_tokens_per_second": 12849.789
    },
    {
      "epoch": 1.9295917791203205,
      "grad_norm": 11.876134872436523,
      "learning_rate": 3.085290147331379e-05,
      "loss": 9.2547,
      "num_input_tokens_seen": 28938629,
      "step": 8544,
      "train_runtime": 2251.6599,
      "train_tokens_per_second": 12852.131
    },
    {
      "epoch": 1.9368189260911297,
      "grad_norm": 10.981074333190918,
      "learning_rate": 3.069949707449534e-05,
      "loss": 8.9989,
      "num_input_tokens_seen": 29046213,
      "step": 8576,
      "train_runtime": 2259.6424,
      "train_tokens_per_second": 12854.34
    },
    {
      "epoch": 1.9440460730619389,
      "grad_norm": 11.765101432800293,
      "learning_rate": 3.0545865972209075e-05,
      "loss": 9.1878,
      "num_input_tokens_seen": 29158437,
      "step": 8608,
      "train_runtime": 2267.6454,
      "train_tokens_per_second": 12858.464
    },
    {
      "epoch": 1.951273220032748,
      "grad_norm": 11.922021865844727,
      "learning_rate": 3.0392014277293072e-05,
      "loss": 9.0097,
      "num_input_tokens_seen": 29265701,
      "step": 8640,
      "train_runtime": 2275.6268,
      "train_tokens_per_second": 12860.501
    },
    {
      "epoch": 1.9585003670035572,
      "grad_norm": 12.406929016113281,
      "learning_rate": 3.0237948109359694e-05,
      "loss": 9.2445,
      "num_input_tokens_seen": 29375813,
      "step": 8672,
      "train_runtime": 2283.622,
      "train_tokens_per_second": 12863.694
    },
    {
      "epoch": 1.9657275139743662,
      "grad_norm": 11.59560775756836,
      "learning_rate": 3.0083673596552177e-05,
      "loss": 9.2006,
      "num_input_tokens_seen": 29486213,
      "step": 8704,
      "train_runtime": 2291.6165,
      "train_tokens_per_second": 12866.993
    },
    {
      "epoch": 1.9729546609451754,
      "grad_norm": 12.328838348388672,
      "learning_rate": 2.9929196875300914e-05,
      "loss": 9.0732,
      "num_input_tokens_seen": 29595045,
      "step": 8736,
      "train_runtime": 2299.5955,
      "train_tokens_per_second": 12869.674
    },
    {
      "epoch": 1.9801818079159843,
      "grad_norm": 11.69501781463623,
      "learning_rate": 2.977452409007936e-05,
      "loss": 9.0743,
      "num_input_tokens_seen": 29704869,
      "step": 8768,
      "train_runtime": 2307.597,
      "train_tokens_per_second": 12872.642
    },
    {
      "epoch": 1.9874089548867935,
      "grad_norm": 12.257323265075684,
      "learning_rate": 2.96196613931596e-05,
      "loss": 9.1537,
      "num_input_tokens_seen": 29813349,
      "step": 8800,
      "train_runtime": 2315.6099,
      "train_tokens_per_second": 12874.945
    },
    {
      "epoch": 1.9946361018576027,
      "grad_norm": 11.328304290771484,
      "learning_rate": 2.9464614944367652e-05,
      "loss": 9.1515,
      "num_input_tokens_seen": 29922565,
      "step": 8832,
      "train_runtime": 2323.6269,
      "train_tokens_per_second": 12877.526
    },
    {
      "epoch": 2.0,
      "eval_exact_token_accuracy": 0.001560130389407277,
      "eval_loss": 2.368617057800293,
      "eval_prediction_target_similarity": 0.006772580283997492,
      "eval_runtime": 38.0826,
      "eval_samples_per_second": 656.731,
      "eval_sequence_accuracy": 0.0,
      "eval_steps_per_second": 20.534,
      "eval_top3_token_accuracy": 0.010574848391115665,
      "eval_top5_token_accuracy": 0.022890767082571983,
      "num_input_tokens_seen": 29999703,
      "step": 8856
    },
    {
      "epoch": 2.0018067867427023,
      "grad_norm": 11.420088768005371,
      "learning_rate": 2.930939091083848e-05,
      "loss": 8.8837,
      "num_input_tokens_seen": 30026679,
      "step": 8864,
      "train_runtime": 2371.3765,
      "train_tokens_per_second": 12662.131
    },
    {
      "epoch": 2.0090339337135115,
      "grad_norm": 12.052112579345703,
      "learning_rate": 2.9153995466770643e-05,
      "loss": 8.5013,
      "num_input_tokens_seen": 30135543,
      "step": 8896,
      "train_runtime": 2379.3266,
      "train_tokens_per_second": 12665.576
    },
    {
      "epoch": 2.0162610806843206,
      "grad_norm": 12.40384292602539,
      "learning_rate": 2.8998434793180767e-05,
      "loss": 8.5487,
      "num_input_tokens_seen": 30244919,
      "step": 8928,
      "train_runtime": 2387.2932,
      "train_tokens_per_second": 12669.126
    },
    {
      "epoch": 2.0234882276551294,
      "grad_norm": 12.92469310760498,
      "learning_rate": 2.8842715077657605e-05,
      "loss": 8.542,
      "num_input_tokens_seen": 30353751,
      "step": 8960,
      "train_runtime": 2395.26,
      "train_tokens_per_second": 12672.424
    },
    {
      "epoch": 2.0307153746259385,
      "grad_norm": 12.889348030090332,
      "learning_rate": 2.868684251411602e-05,
      "loss": 8.5923,
      "num_input_tokens_seen": 30463703,
      "step": 8992,
      "train_runtime": 2403.224,
      "train_tokens_per_second": 12676.181
    },
    {
      "epoch": 2.0379425215967477,
      "grad_norm": 12.52094554901123,
      "learning_rate": 2.8530823302550524e-05,
      "loss": 8.5635,
      "num_input_tokens_seen": 30571479,
      "step": 9024,
      "train_runtime": 2411.1575,
      "train_tokens_per_second": 12679.171
    },
    {
      "epoch": 2.045169668567557,
      "grad_norm": 12.712393760681152,
      "learning_rate": 2.8374663648788745e-05,
      "loss": 8.4725,
      "num_input_tokens_seen": 30676887,
      "step": 9056,
      "train_runtime": 2419.0677,
      "train_tokens_per_second": 12681.285
    },
    {
      "epoch": 2.052396815538366,
      "grad_norm": 14.137685775756836,
      "learning_rate": 2.8218369764244512e-05,
      "loss": 8.5282,
      "num_input_tokens_seen": 30784503,
      "step": 9088,
      "train_runtime": 2427.0131,
      "train_tokens_per_second": 12684.111
    },
    {
      "epoch": 2.0596239625091752,
      "grad_norm": 12.307132720947266,
      "learning_rate": 2.8061947865670845e-05,
      "loss": 8.4796,
      "num_input_tokens_seen": 30891255,
      "step": 9120,
      "train_runtime": 2434.9482,
      "train_tokens_per_second": 12686.617
    },
    {
      "epoch": 2.0668511094799844,
      "grad_norm": 13.42442798614502,
      "learning_rate": 2.790540417491266e-05,
      "loss": 8.6782,
      "num_input_tokens_seen": 30998743,
      "step": 9152,
      "train_runtime": 2442.891,
      "train_tokens_per_second": 12689.368
    },
    {
      "epoch": 2.074078256450793,
      "grad_norm": 13.211610794067383,
      "learning_rate": 2.774874491865927e-05,
      "loss": 8.5238,
      "num_input_tokens_seen": 31110615,
      "step": 9184,
      "train_runtime": 2450.8681,
      "train_tokens_per_second": 12693.712
    },
    {
      "epoch": 2.0813054034216023,
      "grad_norm": 12.80965805053711,
      "learning_rate": 2.759197632819673e-05,
      "loss": 8.6207,
      "num_input_tokens_seen": 31217335,
      "step": 9216,
      "train_runtime": 2458.8072,
      "train_tokens_per_second": 12696.13
    },
    {
      "epoch": 2.0885325503924115,
      "grad_norm": 13.08047866821289,
      "learning_rate": 2.743510463915998e-05,
      "loss": 8.5427,
      "num_input_tokens_seen": 31322647,
      "step": 9248,
      "train_runtime": 2466.7339,
      "train_tokens_per_second": 12698.024
    },
    {
      "epoch": 2.0957596973632207,
      "grad_norm": 12.966691017150879,
      "learning_rate": 2.7278136091284823e-05,
      "loss": 8.5369,
      "num_input_tokens_seen": 31428791,
      "step": 9280,
      "train_runtime": 2474.6636,
      "train_tokens_per_second": 12700.228
    },
    {
      "epoch": 2.10298684433403,
      "grad_norm": 12.955879211425781,
      "learning_rate": 2.712107692815971e-05,
      "loss": 8.5563,
      "num_input_tokens_seen": 31538231,
      "step": 9312,
      "train_runtime": 2482.6492,
      "train_tokens_per_second": 12703.458
    },
    {
      "epoch": 2.110213991304839,
      "grad_norm": 13.605575561523438,
      "learning_rate": 2.696393339697742e-05,
      "loss": 8.4743,
      "num_input_tokens_seen": 31645207,
      "step": 9344,
      "train_runtime": 2490.5787,
      "train_tokens_per_second": 12705.966
    },
    {
      "epoch": 2.1174411382756477,
      "grad_norm": 13.237788200378418,
      "learning_rate": 2.680671174828654e-05,
      "loss": 8.5807,
      "num_input_tokens_seen": 31752343,
      "step": 9376,
      "train_runtime": 2498.5035,
      "train_tokens_per_second": 12708.544
    },
    {
      "epoch": 2.124668285246457,
      "grad_norm": 12.788183212280273,
      "learning_rate": 2.6649418235742878e-05,
      "loss": 8.4745,
      "num_input_tokens_seen": 31860535,
      "step": 9408,
      "train_runtime": 2506.477,
      "train_tokens_per_second": 12711.282
    },
    {
      "epoch": 2.131895432217266,
      "grad_norm": 13.619014739990234,
      "learning_rate": 2.6492059115860712e-05,
      "loss": 8.6759,
      "num_input_tokens_seen": 31969623,
      "step": 9440,
      "train_runtime": 2514.468,
      "train_tokens_per_second": 12714.269
    },
    {
      "epoch": 2.1391225791880752,
      "grad_norm": 13.0129976272583,
      "learning_rate": 2.6334640647763903e-05,
      "loss": 8.4934,
      "num_input_tokens_seen": 32077239,
      "step": 9472,
      "train_runtime": 2522.4105,
      "train_tokens_per_second": 12716.899
    },
    {
      "epoch": 2.1463497261588844,
      "grad_norm": 13.350095748901367,
      "learning_rate": 2.617716909293695e-05,
      "loss": 8.5095,
      "num_input_tokens_seen": 32184823,
      "step": 9504,
      "train_runtime": 2530.3545,
      "train_tokens_per_second": 12719.492
    },
    {
      "epoch": 2.1535768731296936,
      "grad_norm": 13.572299003601074,
      "learning_rate": 2.601965071497594e-05,
      "loss": 8.5252,
      "num_input_tokens_seen": 32299223,
      "step": 9536,
      "train_runtime": 2538.3337,
      "train_tokens_per_second": 12724.577
    },
    {
      "epoch": 2.1608040201005023,
      "grad_norm": 13.185602188110352,
      "learning_rate": 2.586209177933941e-05,
      "loss": 8.4428,
      "num_input_tokens_seen": 32402455,
      "step": 9568,
      "train_runtime": 2546.2504,
      "train_tokens_per_second": 12725.557
    },
    {
      "epoch": 2.1680311670713115,
      "grad_norm": 13.621606826782227,
      "learning_rate": 2.5704498553099086e-05,
      "loss": 8.4567,
      "num_input_tokens_seen": 32512215,
      "step": 9600,
      "train_runtime": 2554.2181,
      "train_tokens_per_second": 12728.833
    },
    {
      "epoch": 2.1752583140421207,
      "grad_norm": 13.899910926818848,
      "learning_rate": 2.5546877304690675e-05,
      "loss": 8.6408,
      "num_input_tokens_seen": 32619415,
      "step": 9632,
      "train_runtime": 2562.1843,
      "train_tokens_per_second": 12731.096
    },
    {
      "epoch": 2.18248546101293,
      "grad_norm": 12.950215339660645,
      "learning_rate": 2.5389234303664477e-05,
      "loss": 8.4548,
      "num_input_tokens_seen": 32724663,
      "step": 9664,
      "train_runtime": 2570.1281,
      "train_tokens_per_second": 12732.697
    },
    {
      "epoch": 2.189712607983739,
      "grad_norm": 12.629233360290527,
      "learning_rate": 2.5231575820436037e-05,
      "loss": 8.5001,
      "num_input_tokens_seen": 32832631,
      "step": 9696,
      "train_runtime": 2578.0887,
      "train_tokens_per_second": 12735.261
    },
    {
      "epoch": 2.196939754954548,
      "grad_norm": 12.966554641723633,
      "learning_rate": 2.50739081260367e-05,
      "loss": 8.4603,
      "num_input_tokens_seen": 32940695,
      "step": 9728,
      "train_runtime": 2586.0529,
      "train_tokens_per_second": 12737.827
    },
    {
      "epoch": 2.2041669019253574,
      "grad_norm": 14.466899871826172,
      "learning_rate": 2.49162374918642e-05,
      "loss": 8.64,
      "num_input_tokens_seen": 33047191,
      "step": 9760,
      "train_runtime": 2594.0051,
      "train_tokens_per_second": 12739.833
    },
    {
      "epoch": 2.211394048896166,
      "grad_norm": 12.524877548217773,
      "learning_rate": 2.475857018943324e-05,
      "loss": 8.4908,
      "num_input_tokens_seen": 33153559,
      "step": 9792,
      "train_runtime": 2601.9522,
      "train_tokens_per_second": 12741.802
    },
    {
      "epoch": 2.2186211958669753,
      "grad_norm": 13.883596420288086,
      "learning_rate": 2.4600912490125945e-05,
      "loss": 8.6467,
      "num_input_tokens_seen": 33261175,
      "step": 9824,
      "train_runtime": 2609.9463,
      "train_tokens_per_second": 12744.007
    },
    {
      "epoch": 2.2258483428377844,
      "grad_norm": 13.08034896850586,
      "learning_rate": 2.444327066494251e-05,
      "loss": 8.5561,
      "num_input_tokens_seen": 33369783,
      "step": 9856,
      "train_runtime": 2617.9165,
      "train_tokens_per_second": 12746.695
    },
    {
      "epoch": 2.2330754898085936,
      "grad_norm": 13.072915077209473,
      "learning_rate": 2.428565098425169e-05,
      "loss": 8.6015,
      "num_input_tokens_seen": 33478551,
      "step": 9888,
      "train_runtime": 2625.8754,
      "train_tokens_per_second": 12749.482
    },
    {
      "epoch": 2.240302636779403,
      "grad_norm": 14.253475189208984,
      "learning_rate": 2.4128059717541418e-05,
      "loss": 8.4578,
      "num_input_tokens_seen": 33588631,
      "step": 9920,
      "train_runtime": 2633.8525,
      "train_tokens_per_second": 12752.662
    },
    {
      "epoch": 2.2475297837502115,
      "grad_norm": 12.848788261413574,
      "learning_rate": 2.3970503133169475e-05,
      "loss": 8.4616,
      "num_input_tokens_seen": 33693783,
      "step": 9952,
      "train_runtime": 2641.7954,
      "train_tokens_per_second": 12754.123
    },
    {
      "epoch": 2.2547569307210207,
      "grad_norm": 13.626008987426758,
      "learning_rate": 2.381298749811406e-05,
      "loss": 8.6222,
      "num_input_tokens_seen": 33803927,
      "step": 9984,
      "train_runtime": 2649.7797,
      "train_tokens_per_second": 12757.259
    },
    {
      "epoch": 2.26198407769183,
      "grad_norm": 12.758528709411621,
      "learning_rate": 2.3655519077724613e-05,
      "loss": 8.425,
      "num_input_tokens_seen": 33910519,
      "step": 10016,
      "train_runtime": 2657.7364,
      "train_tokens_per_second": 12759.173
    },
    {
      "epoch": 2.269211224662639,
      "grad_norm": 13.3062105178833,
      "learning_rate": 2.3498104135472527e-05,
      "loss": 8.4653,
      "num_input_tokens_seen": 34016439,
      "step": 10048,
      "train_runtime": 2665.6703,
      "train_tokens_per_second": 12760.933
    },
    {
      "epoch": 2.276438371633448,
      "grad_norm": 13.160040855407715,
      "learning_rate": 2.334074893270208e-05,
      "loss": 8.6559,
      "num_input_tokens_seen": 34125879,
      "step": 10080,
      "train_runtime": 2673.6417,
      "train_tokens_per_second": 12763.819
    },
    {
      "epoch": 2.2836655186042574,
      "grad_norm": 13.150819778442383,
      "learning_rate": 2.3183459728381308e-05,
      "loss": 8.6379,
      "num_input_tokens_seen": 34235447,
      "step": 10112,
      "train_runtime": 2681.6113,
      "train_tokens_per_second": 12766.745
    },
    {
      "epoch": 2.2908926655750665,
      "grad_norm": 13.510260581970215,
      "learning_rate": 2.3026242778853132e-05,
      "loss": 8.5794,
      "num_input_tokens_seen": 34346071,
      "step": 10144,
      "train_runtime": 2689.5929,
      "train_tokens_per_second": 12769.988
    },
    {
      "epoch": 2.2981198125458753,
      "grad_norm": 13.787972450256348,
      "learning_rate": 2.286910433758644e-05,
      "loss": 8.6733,
      "num_input_tokens_seen": 34453431,
      "step": 10176,
      "train_runtime": 2697.5472,
      "train_tokens_per_second": 12772.133
    },
    {
      "epoch": 2.3053469595166844,
      "grad_norm": 13.77298641204834,
      "learning_rate": 2.2712050654927375e-05,
      "loss": 8.5268,
      "num_input_tokens_seen": 34561559,
      "step": 10208,
      "train_runtime": 2705.5111,
      "train_tokens_per_second": 12774.503
    },
    {
      "epoch": 2.3125741064874936,
      "grad_norm": 14.08443546295166,
      "learning_rate": 2.255508797785072e-05,
      "loss": 8.5199,
      "num_input_tokens_seen": 34671255,
      "step": 10240,
      "train_runtime": 2713.4893,
      "train_tokens_per_second": 12777.37
    },
    {
      "epoch": 2.319801253458303,
      "grad_norm": 13.418351173400879,
      "learning_rate": 2.2398222549711398e-05,
      "loss": 8.5322,
      "num_input_tokens_seen": 34776503,
      "step": 10272,
      "train_runtime": 2721.4313,
      "train_tokens_per_second": 12778.755
    },
    {
      "epoch": 2.327028400429112,
      "grad_norm": 13.73035717010498,
      "learning_rate": 2.2241460609996196e-05,
      "loss": 8.637,
      "num_input_tokens_seen": 34884471,
      "step": 10304,
      "train_runtime": 2729.3911,
      "train_tokens_per_second": 12781.045
    },
    {
      "epoch": 2.334255547399921,
      "grad_norm": 13.186112403869629,
      "learning_rate": 2.2084808394075493e-05,
      "loss": 8.5641,
      "num_input_tokens_seen": 34993719,
      "step": 10336,
      "train_runtime": 2737.332,
      "train_tokens_per_second": 12783.878
    },
    {
      "epoch": 2.34148269437073,
      "grad_norm": 13.19947338104248,
      "learning_rate": 2.1928272132955324e-05,
      "loss": 8.4481,
      "num_input_tokens_seen": 35101143,
      "step": 10368,
      "train_runtime": 2745.2997,
      "train_tokens_per_second": 12785.906
    },
    {
      "epoch": 2.348709841341539,
      "grad_norm": 13.604333877563477,
      "learning_rate": 2.1771858053029473e-05,
      "loss": 8.6155,
      "num_input_tokens_seen": 35208919,
      "step": 10400,
      "train_runtime": 2753.2622,
      "train_tokens_per_second": 12788.073
    },
    {
      "epoch": 2.355936988312348,
      "grad_norm": 13.544200897216797,
      "learning_rate": 2.161557237583187e-05,
      "loss": 8.5816,
      "num_input_tokens_seen": 35315543,
      "step": 10432,
      "train_runtime": 2761.201,
      "train_tokens_per_second": 12789.921
    },
    {
      "epoch": 2.3631641352831574,
      "grad_norm": 13.159677505493164,
      "learning_rate": 2.145942131778906e-05,
      "loss": 8.557,
      "num_input_tokens_seen": 35423639,
      "step": 10464,
      "train_runtime": 2769.1404,
      "train_tokens_per_second": 12792.287
    },
    {
      "epoch": 2.3703912822539666,
      "grad_norm": 13.073559761047363,
      "learning_rate": 2.1303411089972992e-05,
      "loss": 8.4899,
      "num_input_tokens_seen": 35532407,
      "step": 10496,
      "train_runtime": 2777.0919,
      "train_tokens_per_second": 12794.826
    },
    {
      "epoch": 2.3776184292247757,
      "grad_norm": 13.716588020324707,
      "learning_rate": 2.1147547897853924e-05,
      "loss": 8.4294,
      "num_input_tokens_seen": 35645079,
      "step": 10528,
      "train_runtime": 2785.0845,
      "train_tokens_per_second": 12798.563
    },
    {
      "epoch": 2.384845576195585,
      "grad_norm": 13.77188777923584,
      "learning_rate": 2.099183794105365e-05,
      "loss": 8.4673,
      "num_input_tokens_seen": 35754199,
      "step": 10560,
      "train_runtime": 2793.0654,
      "train_tokens_per_second": 12801.06
    },
    {
      "epoch": 2.3920727231663936,
      "grad_norm": 13.467292785644531,
      "learning_rate": 2.0836287413098836e-05,
      "loss": 8.5639,
      "num_input_tokens_seen": 35862327,
      "step": 10592,
      "train_runtime": 2801.0509,
      "train_tokens_per_second": 12803.168
    },
    {
      "epoch": 2.399299870137203,
      "grad_norm": 13.510778427124023,
      "learning_rate": 2.068090250117471e-05,
      "loss": 8.6739,
      "num_input_tokens_seen": 35970583,
      "step": 10624,
      "train_runtime": 2808.9683,
      "train_tokens_per_second": 12805.621
    },
    {
      "epoch": 2.406527017108012,
      "grad_norm": 13.574424743652344,
      "learning_rate": 2.0525689385878953e-05,
      "loss": 8.5337,
      "num_input_tokens_seen": 36080983,
      "step": 10656,
      "train_runtime": 2816.9086,
      "train_tokens_per_second": 12808.716
    },
    {
      "epoch": 2.413754164078821,
      "grad_norm": 13.423269271850586,
      "learning_rate": 2.0370654240975838e-05,
      "loss": 8.6349,
      "num_input_tokens_seen": 36190231,
      "step": 10688,
      "train_runtime": 2824.8504,
      "train_tokens_per_second": 12811.38
    },
    {
      "epoch": 2.4209813110496303,
      "grad_norm": 13.490918159484863,
      "learning_rate": 2.021580323315071e-05,
      "loss": 8.6286,
      "num_input_tokens_seen": 36299287,
      "step": 10720,
      "train_runtime": 2832.795,
      "train_tokens_per_second": 12813.948
    },
    {
      "epoch": 2.428208458020439,
      "grad_norm": 13.954465866088867,
      "learning_rate": 2.006114252176464e-05,
      "loss": 8.454,
      "num_input_tokens_seen": 36405591,
      "step": 10752,
      "train_runtime": 2840.7198,
      "train_tokens_per_second": 12815.622
    },
    {
      "epoch": 2.4354356049912482,
      "grad_norm": 12.629215240478516,
      "learning_rate": 1.990667825860948e-05,
      "loss": 8.5508,
      "num_input_tokens_seen": 36513655,
      "step": 10784,
      "train_runtime": 2848.6391,
      "train_tokens_per_second": 12817.929
    },
    {
      "epoch": 2.4426627519620574,
      "grad_norm": 13.557693481445312,
      "learning_rate": 1.9752416587663115e-05,
      "loss": 8.5642,
      "num_input_tokens_seen": 36622103,
      "step": 10816,
      "train_runtime": 2856.5588,
      "train_tokens_per_second": 12820.357
    },
    {
      "epoch": 2.4498898989328666,
      "grad_norm": 14.151754379272461,
      "learning_rate": 1.9598363644845167e-05,
      "loss": 8.6102,
      "num_input_tokens_seen": 36733879,
      "step": 10848,
      "train_runtime": 2864.5108,
      "train_tokens_per_second": 12823.788
    },
    {
      "epoch": 2.4571170459036757,
      "grad_norm": 13.095513343811035,
      "learning_rate": 1.9444525557772814e-05,
      "loss": 8.5438,
      "num_input_tokens_seen": 36845207,
      "step": 10880,
      "train_runtime": 2872.4492,
      "train_tokens_per_second": 12827.105
    },
    {
      "epoch": 2.464344192874485,
      "grad_norm": 13.664118766784668,
      "learning_rate": 1.929090844551718e-05,
      "loss": 8.6122,
      "num_input_tokens_seen": 36952183,
      "step": 10912,
      "train_runtime": 2880.3578,
      "train_tokens_per_second": 12829.025
    },
    {
      "epoch": 2.471571339845294,
      "grad_norm": 13.425056457519531,
      "learning_rate": 1.913751841835985e-05,
      "loss": 8.5768,
      "num_input_tokens_seen": 37062743,
      "step": 10944,
      "train_runtime": 2888.2851,
      "train_tokens_per_second": 12832.093
    },
    {
      "epoch": 2.478798486816103,
      "grad_norm": 12.86414623260498,
      "learning_rate": 1.8984361577549856e-05,
      "loss": 8.4736,
      "num_input_tokens_seen": 37172119,
      "step": 10976,
      "train_runtime": 2896.2132,
      "train_tokens_per_second": 12834.732
    },
    {
      "epoch": 2.486025633786912,
      "grad_norm": 13.916574478149414,
      "learning_rate": 1.883144401506103e-05,
      "loss": 8.4968,
      "num_input_tokens_seen": 37279159,
      "step": 11008,
      "train_runtime": 2904.1396,
      "train_tokens_per_second": 12836.559
    },
    {
      "epoch": 2.493252780757721,
      "grad_norm": 13.571022987365723,
      "learning_rate": 1.8678771813349617e-05,
      "loss": 8.5402,
      "num_input_tokens_seen": 37388343,
      "step": 11040,
      "train_runtime": 2912.0796,
      "train_tokens_per_second": 12839.052
    },
    {
      "epoch": 2.5000282310428545,
      "eval_exact_token_accuracy": 0.0017881850944831967,
      "eval_loss": 2.370223045349121,
      "eval_prediction_target_similarity": 0.006462153743663915,
      "eval_runtime": 37.8596,
      "eval_samples_per_second": 660.599,
      "eval_sequence_accuracy": 0.0,
      "eval_steps_per_second": 20.655,
      "eval_top3_token_accuracy": 0.010558952577412128,
      "eval_top5_token_accuracy": 0.022365815937519073,
      "num_input_tokens_seen": 37489079,
      "step": 11070
    },
    {
      "epoch": 2.5004799277285303,
      "grad_norm": 13.098929405212402,
      "learning_rate": 1.8526351045112435e-05,
      "loss": 8.6279,
      "num_input_tokens_seen": 37496247,
      "step": 11072,
      "train_runtime": 2958.7332,
      "train_tokens_per_second": 12673.075
    },
    {
      "epoch": 2.5077070746993395,
      "grad_norm": 14.002643585205078,
      "learning_rate": 1.837418777304521e-05,
      "loss": 8.5427,
      "num_input_tokens_seen": 37605015,
      "step": 11104,
      "train_runtime": 2966.7075,
      "train_tokens_per_second": 12675.673
    },
    {
      "epoch": 2.5149342216701482,
      "grad_norm": 13.377714157104492,
      "learning_rate": 1.8222288049601545e-05,
      "loss": 8.445,
      "num_input_tokens_seen": 37714679,
      "step": 11136,
      "train_runtime": 2974.7059,
      "train_tokens_per_second": 12678.457
    },
    {
      "epoch": 2.5221613686409574,
      "grad_norm": 13.333240509033203,
      "learning_rate": 1.8070657916752062e-05,
      "loss": 8.6028,
      "num_input_tokens_seen": 37825687,
      "step": 11168,
      "train_runtime": 2982.7253,
      "train_tokens_per_second": 12681.586
    },
    {
      "epoch": 2.5293885156117666,
      "grad_norm": 13.7440185546875,
      "learning_rate": 1.79193034057442e-05,
      "loss": 8.6492,
      "num_input_tokens_seen": 37933303,
      "step": 11200,
      "train_runtime": 2990.6915,
      "train_tokens_per_second": 12683.79
    },
    {
      "epoch": 2.5366156625825758,
      "grad_norm": 12.963172912597656,
      "learning_rate": 1.7768230536862183e-05,
      "loss": 8.5553,
      "num_input_tokens_seen": 38041495,
      "step": 11232,
      "train_runtime": 2998.6572,
      "train_tokens_per_second": 12686.177
    },
    {
      "epoch": 2.543842809553385,
      "grad_norm": 13.8705472946167,
      "learning_rate": 1.761744531918769e-05,
      "loss": 8.5358,
      "num_input_tokens_seen": 38147127,
      "step": 11264,
      "train_runtime": 3006.6106,
      "train_tokens_per_second": 12687.751
    },
    {
      "epoch": 2.551069956524194,
      "grad_norm": 14.200339317321777,
      "learning_rate": 1.7466953750360727e-05,
      "loss": 8.5614,
      "num_input_tokens_seen": 38257943,
      "step": 11296,
      "train_runtime": 3014.5865,
      "train_tokens_per_second": 12690.942
    },
    {
      "epoch": 2.5582971034950033,
      "grad_norm": 13.69649600982666,
      "learning_rate": 1.7316761816341105e-05,
      "loss": 8.5361,
      "num_input_tokens_seen": 38367383,
      "step": 11328,
      "train_runtime": 3022.5435,
      "train_tokens_per_second": 12693.74
    },
    {
      "epoch": 2.5655242504658125,
      "grad_norm": 13.467666625976562,
      "learning_rate": 1.7166875491170386e-05,
      "loss": 8.5681,
      "num_input_tokens_seen": 38475831,
      "step": 11360,
      "train_runtime": 3030.4818,
      "train_tokens_per_second": 12696.275
    },
    {
      "epoch": 2.572751397436621,
      "grad_norm": 13.116236686706543,
      "learning_rate": 1.701730073673419e-05,
      "loss": 8.6152,
      "num_input_tokens_seen": 38583671,
      "step": 11392,
      "train_runtime": 3038.4115,
      "train_tokens_per_second": 12698.633
    },
    {
      "epoch": 2.5799785444074304,
      "grad_norm": 14.009608268737793,
      "learning_rate": 1.6868043502525097e-05,
      "loss": 8.5638,
      "num_input_tokens_seen": 38692599,
      "step": 11424,
      "train_runtime": 3046.3413,
      "train_tokens_per_second": 12701.334
    },
    {
      "epoch": 2.5872056913782395,
      "grad_norm": 13.690778732299805,
      "learning_rate": 1.6719109725405967e-05,
      "loss": 8.5498,
      "num_input_tokens_seen": 38804375,
      "step": 11456,
      "train_runtime": 3054.3051,
      "train_tokens_per_second": 12704.813
    },
    {
      "epoch": 2.5944328383490487,
      "grad_norm": 13.899653434753418,
      "learning_rate": 1.6570505329373854e-05,
      "loss": 8.5868,
      "num_input_tokens_seen": 38911895,
      "step": 11488,
      "train_runtime": 3062.2504,
      "train_tokens_per_second": 12706.961
    },
    {
      "epoch": 2.601659985319858,
      "grad_norm": 12.536357879638672,
      "learning_rate": 1.6422236225324288e-05,
      "loss": 8.7154,
      "num_input_tokens_seen": 39021623,
      "step": 11520,
      "train_runtime": 3070.1967,
      "train_tokens_per_second": 12709.812
    },
    {
      "epoch": 2.6088871322906666,
      "grad_norm": 13.021293640136719,
      "learning_rate": 1.6274308310816253e-05,
      "loss": 8.5421,
      "num_input_tokens_seen": 39129719,
      "step": 11552,
      "train_runtime": 3078.148,
      "train_tokens_per_second": 12712.098
    },
    {
      "epoch": 2.6161142792614758,
      "grad_norm": 13.82776165008545,
      "learning_rate": 1.6126727469837524e-05,
      "loss": 8.5999,
      "num_input_tokens_seen": 39238647,
      "step": 11584,
      "train_runtime": 3086.1038,
      "train_tokens_per_second": 12714.623
    },
    {
      "epoch": 2.623341426232285,
      "grad_norm": 14.197108268737793,
      "learning_rate": 1.597949957257069e-05,
      "loss": 8.5617,
      "num_input_tokens_seen": 39346519,
      "step": 11616,
      "train_runtime": 3094.0565,
      "train_tokens_per_second": 12716.807
    },
    {
      "epoch": 2.630568573203094,
      "grad_norm": 13.449432373046875,
      "learning_rate": 1.5832630475159622e-05,
      "loss": 8.5659,
      "num_input_tokens_seen": 39455063,
      "step": 11648,
      "train_runtime": 3102.0079,
      "train_tokens_per_second": 12719.201
    },
    {
      "epoch": 2.6377957201739033,
      "grad_norm": 13.917506217956543,
      "learning_rate": 1.5686126019476537e-05,
      "loss": 8.4212,
      "num_input_tokens_seen": 39564407,
      "step": 11680,
      "train_runtime": 3109.9643,
      "train_tokens_per_second": 12721.82
    },
    {
      "epoch": 2.6450228671447125,
      "grad_norm": 13.79491138458252,
      "learning_rate": 1.553999203288968e-05,
      "loss": 8.4537,
      "num_input_tokens_seen": 39674007,
      "step": 11712,
      "train_runtime": 3117.9342,
      "train_tokens_per_second": 12724.453
    },
    {
      "epoch": 2.6522500141155216,
      "grad_norm": 14.224446296691895,
      "learning_rate": 1.539423432803147e-05,
      "loss": 8.5356,
      "num_input_tokens_seen": 39781431,
      "step": 11744,
      "train_runtime": 3125.9513,
      "train_tokens_per_second": 12726.184
    },
    {
      "epoch": 2.6594771610863304,
      "grad_norm": 13.208168983459473,
      "learning_rate": 1.5248858702567342e-05,
      "loss": 8.5538,
      "num_input_tokens_seen": 39885783,
      "step": 11776,
      "train_runtime": 3133.8947,
      "train_tokens_per_second": 12727.225
    },
    {
      "epoch": 2.6667043080571395,
      "grad_norm": 13.081830978393555,
      "learning_rate": 1.5103870938965107e-05,
      "loss": 8.4196,
      "num_input_tokens_seen": 39992599,
      "step": 11808,
      "train_runtime": 3141.8353,
      "train_tokens_per_second": 12729.057
    },
    {
      "epoch": 2.6739314550279487,
      "grad_norm": 13.94918441772461,
      "learning_rate": 1.4959276804264989e-05,
      "loss": 8.4963,
      "num_input_tokens_seen": 40101655,
      "step": 11840,
      "train_runtime": 3149.7977,
      "train_tokens_per_second": 12731.502
    },
    {
      "epoch": 2.681158601998758,
      "grad_norm": 13.517632484436035,
      "learning_rate": 1.4815082049850177e-05,
      "loss": 8.4508,
      "num_input_tokens_seen": 40208791,
      "step": 11872,
      "train_runtime": 3157.7429,
      "train_tokens_per_second": 12733.396
    },
    {
      "epoch": 2.688385748969567,
      "grad_norm": 13.474502563476562,
      "learning_rate": 1.467129241121812e-05,
      "loss": 8.5499,
      "num_input_tokens_seen": 40320951,
      "step": 11904,
      "train_runtime": 3165.7174,
      "train_tokens_per_second": 12736.75
    },
    {
      "epoch": 2.695612895940376,
      "grad_norm": 12.745155334472656,
      "learning_rate": 1.452791360775234e-05,
      "loss": 8.4973,
      "num_input_tokens_seen": 40428535,
      "step": 11936,
      "train_runtime": 3173.6619,
      "train_tokens_per_second": 12738.766
    },
    {
      "epoch": 2.702840042911185,
      "grad_norm": 13.382939338684082,
      "learning_rate": 1.438495134249499e-05,
      "loss": 8.4714,
      "num_input_tokens_seen": 40538967,
      "step": 11968,
      "train_runtime": 3181.6335,
      "train_tokens_per_second": 12741.558
    },
    {
      "epoch": 2.710067189881994,
      "grad_norm": 13.226701736450195,
      "learning_rate": 1.424241130191995e-05,
      "loss": 8.5294,
      "num_input_tokens_seen": 40645687,
      "step": 12000,
      "train_runtime": 3189.5824,
      "train_tokens_per_second": 12743.263
    },
    {
      "epoch": 2.7172943368528033,
      "grad_norm": 13.796481132507324,
      "learning_rate": 1.4100299155706665e-05,
      "loss": 8.5052,
      "num_input_tokens_seen": 40752023,
      "step": 12032,
      "train_runtime": 3197.5288,
      "train_tokens_per_second": 12744.849
    },
    {
      "epoch": 2.7245214838236125,
      "grad_norm": 13.815497398376465,
      "learning_rate": 1.3958620556514673e-05,
      "loss": 8.6067,
      "num_input_tokens_seen": 40861751,
      "step": 12064,
      "train_runtime": 3205.4889,
      "train_tokens_per_second": 12747.432
    },
    {
      "epoch": 2.7317486307944217,
      "grad_norm": 14.228543281555176,
      "learning_rate": 1.3817381139758692e-05,
      "loss": 8.5658,
      "num_input_tokens_seen": 40972951,
      "step": 12096,
      "train_runtime": 3213.4638,
      "train_tokens_per_second": 12750.401
    },
    {
      "epoch": 2.738975777765231,
      "grad_norm": 13.383831977844238,
      "learning_rate": 1.3676586523384505e-05,
      "loss": 8.5139,
      "num_input_tokens_seen": 41083383,
      "step": 12128,
      "train_runtime": 3221.4329,
      "train_tokens_per_second": 12753.139
    },
    {
      "epoch": 2.74620292473604,
      "grad_norm": 13.46198844909668,
      "learning_rate": 1.3536242307645491e-05,
      "loss": 8.6059,
      "num_input_tokens_seen": 41194551,
      "step": 12160,
      "train_runtime": 3229.4016,
      "train_tokens_per_second": 12756.094
    },
    {
      "epoch": 2.7534300717068487,
      "grad_norm": 13.569401741027832,
      "learning_rate": 1.3396354074879891e-05,
      "loss": 8.557,
      "num_input_tokens_seen": 41305399,
      "step": 12192,
      "train_runtime": 3237.3744,
      "train_tokens_per_second": 12758.919
    },
    {
      "epoch": 2.760657218677658,
      "grad_norm": 13.476645469665527,
      "learning_rate": 1.3256927389288718e-05,
      "loss": 8.587,
      "num_input_tokens_seen": 41417207,
      "step": 12224,
      "train_runtime": 3245.3533,
      "train_tokens_per_second": 12762.002
    },
    {
      "epoch": 2.767884365648467,
      "grad_norm": 14.263786315917969,
      "learning_rate": 1.311796779671449e-05,
      "loss": 8.4979,
      "num_input_tokens_seen": 41524023,
      "step": 12256,
      "train_runtime": 3253.2995,
      "train_tokens_per_second": 12763.665
    },
    {
      "epoch": 2.7751115126192762,
      "grad_norm": 13.691316604614258,
      "learning_rate": 1.2979480824420581e-05,
      "loss": 8.4706,
      "num_input_tokens_seen": 41634199,
      "step": 12288,
      "train_runtime": 3261.2459,
      "train_tokens_per_second": 12766.348
    },
    {
      "epoch": 2.7823386595900854,
      "grad_norm": 13.260598182678223,
      "learning_rate": 1.2841471980871444e-05,
      "loss": 8.3192,
      "num_input_tokens_seen": 41741463,
      "step": 12320,
      "train_runtime": 3269.2101,
      "train_tokens_per_second": 12768.058
    },
    {
      "epoch": 2.789565806560894,
      "grad_norm": 13.375551223754883,
      "learning_rate": 1.2703946755513397e-05,
      "loss": 8.5045,
      "num_input_tokens_seen": 41847863,
      "step": 12352,
      "train_runtime": 3277.1529,
      "train_tokens_per_second": 12769.579
    },
    {
      "epoch": 2.7967929535317033,
      "grad_norm": 14.306396484375,
      "learning_rate": 1.2566910618556376e-05,
      "loss": 8.4138,
      "num_input_tokens_seen": 41958423,
      "step": 12384,
      "train_runtime": 3285.1311,
      "train_tokens_per_second": 12772.222
    },
    {
      "epoch": 2.8040201005025125,
      "grad_norm": 13.981322288513184,
      "learning_rate": 1.2430369020756328e-05,
      "loss": 8.5053,
      "num_input_tokens_seen": 42067191,
      "step": 12416,
      "train_runtime": 3293.0806,
      "train_tokens_per_second": 12774.419
    },
    {
      "epoch": 2.8112472474733217,
      "grad_norm": 14.225761413574219,
      "learning_rate": 1.2294327393198343e-05,
      "loss": 8.5036,
      "num_input_tokens_seen": 42174839,
      "step": 12448,
      "train_runtime": 3301.0427,
      "train_tokens_per_second": 12776.217
    },
    {
      "epoch": 2.818474394444131,
      "grad_norm": 13.931827545166016,
      "learning_rate": 1.2158791147080717e-05,
      "loss": 8.5652,
      "num_input_tokens_seen": 42283767,
      "step": 12480,
      "train_runtime": 3309.0035,
      "train_tokens_per_second": 12778.399
    },
    {
      "epoch": 2.82570154141494,
      "grad_norm": 14.179051399230957,
      "learning_rate": 1.2023765673499631e-05,
      "loss": 8.5105,
      "num_input_tokens_seen": 42391831,
      "step": 12512,
      "train_runtime": 3316.9604,
      "train_tokens_per_second": 12780.325
    },
    {
      "epoch": 2.832928688385749,
      "grad_norm": 13.204866409301758,
      "learning_rate": 1.1889256343234789e-05,
      "loss": 8.5679,
      "num_input_tokens_seen": 42499767,
      "step": 12544,
      "train_runtime": 3324.9035,
      "train_tokens_per_second": 12782.256
    },
    {
      "epoch": 2.840155835356558,
      "grad_norm": 13.789064407348633,
      "learning_rate": 1.1755268506535724e-05,
      "loss": 8.5324,
      "num_input_tokens_seen": 42606839,
      "step": 12576,
      "train_runtime": 3332.8421,
      "train_tokens_per_second": 12783.936
    },
    {
      "epoch": 2.847382982327367,
      "grad_norm": 13.938424110412598,
      "learning_rate": 1.1621807492909026e-05,
      "loss": 8.5222,
      "num_input_tokens_seen": 42715639,
      "step": 12608,
      "train_runtime": 3340.7791,
      "train_tokens_per_second": 12786.131
    },
    {
      "epoch": 2.8546101292981763,
      "grad_norm": 13.118377685546875,
      "learning_rate": 1.1488878610906337e-05,
      "loss": 8.4672,
      "num_input_tokens_seen": 42825815,
      "step": 12640,
      "train_runtime": 3348.7368,
      "train_tokens_per_second": 12788.648
    },
    {
      "epoch": 2.8618372762689854,
      "grad_norm": 14.505343437194824,
      "learning_rate": 1.1356487147913237e-05,
      "loss": 8.4154,
      "num_input_tokens_seen": 42929879,
      "step": 12672,
      "train_runtime": 3356.6698,
      "train_tokens_per_second": 12789.426
    },
    {
      "epoch": 2.8690644232397946,
      "grad_norm": 13.66861343383789,
      "learning_rate": 1.1224638369938866e-05,
      "loss": 8.5167,
      "num_input_tokens_seen": 43039511,
      "step": 12704,
      "train_runtime": 3364.6267,
      "train_tokens_per_second": 12791.764
    },
    {
      "epoch": 2.8762915702106033,
      "grad_norm": 13.850041389465332,
      "learning_rate": 1.1093337521406508e-05,
      "loss": 8.4075,
      "num_input_tokens_seen": 43146359,
      "step": 12736,
      "train_runtime": 3372.5678,
      "train_tokens_per_second": 12793.326
    },
    {
      "epoch": 2.8835187171814125,
      "grad_norm": 13.985395431518555,
      "learning_rate": 1.0962589824945002e-05,
      "loss": 8.5343,
      "num_input_tokens_seen": 43256151,
      "step": 12768,
      "train_runtime": 3380.5427,
      "train_tokens_per_second": 12795.623
    },
    {
      "epoch": 2.8907458641522217,
      "grad_norm": 13.54736042022705,
      "learning_rate": 1.0832400481180947e-05,
      "loss": 8.5117,
      "num_input_tokens_seen": 43362519,
      "step": 12800,
      "train_runtime": 3388.5031,
      "train_tokens_per_second": 12796.954
    },
    {
      "epoch": 2.897973011123031,
      "grad_norm": 13.510107040405273,
      "learning_rate": 1.0702774668531915e-05,
      "loss": 8.4659,
      "num_input_tokens_seen": 43470647,
      "step": 12832,
      "train_runtime": 3396.4588,
      "train_tokens_per_second": 12798.815
    },
    {
      "epoch": 2.90520015809384,
      "grad_norm": 14.258561134338379,
      "learning_rate": 1.0573717543000412e-05,
      "loss": 8.4882,
      "num_input_tokens_seen": 43579671,
      "step": 12864,
      "train_runtime": 3404.4182,
      "train_tokens_per_second": 12800.916
    },
    {
      "epoch": 2.912427305064649,
      "grad_norm": 13.675215721130371,
      "learning_rate": 1.0445234237968827e-05,
      "loss": 8.4731,
      "num_input_tokens_seen": 43686103,
      "step": 12896,
      "train_runtime": 3412.355,
      "train_tokens_per_second": 12802.333
    },
    {
      "epoch": 2.9196544520354584,
      "grad_norm": 13.426660537719727,
      "learning_rate": 1.0317329863995224e-05,
      "loss": 8.4863,
      "num_input_tokens_seen": 43793431,
      "step": 12928,
      "train_runtime": 3420.2828,
      "train_tokens_per_second": 12804.038
    },
    {
      "epoch": 2.9268815990062675,
      "grad_norm": 14.345488548278809,
      "learning_rate": 1.0190009508610112e-05,
      "loss": 8.4943,
      "num_input_tokens_seen": 43902999,
      "step": 12960,
      "train_runtime": 3428.2349,
      "train_tokens_per_second": 12806.298
    },
    {
      "epoch": 2.9341087459770763,
      "grad_norm": 13.953092575073242,
      "learning_rate": 1.0063278236114007e-05,
      "loss": 8.4909,
      "num_input_tokens_seen": 44010615,
      "step": 12992,
      "train_runtime": 3436.1772,
      "train_tokens_per_second": 12808.017
    },
    {
      "epoch": 2.9413358929478854,
      "grad_norm": 13.5239896774292,
      "learning_rate": 9.937141087376076e-06,
      "loss": 8.4719,
      "num_input_tokens_seen": 44117655,
      "step": 13024,
      "train_runtime": 3444.1073,
      "train_tokens_per_second": 12809.605
    },
    {
      "epoch": 2.9485630399186946,
      "grad_norm": 13.68096923828125,
      "learning_rate": 9.811603079633554e-06,
      "loss": 8.4849,
      "num_input_tokens_seen": 44227671,
      "step": 13056,
      "train_runtime": 3452.0631,
      "train_tokens_per_second": 12811.953
    },
    {
      "epoch": 2.955790186889504,
      "grad_norm": 13.848033905029297,
      "learning_rate": 9.686669206292242e-06,
      "loss": 8.5947,
      "num_input_tokens_seen": 44339031,
      "step": 13088,
      "train_runtime": 3460.0295,
      "train_tokens_per_second": 12814.639
    },
    {
      "epoch": 2.963017333860313,
      "grad_norm": 13.153359413146973,
      "learning_rate": 9.562344436727848e-06,
      "loss": 8.4627,
      "num_input_tokens_seen": 44446935,
      "step": 13120,
      "train_runtime": 3467.9676,
      "train_tokens_per_second": 12816.421
    },
    {
      "epoch": 2.9702444808311217,
      "grad_norm": 13.510297775268555,
      "learning_rate": 9.438633716088333e-06,
      "loss": 8.4989,
      "num_input_tokens_seen": 44552823,
      "step": 13152,
      "train_runtime": 3475.9092,
      "train_tokens_per_second": 12817.603
    },
    {
      "epoch": 2.977471627801931,
      "grad_norm": 13.829279899597168,
      "learning_rate": 9.315541965097246e-06,
      "loss": 8.3947,
      "num_input_tokens_seen": 44659063,
      "step": 13184,
      "train_runtime": 3483.8725,
      "train_tokens_per_second": 12818.799
    },
    {
      "epoch": 2.98469877477274,
      "grad_norm": 14.511972427368164,
      "learning_rate": 9.193074079857938e-06,
      "loss": 8.4275,
      "num_input_tokens_seen": 44765943,
      "step": 13216,
      "train_runtime": 3491.8267,
      "train_tokens_per_second": 12820.208
    },
    {
      "epoch": 2.991925921743549,
      "grad_norm": 13.184659957885742,
      "learning_rate": 9.071234931658876e-06,
      "loss": 8.5208,
      "num_input_tokens_seen": 44873815,
      "step": 13248,
      "train_runtime": 3499.7793,
      "train_tokens_per_second": 12821.898
    },
    {
      "epoch": 2.9991530687143584,
      "grad_norm": 13.39197826385498,
      "learning_rate": 8.95002936677982e-06,
      "loss": 8.3965,
      "num_input_tokens_seen": 44982647,
      "step": 13280,
      "train_runtime": 3507.7472,
      "train_tokens_per_second": 12823.8
    },
    {
      "epoch": 3.0,
      "eval_exact_token_accuracy": 0.0009546737419441342,
      "eval_loss": 2.3506674766540527,
      "eval_prediction_target_similarity": 0.004649256927772746,
      "eval_runtime": 38.0011,
      "eval_samples_per_second": 658.139,
      "eval_sequence_accuracy": 0.0,
      "eval_steps_per_second": 20.578,
      "eval_top3_token_accuracy": 0.007603425066918135,
      "eval_top5_token_accuracy": 0.016715914011001587,
      "num_input_tokens_seen": 44996656,
      "step": 13284
    },
    {
      "epoch": 3.006323753599458,
      "grad_norm": 14.361067771911621,
      "learning_rate": 8.829462206299125e-06,
      "loss": 8.0672,
      "num_input_tokens_seen": 45089264,
      "step": 13312,
      "train_runtime": 3555.3681,
      "train_tokens_per_second": 12682.024
    },
    {
      "epoch": 3.013550900570267,
      "grad_norm": 13.923966407775879,
      "learning_rate": 8.70953824590191e-06,
      "loss": 7.9783,
      "num_input_tokens_seen": 45199056,
      "step": 13344,
      "train_runtime": 3563.436,
      "train_tokens_per_second": 12684.122
    },
    {
      "epoch": 3.0207780475410764,
      "grad_norm": 13.826278686523438,
      "learning_rate": 8.590262255689357e-06,
      "loss": 7.9468,
      "num_input_tokens_seen": 45305200,
      "step": 13376,
      "train_runtime": 3571.4402,
      "train_tokens_per_second": 12685.415
    },
    {
      "epoch": 3.028005194511885,
      "grad_norm": 14.82114315032959,
      "learning_rate": 8.471638979988947e-06,
      "loss": 7.8879,
      "num_input_tokens_seen": 45411216,
      "step": 13408,
      "train_runtime": 3579.4082,
      "train_tokens_per_second": 12686.794
    },
    {
      "epoch": 3.0352323414826943,
      "grad_norm": 14.371086120605469,
      "learning_rate": 8.35367313716575e-06,
      "loss": 7.9775,
      "num_input_tokens_seen": 45517456,
      "step": 13440,
      "train_runtime": 3587.3857,
      "train_tokens_per_second": 12688.197
    },
    {
      "epoch": 3.0424594884535034,
      "grad_norm": 14.246870040893555,
      "learning_rate": 8.236369419434778e-06,
      "loss": 7.9476,
      "num_input_tokens_seen": 45621328,
      "step": 13472,
      "train_runtime": 3595.3436,
      "train_tokens_per_second": 12689.003
    },
    {
      "epoch": 3.0496866354243126,
      "grad_norm": 14.660003662109375,
      "learning_rate": 8.119732492674295e-06,
      "loss": 7.9518,
      "num_input_tokens_seen": 45729680,
      "step": 13504,
      "train_runtime": 3603.3215,
      "train_tokens_per_second": 12690.98
    },
    {
      "epoch": 3.056913782395122,
      "grad_norm": 14.382355690002441,
      "learning_rate": 8.00376699624029e-06,
      "loss": 7.8987,
      "num_input_tokens_seen": 45837360,
      "step": 13536,
      "train_runtime": 3611.3956,
      "train_tokens_per_second": 12692.423
    },
    {
      "epoch": 3.064140929365931,
      "grad_norm": 14.389463424682617,
      "learning_rate": 7.888477542781861e-06,
      "loss": 8.0439,
      "num_input_tokens_seen": 45947600,
      "step": 13568,
      "train_runtime": 3619.3991,
      "train_tokens_per_second": 12694.814
    },
    {
      "epoch": 3.0713680763367397,
      "grad_norm": 14.329747200012207,
      "learning_rate": 7.773868718057837e-06,
      "loss": 8.0063,
      "num_input_tokens_seen": 46058416,
      "step": 13600,
      "train_runtime": 3627.3975,
      "train_tokens_per_second": 12697.372
    },
    {
      "epoch": 3.078595223307549,
      "grad_norm": 14.411673545837402,
      "learning_rate": 7.65994508075429e-06,
      "loss": 8.0226,
      "num_input_tokens_seen": 46172208,
      "step": 13632,
      "train_runtime": 3635.4109,
      "train_tokens_per_second": 12700.685
    },
    {
      "epoch": 3.085822370278358,
      "grad_norm": 14.363574981689453,
      "learning_rate": 7.546711162303266e-06,
      "loss": 8.0435,
      "num_input_tokens_seen": 46278512,
      "step": 13664,
      "train_runtime": 3643.3786,
      "train_tokens_per_second": 12702.087
    },
    {
      "epoch": 3.093049517249167,
      "grad_norm": 14.501341819763184,
      "learning_rate": 7.4341714667025e-06,
      "loss": 7.9929,
      "num_input_tokens_seen": 46386704,
      "step": 13696,
      "train_runtime": 3651.3661,
      "train_tokens_per_second": 12703.931
    },
    {
      "epoch": 3.1002766642199764,
      "grad_norm": 14.917803764343262,
      "learning_rate": 7.3223304703363135e-06,
      "loss": 7.9328,
      "num_input_tokens_seen": 46492976,
      "step": 13728,
      "train_runtime": 3659.3389,
      "train_tokens_per_second": 12705.294
    },
    {
      "epoch": 3.1075038111907856,
      "grad_norm": 14.189013481140137,
      "learning_rate": 7.211192621797511e-06,
      "loss": 8.0279,
      "num_input_tokens_seen": 46599472,
      "step": 13760,
      "train_runtime": 3667.4048,
      "train_tokens_per_second": 12706.389
    },
    {
      "epoch": 3.1147309581615943,
      "grad_norm": 14.08942699432373,
      "learning_rate": 7.100762341710457e-06,
      "loss": 8.014,
      "num_input_tokens_seen": 46708272,
      "step": 13792,
      "train_runtime": 3675.4054,
      "train_tokens_per_second": 12708.332
    },
    {
      "epoch": 3.1219581051324035,
      "grad_norm": 14.035566329956055,
      "learning_rate": 6.99104402255526e-06,
      "loss": 8.0607,
      "num_input_tokens_seen": 46818000,
      "step": 13824,
      "train_runtime": 3683.4407,
      "train_tokens_per_second": 12710.399
    },
    {
      "epoch": 3.1291852521032126,
      "grad_norm": 14.473739624023438,
      "learning_rate": 6.882042028493016e-06,
      "loss": 7.9434,
      "num_input_tokens_seen": 46926544,
      "step": 13856,
      "train_runtime": 3691.405,
      "train_tokens_per_second": 12712.38
    },
    {
      "epoch": 3.136412399074022,
      "grad_norm": 15.244073867797852,
      "learning_rate": 6.773760695192244e-06,
      "loss": 8.0213,
      "num_input_tokens_seen": 47032912,
      "step": 13888,
      "train_runtime": 3699.354,
      "train_tokens_per_second": 12713.818
    },
    {
      "epoch": 3.143639546044831,
      "grad_norm": 14.800445556640625,
      "learning_rate": 6.66620432965642e-06,
      "loss": 8.0026,
      "num_input_tokens_seen": 47142000,
      "step": 13920,
      "train_runtime": 3707.3345,
      "train_tokens_per_second": 12715.875
    },
    {
      "epoch": 3.15086669301564,
      "grad_norm": 14.435463905334473,
      "learning_rate": 6.559377210052694e-06,
      "loss": 7.9598,
      "num_input_tokens_seen": 47251440,
      "step": 13952,
      "train_runtime": 3715.318,
      "train_tokens_per_second": 12718.007
    },
    {
      "epoch": 3.1580938399864493,
      "grad_norm": 14.077573776245117,
      "learning_rate": 6.453283585541661e-06,
      "loss": 7.9772,
      "num_input_tokens_seen": 47359984,
      "step": 13984,
      "train_runtime": 3723.3439,
      "train_tokens_per_second": 12719.745
    },
    {
      "epoch": 3.165320986957258,
      "grad_norm": 14.514512062072754,
      "learning_rate": 6.347927676108409e-06,
      "loss": 8.0936,
      "num_input_tokens_seen": 47470288,
      "step": 14016,
      "train_runtime": 3731.3493,
      "train_tokens_per_second": 12722.017
    },
    {
      "epoch": 3.1725481339280672,
      "grad_norm": 14.862444877624512,
      "learning_rate": 6.243313672394608e-06,
      "loss": 7.9356,
      "num_input_tokens_seen": 47577296,
      "step": 14048,
      "train_runtime": 3739.3401,
      "train_tokens_per_second": 12723.447
    },
    {
      "epoch": 3.1797752808988764,
      "grad_norm": 14.844301223754883,
      "learning_rate": 6.139445735531882e-06,
      "loss": 8.0865,
      "num_input_tokens_seen": 47683664,
      "step": 14080,
      "train_runtime": 3747.3265,
      "train_tokens_per_second": 12724.716
    },
    {
      "epoch": 3.1870024278696856,
      "grad_norm": 15.102490425109863,
      "learning_rate": 6.036327996976237e-06,
      "loss": 7.9665,
      "num_input_tokens_seen": 47790032,
      "step": 14112,
      "train_runtime": 3755.304,
      "train_tokens_per_second": 12726.009
    },
    {
      "epoch": 3.1942295748404947,
      "grad_norm": 14.562459945678711,
      "learning_rate": 5.9339645583437715e-06,
      "loss": 8.0143,
      "num_input_tokens_seen": 47898224,
      "step": 14144,
      "train_runtime": 3763.2498,
      "train_tokens_per_second": 12727.888
    },
    {
      "epoch": 3.2014567218113035,
      "grad_norm": 14.377050399780273,
      "learning_rate": 5.832359491247502e-06,
      "loss": 7.9633,
      "num_input_tokens_seen": 48004656,
      "step": 14176,
      "train_runtime": 3771.1889,
      "train_tokens_per_second": 12729.316
    },
    {
      "epoch": 3.2086838687821126,
      "grad_norm": 14.929991722106934,
      "learning_rate": 5.731516837135428e-06,
      "loss": 8.0003,
      "num_input_tokens_seen": 48113872,
      "step": 14208,
      "train_runtime": 3779.1385,
      "train_tokens_per_second": 12731.439
    },
    {
      "epoch": 3.215911015752922,
      "grad_norm": 15.12122631072998,
      "learning_rate": 5.631440607129787e-06,
      "loss": 7.9471,
      "num_input_tokens_seen": 48225680,
      "step": 14240,
      "train_runtime": 3787.0941,
      "train_tokens_per_second": 12734.217
    },
    {
      "epoch": 3.223138162723731,
      "grad_norm": 14.849349021911621,
      "learning_rate": 5.532134781867471e-06,
      "loss": 7.985,
      "num_input_tokens_seen": 48333744,
      "step": 14272,
      "train_runtime": 3795.0321,
      "train_tokens_per_second": 12736.057
    },
    {
      "epoch": 3.23036530969454,
      "grad_norm": 14.854914665222168,
      "learning_rate": 5.433603311341737e-06,
      "loss": 8.0297,
      "num_input_tokens_seen": 48440208,
      "step": 14304,
      "train_runtime": 3802.9661,
      "train_tokens_per_second": 12737.481
    },
    {
      "epoch": 3.2375924566653493,
      "grad_norm": 14.715860366821289,
      "learning_rate": 5.33585011474505e-06,
      "loss": 8.0022,
      "num_input_tokens_seen": 48548016,
      "step": 14336,
      "train_runtime": 3810.9037,
      "train_tokens_per_second": 12739.24
    },
    {
      "epoch": 3.2448196036361585,
      "grad_norm": 14.94886302947998,
      "learning_rate": 5.2388790803132415e-06,
      "loss": 8.0139,
      "num_input_tokens_seen": 48654992,
      "step": 14368,
      "train_runtime": 3818.8607,
      "train_tokens_per_second": 12740.709
    },
    {
      "epoch": 3.2520467506069672,
      "grad_norm": 15.768600463867188,
      "learning_rate": 5.1426940651708e-06,
      "loss": 8.1219,
      "num_input_tokens_seen": 48763760,
      "step": 14400,
      "train_runtime": 3826.8031,
      "train_tokens_per_second": 12742.688
    },
    {
      "epoch": 3.2592738975777764,
      "grad_norm": 14.71296501159668,
      "learning_rate": 5.0472988951774795e-06,
      "loss": 8.0213,
      "num_input_tokens_seen": 48874896,
      "step": 14432,
      "train_runtime": 3834.753,
      "train_tokens_per_second": 12745.253
    },
    {
      "epoch": 3.2665010445485856,
      "grad_norm": 14.33068561553955,
      "learning_rate": 4.952697364776118e-06,
      "loss": 8.0203,
      "num_input_tokens_seen": 48979632,
      "step": 14464,
      "train_runtime": 3842.7274,
      "train_tokens_per_second": 12746.059
    },
    {
      "epoch": 3.2737281915193948,
      "grad_norm": 14.701850891113281,
      "learning_rate": 4.858893236841702e-06,
      "loss": 8.0804,
      "num_input_tokens_seen": 49088944,
      "step": 14496,
      "train_runtime": 3850.6721,
      "train_tokens_per_second": 12748.15
    },
    {
      "epoch": 3.280955338490204,
      "grad_norm": 14.752975463867188,
      "learning_rate": 4.765890242531712e-06,
      "loss": 7.8689,
      "num_input_tokens_seen": 49196720,
      "step": 14528,
      "train_runtime": 3858.6118,
      "train_tokens_per_second": 12749.85
    },
    {
      "epoch": 3.288182485461013,
      "grad_norm": 14.24780559539795,
      "learning_rate": 4.673692081137682e-06,
      "loss": 7.9749,
      "num_input_tokens_seen": 49305936,
      "step": 14560,
      "train_runtime": 3866.5486,
      "train_tokens_per_second": 12751.925
    },
    {
      "epoch": 3.295409632431822,
      "grad_norm": 15.371349334716797,
      "learning_rate": 4.582302419938095e-06,
      "loss": 7.9916,
      "num_input_tokens_seen": 49416944,
      "step": 14592,
      "train_runtime": 3874.5027,
      "train_tokens_per_second": 12754.397
    },
    {
      "epoch": 3.302636779402631,
      "grad_norm": 14.78189754486084,
      "learning_rate": 4.491724894052471e-06,
      "loss": 7.9999,
      "num_input_tokens_seen": 49521840,
      "step": 14624,
      "train_runtime": 3882.4181,
      "train_tokens_per_second": 12755.411
    },
    {
      "epoch": 3.30986392637344,
      "grad_norm": 15.27332878112793,
      "learning_rate": 4.401963106296806e-06,
      "loss": 8.2099,
      "num_input_tokens_seen": 49630992,
      "step": 14656,
      "train_runtime": 3890.3651,
      "train_tokens_per_second": 12757.413
    },
    {
      "epoch": 3.3170910733442494,
      "grad_norm": 14.995861053466797,
      "learning_rate": 4.313020627040248e-06,
      "loss": 7.9757,
      "num_input_tokens_seen": 49739600,
      "step": 14688,
      "train_runtime": 3898.3615,
      "train_tokens_per_second": 12759.104
    },
    {
      "epoch": 3.3243182203150585,
      "grad_norm": 14.900314331054688,
      "learning_rate": 4.224900994063113e-06,
      "loss": 8.0668,
      "num_input_tokens_seen": 49850352,
      "step": 14720,
      "train_runtime": 3906.3309,
      "train_tokens_per_second": 12761.426
    },
    {
      "epoch": 3.3315453672858677,
      "grad_norm": 14.742110252380371,
      "learning_rate": 4.137607712416119e-06,
      "loss": 8.0225,
      "num_input_tokens_seen": 49957424,
      "step": 14752,
      "train_runtime": 3914.3589,
      "train_tokens_per_second": 12762.607
    },
    {
      "epoch": 3.338772514256677,
      "grad_norm": 15.610304832458496,
      "learning_rate": 4.05114425428102e-06,
      "loss": 8.0611,
      "num_input_tokens_seen": 50066096,
      "step": 14784,
      "train_runtime": 3922.3031,
      "train_tokens_per_second": 12764.464
    },
    {
      "epoch": 3.3459996612274856,
      "grad_norm": 15.010846138000488,
      "learning_rate": 3.965514058832442e-06,
      "loss": 8.0896,
      "num_input_tokens_seen": 50177968,
      "step": 14816,
      "train_runtime": 3930.3325,
      "train_tokens_per_second": 12766.851
    },
    {
      "epoch": 3.3532268081982948,
      "grad_norm": 14.546292304992676,
      "learning_rate": 3.880720532101131e-06,
      "loss": 7.9714,
      "num_input_tokens_seen": 50284080,
      "step": 14848,
      "train_runtime": 3938.2597,
      "train_tokens_per_second": 12768.096
    },
    {
      "epoch": 3.360453955169104,
      "grad_norm": 14.815733909606934,
      "learning_rate": 3.796767046838459e-06,
      "loss": 8.1518,
      "num_input_tokens_seen": 50394448,
      "step": 14880,
      "train_runtime": 3946.2307,
      "train_tokens_per_second": 12770.274
    },
    {
      "epoch": 3.367681102139913,
      "grad_norm": 14.606820106506348,
      "learning_rate": 3.713656942382268e-06,
      "loss": 8.0249,
      "num_input_tokens_seen": 50500976,
      "step": 14912,
      "train_runtime": 3954.1747,
      "train_tokens_per_second": 12771.559
    },
    {
      "epoch": 3.3749082491107223,
      "grad_norm": 15.198198318481445,
      "learning_rate": 3.631393524524032e-06,
      "loss": 7.9726,
      "num_input_tokens_seen": 50611504,
      "step": 14944,
      "train_runtime": 3962.1251,
      "train_tokens_per_second": 12773.828
    },
    {
      "epoch": 3.382135396081531,
      "grad_norm": 15.303020477294922,
      "learning_rate": 3.5499800653773895e-06,
      "loss": 8.1203,
      "num_input_tokens_seen": 50722576,
      "step": 14976,
      "train_runtime": 3970.0831,
      "train_tokens_per_second": 12776.2
    },
    {
      "epoch": 3.38936254305234,
      "grad_norm": 15.221779823303223,
      "learning_rate": 3.469419803247989e-06,
      "loss": 7.9242,
      "num_input_tokens_seen": 50830000,
      "step": 15008,
      "train_runtime": 3978.0175,
      "train_tokens_per_second": 12777.722
    },
    {
      "epoch": 3.3965896900231494,
      "grad_norm": 14.468530654907227,
      "learning_rate": 3.389715942504651e-06,
      "loss": 8.0341,
      "num_input_tokens_seen": 50938384,
      "step": 15040,
      "train_runtime": 3985.958,
      "train_tokens_per_second": 12779.458
    },
    {
      "epoch": 3.4038168369939585,
      "grad_norm": 15.164073944091797,
      "learning_rate": 3.3108716534519606e-06,
      "loss": 7.965,
      "num_input_tokens_seen": 51043600,
      "step": 15072,
      "train_runtime": 3993.9394,
      "train_tokens_per_second": 12780.264
    },
    {
      "epoch": 3.4110439839647677,
      "grad_norm": 15.324823379516602,
      "learning_rate": 3.2328900722041116e-06,
      "loss": 8.0279,
      "num_input_tokens_seen": 51153104,
      "step": 15104,
      "train_runtime": 4001.8977,
      "train_tokens_per_second": 12782.212
    },
    {
      "epoch": 3.418271130935577,
      "grad_norm": 14.990755081176758,
      "learning_rate": 3.155774300560213e-06,
      "loss": 8.0987,
      "num_input_tokens_seen": 51258288,
      "step": 15136,
      "train_runtime": 4009.8235,
      "train_tokens_per_second": 12783.178
    },
    {
      "epoch": 3.425498277906386,
      "grad_norm": 14.568346977233887,
      "learning_rate": 3.0795274058808805e-06,
      "loss": 7.8112,
      "num_input_tokens_seen": 51366960,
      "step": 15168,
      "train_runtime": 4017.7682,
      "train_tokens_per_second": 12784.949
    },
    {
      "epoch": 3.432725424877195,
      "grad_norm": 14.895440101623535,
      "learning_rate": 3.0041524209662246e-06,
      "loss": 8.0224,
      "num_input_tokens_seen": 51476400,
      "step": 15200,
      "train_runtime": 4025.7218,
      "train_tokens_per_second": 12786.875
    },
    {
      "epoch": 3.439952571848004,
      "grad_norm": 14.492246627807617,
      "learning_rate": 2.92965234393526e-06,
      "loss": 8.0369,
      "num_input_tokens_seen": 51584656,
      "step": 15232,
      "train_runtime": 4033.6777,
      "train_tokens_per_second": 12788.492
    },
    {
      "epoch": 3.447179718818813,
      "grad_norm": 15.4498291015625,
      "learning_rate": 2.8560301381065967e-06,
      "loss": 8.0454,
      "num_input_tokens_seen": 51693296,
      "step": 15264,
      "train_runtime": 4041.6853,
      "train_tokens_per_second": 12790.035
    },
    {
      "epoch": 3.4544068657896223,
      "grad_norm": 14.651299476623535,
      "learning_rate": 2.783288731880618e-06,
      "loss": 7.9635,
      "num_input_tokens_seen": 51802992,
      "step": 15296,
      "train_runtime": 4049.6512,
      "train_tokens_per_second": 12791.964
    },
    {
      "epoch": 3.4616340127604315,
      "grad_norm": 15.70932674407959,
      "learning_rate": 2.711431018622965e-06,
      "loss": 8.0133,
      "num_input_tokens_seen": 51913104,
      "step": 15328,
      "train_runtime": 4057.6097,
      "train_tokens_per_second": 12794.011
    },
    {
      "epoch": 3.4688611597312407,
      "grad_norm": 15.453389167785645,
      "learning_rate": 2.640459856549479e-06,
      "loss": 8.0929,
      "num_input_tokens_seen": 52021328,
      "step": 15360,
      "train_runtime": 4065.5544,
      "train_tokens_per_second": 12795.629
    },
    {
      "epoch": 3.4760883067020494,
      "grad_norm": 15.252202033996582,
      "learning_rate": 2.5703780686124917e-06,
      "loss": 8.0482,
      "num_input_tokens_seen": 52126256,
      "step": 15392,
      "train_runtime": 4073.5323,
      "train_tokens_per_second": 12796.328
    },
    {
      "epoch": 3.4833154536728586,
      "grad_norm": 14.155099868774414,
      "learning_rate": 2.5011884423885383e-06,
      "loss": 7.9375,
      "num_input_tokens_seen": 52235152,
      "step": 15424,
      "train_runtime": 4081.4971,
      "train_tokens_per_second": 12798.037
    },
    {
      "epoch": 3.4905426006436677,
      "grad_norm": 14.307854652404785,
      "learning_rate": 2.4328937299674976e-06,
      "loss": 7.9919,
      "num_input_tokens_seen": 52342352,
      "step": 15456,
      "train_runtime": 4089.4583,
      "train_tokens_per_second": 12799.336
    },
    {
      "epoch": 3.497769747614477,
      "grad_norm": 15.397934913635254,
      "learning_rate": 2.365496647843121e-06,
      "loss": 8.0518,
      "num_input_tokens_seen": 52452400,
      "step": 15488,
      "train_runtime": 4097.4305,
      "train_tokens_per_second": 12801.291
    },
    {
      "epoch": 3.5000282310428545,
      "eval_exact_token_accuracy": 0.0006737703806720674,
      "eval_loss": 2.3710410594940186,
      "eval_prediction_target_similarity": 0.0035142974497123476,
      "eval_runtime": 37.998,
      "eval_samples_per_second": 658.192,
      "eval_sequence_accuracy": 0.0,
      "eval_steps_per_second": 20.58,
      "eval_top3_token_accuracy": 0.006679022219032049,
      "eval_top5_token_accuracy": 0.015398715622723103,
      "num_input_tokens_seen": 52485200,
      "step": 15498
    },
    {
      "epoch": 3.504996894585286,
      "grad_norm": 15.67055606842041,
      "learning_rate": 2.298999876804958e-06,
      "loss": 8.1834,
      "num_input_tokens_seen": 52562384,
      "step": 15520,
      "train_runtime": 4144.2902,
      "train_tokens_per_second": 12683.085
    },
    {
      "epoch": 3.5122240415560952,
      "grad_norm": 15.183006286621094,
      "learning_rate": 2.2334060618317444e-06,
      "loss": 7.9191,
      "num_input_tokens_seen": 52671152,
      "step": 15552,
      "train_runtime": 4152.298,
      "train_tokens_per_second": 12684.82
    },
    {
      "epoch": 3.5194511885269044,
      "grad_norm": 15.487775802612305,
      "learning_rate": 2.1687178119862094e-06,
      "loss": 7.9759,
      "num_input_tokens_seen": 52779120,
      "step": 15584,
      "train_runtime": 4160.2897,
      "train_tokens_per_second": 12686.405
    },
    {
      "epoch": 3.526678335497713,
      "grad_norm": 15.197797775268555,
      "learning_rate": 2.104937700311255e-06,
      "loss": 8.0444,
      "num_input_tokens_seen": 52890416,
      "step": 15616,
      "train_runtime": 4168.309,
      "train_tokens_per_second": 12688.698
    },
    {
      "epoch": 3.5339054824685223,
      "grad_norm": 14.064817428588867,
      "learning_rate": 2.0420682637276615e-06,
      "loss": 7.9793,
      "num_input_tokens_seen": 52996976,
      "step": 15648,
      "train_runtime": 4176.3129,
      "train_tokens_per_second": 12689.896
    },
    {
      "epoch": 3.5411326294393315,
      "grad_norm": 14.639705657958984,
      "learning_rate": 1.9801120029331347e-06,
      "loss": 7.957,
      "num_input_tokens_seen": 53104464,
      "step": 15680,
      "train_runtime": 4184.3194,
      "train_tokens_per_second": 12691.303
    },
    {
      "epoch": 3.5483597764101407,
      "grad_norm": 14.778809547424316,
      "learning_rate": 1.919071382302862e-06,
      "loss": 7.964,
      "num_input_tokens_seen": 53212304,
      "step": 15712,
      "train_runtime": 4192.3116,
      "train_tokens_per_second": 12692.831
    },
    {
      "epoch": 3.55558692338095,
      "grad_norm": 14.687665939331055,
      "learning_rate": 1.8589488297914791e-06,
      "loss": 7.9386,
      "num_input_tokens_seen": 53320784,
      "step": 15744,
      "train_runtime": 4200.3178,
      "train_tokens_per_second": 12694.464
    },
    {
      "epoch": 3.5628140703517586,
      "grad_norm": 14.958569526672363,
      "learning_rate": 1.7997467368365162e-06,
      "loss": 7.9544,
      "num_input_tokens_seen": 53426416,
      "step": 15776,
      "train_runtime": 4208.3036,
      "train_tokens_per_second": 12695.476
    },
    {
      "epoch": 3.5700412173225677,
      "grad_norm": 14.912291526794434,
      "learning_rate": 1.7414674582632406e-06,
      "loss": 8.0549,
      "num_input_tokens_seen": 53536080,
      "step": 15808,
      "train_runtime": 4216.3013,
      "train_tokens_per_second": 12697.404
    },
    {
      "epoch": 3.577268364293377,
      "grad_norm": 15.265549659729004,
      "learning_rate": 1.6841133121910295e-06,
      "loss": 8.0483,
      "num_input_tokens_seen": 53643600,
      "step": 15840,
      "train_runtime": 4224.3039,
      "train_tokens_per_second": 12698.802
    },
    {
      "epoch": 3.584495511264186,
      "grad_norm": 14.168988227844238,
      "learning_rate": 1.6276865799411318e-06,
      "loss": 7.9627,
      "num_input_tokens_seen": 53750928,
      "step": 15872,
      "train_runtime": 4232.2937,
      "train_tokens_per_second": 12700.189
    },
    {
      "epoch": 3.5917226582349953,
      "grad_norm": 14.735872268676758,
      "learning_rate": 1.5721895059459369e-06,
      "loss": 7.913,
      "num_input_tokens_seen": 53860848,
      "step": 15904,
      "train_runtime": 4240.3016,
      "train_tokens_per_second": 12702.127
    },
    {
      "epoch": 3.5989498052058044,
      "grad_norm": 15.577031135559082,
      "learning_rate": 1.5176242976597254e-06,
      "loss": 8.0088,
      "num_input_tokens_seen": 53969744,
      "step": 15936,
      "train_runtime": 4248.3097,
      "train_tokens_per_second": 12703.816
    },
    {
      "epoch": 3.6061769521766136,
      "grad_norm": 14.61108684539795,
      "learning_rate": 1.4639931254708112e-06,
      "loss": 7.9253,
      "num_input_tokens_seen": 54079280,
      "step": 15968,
      "train_runtime": 4256.3206,
      "train_tokens_per_second": 12705.641
    },
    {
      "epoch": 3.6134040991474228,
      "grad_norm": 14.791561126708984,
      "learning_rate": 1.4112981226152704e-06,
      "loss": 8.0097,
      "num_input_tokens_seen": 54192048,
      "step": 16000,
      "train_runtime": 4264.3544,
      "train_tokens_per_second": 12708.148
    },
    {
      "epoch": 3.6206312461182315,
      "grad_norm": 15.715437889099121,
      "learning_rate": 1.3595413850920474e-06,
      "loss": 8.0002,
      "num_input_tokens_seen": 54300816,
      "step": 16032,
      "train_runtime": 4272.3441,
      "train_tokens_per_second": 12709.842
    },
    {
      "epoch": 3.6278583930890407,
      "grad_norm": 15.197117805480957,
      "learning_rate": 1.308724971579614e-06,
      "loss": 8.0412,
      "num_input_tokens_seen": 54406416,
      "step": 16064,
      "train_runtime": 4280.3345,
      "train_tokens_per_second": 12710.786
    },
    {
      "epoch": 3.63508554005985,
      "grad_norm": 15.143659591674805,
      "learning_rate": 1.2588509033540503e-06,
      "loss": 8.0096,
      "num_input_tokens_seen": 54513360,
      "step": 16096,
      "train_runtime": 4288.3402,
      "train_tokens_per_second": 12711.995
    },
    {
      "epoch": 3.642312687030659,
      "grad_norm": 14.960977554321289,
      "learning_rate": 1.2099211642086828e-06,
      "loss": 8.004,
      "num_input_tokens_seen": 54624848,
      "step": 16128,
      "train_runtime": 4296.357,
      "train_tokens_per_second": 12714.225
    },
    {
      "epoch": 3.6495398340014678,
      "grad_norm": 14.769020080566406,
      "learning_rate": 1.1619377003751513e-06,
      "loss": 7.9512,
      "num_input_tokens_seen": 54731856,
      "step": 16160,
      "train_runtime": 4304.3531,
      "train_tokens_per_second": 12715.466
    },
    {
      "epoch": 3.656766980972277,
      "grad_norm": 15.397634506225586,
      "learning_rate": 1.114902420446004e-06,
      "loss": 7.9495,
      "num_input_tokens_seen": 54843312,
      "step": 16192,
      "train_runtime": 4312.3742,
      "train_tokens_per_second": 12717.661
    },
    {
      "epoch": 3.663994127943086,
      "grad_norm": 15.258830070495605,
      "learning_rate": 1.0688171952987825e-06,
      "loss": 8.032,
      "num_input_tokens_seen": 54951856,
      "step": 16224,
      "train_runtime": 4320.3765,
      "train_tokens_per_second": 12719.229
    },
    {
      "epoch": 3.6712212749138953,
      "grad_norm": 15.004085540771484,
      "learning_rate": 1.0236838580215957e-06,
      "loss": 7.8575,
      "num_input_tokens_seen": 55060656,
      "step": 16256,
      "train_runtime": 4328.3799,
      "train_tokens_per_second": 12720.846
    },
    {
      "epoch": 3.6784484218847044,
      "grad_norm": 14.686347007751465,
      "learning_rate": 9.79504203840234e-07,
      "loss": 7.956,
      "num_input_tokens_seen": 55167696,
      "step": 16288,
      "train_runtime": 4336.3593,
      "train_tokens_per_second": 12722.123
    },
    {
      "epoch": 3.6856755688555136,
      "grad_norm": 14.933089256286621,
      "learning_rate": 9.362799900467206e-07,
      "loss": 8.0369,
      "num_input_tokens_seen": 55275856,
      "step": 16320,
      "train_runtime": 4344.4897,
      "train_tokens_per_second": 12723.21
    },
    {
      "epoch": 3.692902715826323,
      "grad_norm": 15.277092933654785,
      "learning_rate": 8.940129359294558e-07,
      "loss": 8.0177,
      "num_input_tokens_seen": 55380464,
      "step": 16352,
      "train_runtime": 4352.4612,
      "train_tokens_per_second": 12723.942
    },
    {
      "epoch": 3.700129862797132,
      "grad_norm": 15.997627258300781,
      "learning_rate": 8.527047227048002e-07,
      "loss": 8.0183,
      "num_input_tokens_seen": 55488880,
      "step": 16384,
      "train_runtime": 4360.4509,
      "train_tokens_per_second": 12725.491
    },
    {
      "epoch": 3.7073570097679407,
      "grad_norm": 15.123531341552734,
      "learning_rate": 8.12356993450214e-07,
      "loss": 8.0581,
      "num_input_tokens_seen": 55598160,
      "step": 16416,
      "train_runtime": 4368.4487,
      "train_tokens_per_second": 12727.209
    },
    {
      "epoch": 3.71458415673875,
      "grad_norm": 14.275647163391113,
      "learning_rate": 7.729713530389066e-07,
      "loss": 7.9693,
      "num_input_tokens_seen": 55709872,
      "step": 16448,
      "train_runtime": 4376.4672,
      "train_tokens_per_second": 12729.416
    },
    {
      "epoch": 3.721811303709559,
      "grad_norm": 15.680527687072754,
      "learning_rate": 7.345493680759874e-07,
      "loss": 7.9607,
      "num_input_tokens_seen": 55819152,
      "step": 16480,
      "train_runtime": 4384.4686,
      "train_tokens_per_second": 12731.11
    },
    {
      "epoch": 3.729038450680368,
      "grad_norm": 15.36308479309082,
      "learning_rate": 6.97092566836166e-07,
      "loss": 7.9683,
      "num_input_tokens_seen": 55927056,
      "step": 16512,
      "train_runtime": 4392.4656,
      "train_tokens_per_second": 12732.497
    },
    {
      "epoch": 3.7362655976511774,
      "grad_norm": 14.829939842224121,
      "learning_rate": 6.606024392029592e-07,
      "loss": 7.923,
      "num_input_tokens_seen": 56033744,
      "step": 16544,
      "train_runtime": 4400.4709,
      "train_tokens_per_second": 12733.579
    },
    {
      "epoch": 3.743492744621986,
      "grad_norm": 14.969654083251953,
      "learning_rate": 6.250804366094265e-07,
      "loss": 8.0806,
      "num_input_tokens_seen": 56141648,
      "step": 16576,
      "train_runtime": 4408.4321,
      "train_tokens_per_second": 12735.06
    },
    {
      "epoch": 3.7507198915927953,
      "grad_norm": 15.297309875488281,
      "learning_rate": 5.905279719804341e-07,
      "loss": 8.0066,
      "num_input_tokens_seen": 56251440,
      "step": 16608,
      "train_runtime": 4416.5253,
      "train_tokens_per_second": 12736.583
    },
    {
      "epoch": 3.7579470385636045,
      "grad_norm": 15.068950653076172,
      "learning_rate": 5.56946419676474e-07,
      "loss": 7.8975,
      "num_input_tokens_seen": 56361744,
      "step": 16640,
      "train_runtime": 4424.5476,
      "train_tokens_per_second": 12738.42
    },
    {
      "epoch": 3.7651741855344136,
      "grad_norm": 15.144413948059082,
      "learning_rate": 5.243371154389637e-07,
      "loss": 8.0001,
      "num_input_tokens_seen": 56470288,
      "step": 16672,
      "train_runtime": 4432.5378,
      "train_tokens_per_second": 12739.945
    },
    {
      "epoch": 3.772401332505223,
      "grad_norm": 15.6858549118042,
      "learning_rate": 4.92701356337158e-07,
      "loss": 7.921,
      "num_input_tokens_seen": 56577296,
      "step": 16704,
      "train_runtime": 4440.5362,
      "train_tokens_per_second": 12741.095
    },
    {
      "epoch": 3.779628479476032,
      "grad_norm": 14.284409523010254,
      "learning_rate": 4.6204040071650967e-07,
      "loss": 8.0699,
      "num_input_tokens_seen": 56683056,
      "step": 16736,
      "train_runtime": 4448.5294,
      "train_tokens_per_second": 12741.976
    },
    {
      "epoch": 3.786855626446841,
      "grad_norm": 15.042832374572754,
      "learning_rate": 4.3235546814866245e-07,
      "loss": 8.0459,
      "num_input_tokens_seen": 56790544,
      "step": 16768,
      "train_runtime": 4456.5287,
      "train_tokens_per_second": 12743.224
    },
    {
      "epoch": 3.7940827734176503,
      "grad_norm": 14.36522102355957,
      "learning_rate": 4.0364773938290357e-07,
      "loss": 7.9496,
      "num_input_tokens_seen": 56897008,
      "step": 16800,
      "train_runtime": 4464.5085,
      "train_tokens_per_second": 12744.294
    },
    {
      "epoch": 3.801309920388459,
      "grad_norm": 13.902327537536621,
      "learning_rate": 3.7591835629923187e-07,
      "loss": 7.9999,
      "num_input_tokens_seen": 57005232,
      "step": 16832,
      "train_runtime": 4472.5065,
      "train_tokens_per_second": 12745.701
    },
    {
      "epoch": 3.8085370673592682,
      "grad_norm": 14.818544387817383,
      "learning_rate": 3.4916842186290824e-07,
      "loss": 8.0966,
      "num_input_tokens_seen": 57113072,
      "step": 16864,
      "train_runtime": 4480.5281,
      "train_tokens_per_second": 12746.951
    },
    {
      "epoch": 3.8157642143300774,
      "grad_norm": 15.683215141296387,
      "learning_rate": 3.233990000806042e-07,
      "loss": 8.0241,
      "num_input_tokens_seen": 57223792,
      "step": 16896,
      "train_runtime": 4488.4946,
      "train_tokens_per_second": 12748.994
    },
    {
      "epoch": 3.8229913613008866,
      "grad_norm": 15.654497146606445,
      "learning_rate": 2.9861111595806956e-07,
      "loss": 7.9998,
      "num_input_tokens_seen": 57333648,
      "step": 16928,
      "train_runtime": 4496.4407,
      "train_tokens_per_second": 12750.896
    },
    {
      "epoch": 3.8302185082716953,
      "grad_norm": 15.019538879394531,
      "learning_rate": 2.748057554593675e-07,
      "loss": 8.0363,
      "num_input_tokens_seen": 57441520,
      "step": 16960,
      "train_runtime": 4504.3632,
      "train_tokens_per_second": 12752.417
    },
    {
      "epoch": 3.8374456552425045,
      "grad_norm": 14.674259185791016,
      "learning_rate": 2.519838654676532e-07,
      "loss": 7.9115,
      "num_input_tokens_seen": 57556528,
      "step": 16992,
      "train_runtime": 4512.4153,
      "train_tokens_per_second": 12755.149
    },
    {
      "epoch": 3.8446728022133136,
      "grad_norm": 14.506817817687988,
      "learning_rate": 2.3014635374751236e-07,
      "loss": 7.9574,
      "num_input_tokens_seen": 57665040,
      "step": 17024,
      "train_runtime": 4520.353,
      "train_tokens_per_second": 12756.756
    },
    {
      "epoch": 3.851899949184123,
      "grad_norm": 14.828859329223633,
      "learning_rate": 2.0929408890885682e-07,
      "loss": 8.0091,
      "num_input_tokens_seen": 57774640,
      "step": 17056,
      "train_runtime": 4528.2897,
      "train_tokens_per_second": 12758.601
    },
    {
      "epoch": 3.859127096154932,
      "grad_norm": 14.838537216186523,
      "learning_rate": 1.894279003723659e-07,
      "loss": 8.0041,
      "num_input_tokens_seen": 57884688,
      "step": 17088,
      "train_runtime": 4536.2331,
      "train_tokens_per_second": 12760.519
    },
    {
      "epoch": 3.866354243125741,
      "grad_norm": 15.821024894714355,
      "learning_rate": 1.705485783365074e-07,
      "loss": 8.1002,
      "num_input_tokens_seen": 57991152,
      "step": 17120,
      "train_runtime": 4544.1559,
      "train_tokens_per_second": 12761.699
    },
    {
      "epoch": 3.8735813900965503,
      "grad_norm": 15.719240188598633,
      "learning_rate": 1.526568737460904e-07,
      "loss": 7.9625,
      "num_input_tokens_seen": 58101712,
      "step": 17152,
      "train_runtime": 4552.0903,
      "train_tokens_per_second": 12763.743
    },
    {
      "epoch": 3.8808085370673595,
      "grad_norm": 15.248381614685059,
      "learning_rate": 1.3575349826241424e-07,
      "loss": 8.0922,
      "num_input_tokens_seen": 58211920,
      "step": 17184,
      "train_runtime": 4560.0365,
      "train_tokens_per_second": 12765.67
    },
    {
      "epoch": 3.8880356840381682,
      "grad_norm": 15.284089088439941,
      "learning_rate": 1.1983912423495224e-07,
      "loss": 8.0552,
      "num_input_tokens_seen": 58323664,
      "step": 17216,
      "train_runtime": 4567.9952,
      "train_tokens_per_second": 12767.891
    },
    {
      "epoch": 3.8952628310089774,
      "grad_norm": 14.716450691223145,
      "learning_rate": 1.0491438467459813e-07,
      "loss": 7.9825,
      "num_input_tokens_seen": 58433936,
      "step": 17248,
      "train_runtime": 4576.0054,
      "train_tokens_per_second": 12769.639
    },
    {
      "epoch": 3.9024899779797866,
      "grad_norm": 15.506897926330566,
      "learning_rate": 9.097987322851387e-08,
      "loss": 8.0555,
      "num_input_tokens_seen": 58542608,
      "step": 17280,
      "train_runtime": 4583.9422,
      "train_tokens_per_second": 12771.236
    },
    {
      "epoch": 3.9097171249505958,
      "grad_norm": 15.946669578552246,
      "learning_rate": 7.803614415648475e-08,
      "loss": 7.9571,
      "num_input_tokens_seen": 58650928,
      "step": 17312,
      "train_runtime": 4591.884,
      "train_tokens_per_second": 12772.737
    },
    {
      "epoch": 3.916944271921405,
      "grad_norm": 15.385106086730957,
      "learning_rate": 6.608371230890087e-08,
      "loss": 8.0178,
      "num_input_tokens_seen": 58756944,
      "step": 17344,
      "train_runtime": 4599.8013,
      "train_tokens_per_second": 12773.801
    },
    {
      "epoch": 3.9241714188922137,
      "grad_norm": 14.481250762939453,
      "learning_rate": 5.5123053106262424e-08,
      "loss": 7.991,
      "num_input_tokens_seen": 58866000,
      "step": 17376,
      "train_runtime": 4607.7354,
      "train_tokens_per_second": 12775.473
    },
    {
      "epoch": 3.931398565863023,
      "grad_norm": 14.802043914794922,
      "learning_rate": 4.515460252026982e-08,
      "loss": 8.0476,
      "num_input_tokens_seen": 58977552,
      "step": 17408,
      "train_runtime": 4615.7044,
      "train_tokens_per_second": 12777.584
    },
    {
      "epoch": 3.938625712833832,
      "grad_norm": 16.01955223083496,
      "learning_rate": 3.6178757056493095e-08,
      "loss": 8.0562,
      "num_input_tokens_seen": 59084688,
      "step": 17440,
      "train_runtime": 4623.6537,
      "train_tokens_per_second": 12778.788
    },
    {
      "epoch": 3.945852859804641,
      "grad_norm": 14.824498176574707,
      "learning_rate": 2.819587373858734e-08,
      "loss": 7.8973,
      "num_input_tokens_seen": 59188080,
      "step": 17472,
      "train_runtime": 4631.5805,
      "train_tokens_per_second": 12779.24
    },
    {
      "epoch": 3.9530800067754504,
      "grad_norm": 14.88594913482666,
      "learning_rate": 2.1206270094104032e-08,
      "loss": 8.0014,
      "num_input_tokens_seen": 59297552,
      "step": 17504,
      "train_runtime": 4639.5389,
      "train_tokens_per_second": 12780.915
    },
    {
      "epoch": 3.9603071537462595,
      "grad_norm": 14.604466438293457,
      "learning_rate": 1.5210224141848363e-08,
      "loss": 7.9783,
      "num_input_tokens_seen": 59406544,
      "step": 17536,
      "train_runtime": 4647.5043,
      "train_tokens_per_second": 12782.461
    },
    {
      "epoch": 3.9675343007170687,
      "grad_norm": 15.572702407836914,
      "learning_rate": 1.0207974380829766e-08,
      "loss": 7.9326,
      "num_input_tokens_seen": 59515824,
      "step": 17568,
      "train_runtime": 4655.5234,
      "train_tokens_per_second": 12783.917
    },
    {
      "epoch": 3.9747614476878774,
      "grad_norm": 14.79893970489502,
      "learning_rate": 6.199719780777824e-09,
      "loss": 8.004,
      "num_input_tokens_seen": 59622896,
      "step": 17600,
      "train_runtime": 4663.5183,
      "train_tokens_per_second": 12784.96
    },
    {
      "epoch": 3.9819885946586866,
      "grad_norm": 15.172889709472656,
      "learning_rate": 3.185619774215276e-09,
      "loss": 7.9362,
      "num_input_tokens_seen": 59731760,
      "step": 17632,
      "train_runtime": 4671.4627,
      "train_tokens_per_second": 12786.522
    },
    {
      "epoch": 3.9892157416294958,
      "grad_norm": 14.32496452331543,
      "learning_rate": 1.1657942501297481e-09,
      "loss": 7.9922,
      "num_input_tokens_seen": 59838640,
      "step": 17664,
      "train_runtime": 4679.3973,
      "train_tokens_per_second": 12787.681
    },
    {
      "epoch": 3.996442888600305,
      "grad_norm": 15.33981704711914,
      "learning_rate": 1.4032354919701895e-10,
      "loss": 7.953,
      "num_input_tokens_seen": 59946448,
      "step": 17696,
      "train_runtime": 4687.3647,
      "train_tokens_per_second": 12788.945
    },
    {
      "epoch": 4.0,
      "eval_exact_token_accuracy": 0.0007012144196778536,
      "eval_loss": 2.369314670562744,
      "eval_prediction_target_similarity": 0.0034487060444689213,
      "eval_runtime": 37.9071,
      "eval_samples_per_second": 659.772,
      "eval_sequence_accuracy": 0.0,
      "eval_steps_per_second": 20.629,
      "eval_top3_token_accuracy": 0.006592845544219017,
      "eval_top5_token_accuracy": 0.014870504848659039,
      "num_input_tokens_seen": 60000910,
      "step": 17712
    }
  ],
  "logging_steps": 32,
  "max_steps": 17712,
  "num_input_tokens_seen": 60000910,
  "num_train_epochs": 4,
  "save_steps": 2214,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.697279185677234e+16,
  "train_batch_size": 32,
  "trial_name": null,
  "trial_params": null
}
